{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee1e4046",
   "metadata": {},
   "source": [
    "# Environment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "573d9abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f0455d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of GPU: 4\n",
      "Total GPU memory: 51.033931776 GB\n",
      "GPU ID: 0\n",
      "GPU Name: NVIDIA RTX A6000\n",
      "GPU Compute Capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_id = torch.cuda.current_device()\n",
    "    gpu_name = torch.cuda.get_device_name(gpu_id)\n",
    "    gpu_capability = torch.cuda.get_device_capability(gpu_id)\n",
    "    total_memory = torch.cuda.get_device_properties(gpu_id).total_memory\n",
    "    \n",
    "    print(f\"Total number of GPU: {torch.cuda.device_count()}\")  \n",
    "    print(f\"Total GPU memory: {total_memory / 1e9} GB\")\n",
    "    print(f\"GPU ID: {gpu_id}\")\n",
    "    print(f\"GPU Name: {gpu_name}\")\n",
    "    print(f\"GPU Compute Capability: {gpu_capability}\")\n",
    "else:\n",
    "    print(\"No GPU is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7125a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device: 2\n",
      "Device Name: NVIDIA RTX A6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.set_device(2)\n",
    "\n",
    "print(\"Current Device:\", torch.cuda.current_device())\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354fbb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebf7b0d",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d78931a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cfdc24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../dataset/ETTh1.csv', index_col='date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6aea87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HUFL</th>\n",
       "      <th>HULL</th>\n",
       "      <th>MUFL</th>\n",
       "      <th>MULL</th>\n",
       "      <th>LUFL</th>\n",
       "      <th>LULL</th>\n",
       "      <th>OT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-07-01 00:00:00</th>\n",
       "      <td>5.827</td>\n",
       "      <td>2.009</td>\n",
       "      <td>1.599</td>\n",
       "      <td>0.462</td>\n",
       "      <td>4.203</td>\n",
       "      <td>1.340</td>\n",
       "      <td>30.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01 01:00:00</th>\n",
       "      <td>5.693</td>\n",
       "      <td>2.076</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.426</td>\n",
       "      <td>4.142</td>\n",
       "      <td>1.371</td>\n",
       "      <td>27.787001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01 02:00:00</th>\n",
       "      <td>5.157</td>\n",
       "      <td>1.741</td>\n",
       "      <td>1.279</td>\n",
       "      <td>0.355</td>\n",
       "      <td>3.777</td>\n",
       "      <td>1.218</td>\n",
       "      <td>27.787001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01 03:00:00</th>\n",
       "      <td>5.090</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.279</td>\n",
       "      <td>0.391</td>\n",
       "      <td>3.807</td>\n",
       "      <td>1.279</td>\n",
       "      <td>25.044001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01 04:00:00</th>\n",
       "      <td>5.358</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.462</td>\n",
       "      <td>3.868</td>\n",
       "      <td>1.279</td>\n",
       "      <td>21.948000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      HUFL   HULL   MUFL   MULL   LUFL   LULL         OT\n",
       "date                                                                    \n",
       "2016-07-01 00:00:00  5.827  2.009  1.599  0.462  4.203  1.340  30.531000\n",
       "2016-07-01 01:00:00  5.693  2.076  1.492  0.426  4.142  1.371  27.787001\n",
       "2016-07-01 02:00:00  5.157  1.741  1.279  0.355  3.777  1.218  27.787001\n",
       "2016-07-01 03:00:00  5.090  1.942  1.279  0.391  3.807  1.279  25.044001\n",
       "2016-07-01 04:00:00  5.358  1.942  1.492  0.462  3.868  1.279  21.948000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "670cdf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUFL    float64\n",
      "HULL    float64\n",
      "MUFL    float64\n",
      "MULL    float64\n",
      "LUFL    float64\n",
      "LULL    float64\n",
      "OT      float64\n",
      "dtype: object\n",
      "shape:  (17420, 7)\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)\n",
    "print('shape: ', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4925c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['OT']\n",
    "features = data.drop(['OT'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d2406d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['DATE'] = features.index.strftime('%Y%m%d%H').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9334e824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2016-07-01 00:00:00', '2016-07-01 01:00:00',\n",
       "               '2016-07-01 02:00:00', '2016-07-01 03:00:00',\n",
       "               '2016-07-01 04:00:00', '2016-07-01 05:00:00',\n",
       "               '2016-07-01 06:00:00', '2016-07-01 07:00:00',\n",
       "               '2016-07-01 08:00:00', '2016-07-01 09:00:00',\n",
       "               ...\n",
       "               '2018-06-26 10:00:00', '2018-06-26 11:00:00',\n",
       "               '2018-06-26 12:00:00', '2018-06-26 13:00:00',\n",
       "               '2018-06-26 14:00:00', '2018-06-26 15:00:00',\n",
       "               '2018-06-26 16:00:00', '2018-06-26 17:00:00',\n",
       "               '2018-06-26 18:00:00', '2018-06-26 19:00:00'],\n",
       "              dtype='datetime64[ns]', name='date', length=17420, freq=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b069872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)                          \n",
    "    np.random.seed(seed)                        \n",
    "    torch.manual_seed(seed)                     \n",
    "    torch.cuda.manual_seed(seed)                \n",
    "    torch.cuda.manual_seed_all(seed)            \n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.benchmark = False      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70a07240",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(24) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0140c72",
   "metadata": {},
   "source": [
    "# Model Modification for DLinear (Decomposition-wise Look-ahead Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df9b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.individual = configs.individual\n",
    "        self.channels = configs.enc_in\n",
    "        self.attach_to_trend = configs.attach_to_trend\n",
    "        self.attach_to_seasonal = configs.attach_to_seasonal\n",
    "        \n",
    "\n",
    "        kernel_size = 25\n",
    "        self.decompsition = series_decomp(kernel_size)\n",
    "        \n",
    "\n",
    "        self.seq_len_trend = self.seq_len + (self.pred_len // 3) if self.attach_to_trend else self.seq_len\n",
    "        self.seq_len_seasonal = self.seq_len + (self.pred_len // 3) if self.attach_to_seasonal else self.seq_len\n",
    "\n",
    "        \n",
    "        if self.individual:\n",
    "            self.Linear_Seasonal = nn.ModuleList()\n",
    "            self.Linear_Trend = nn.ModuleList()\n",
    "            \n",
    "            for i in range(self.channels):\n",
    "                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))\n",
    "                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))\n",
    "\n",
    "        else:\n",
    "            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)\n",
    "            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)\n",
    "\n",
    "    def forward(self, x, ground_truth=None):\n",
    "        seasonal_init, trend_init = self.decompsition(x)\n",
    "        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)\n",
    "        if ground_truth is not None:\n",
    "            kernel_size = 25\n",
    "            self.decompsition = series_decomp(kernel_size)\n",
    "            ground_truth_seasonal, ground_truth_trend = self.decompsition(ground_truth)\n",
    "\n",
    "            if self.seq_len_seasonal > self.seq_len:\n",
    "                ground_truth_seasonal=ground_truth_seasonal.permute(0, 2, 1)\n",
    "                seasonal_init = torch.cat([seasonal_init, ground_truth_seasonal], dim=2)\n",
    "            if self.seq_len_trend > self.seq_len:\n",
    "                ground_truth_trend=ground_truth_trend.permute(0, 2, 1)\n",
    "                trend_init = torch.cat([trend_init, ground_truth_trend], dim=2)\n",
    "        \n",
    "        \n",
    "        if self.individual:\n",
    "            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)\n",
    "            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)\n",
    "            for i in range(self.channels):\n",
    "                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])\n",
    "                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])\n",
    "        else:\n",
    "            seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "            trend_output = self.Linear_Trend(trend_init)\n",
    "\n",
    "        x = seasonal_output + trend_output\n",
    "        return x.permute(0,2,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62708d9",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eda30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Sequences: torch.Size([8209, 336, 1])\n",
      "Train Labels: torch.Size([8209, 96, 1])\n",
      "Val Sequences: torch.Size([2785, 336, 1])\n",
      "Val Labels: torch.Size([2785, 96, 1])\n",
      "Test Sequences: torch.Size([2785, 336, 1])\n",
      "Test Labels: torch.Size([2785, 96, 1])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "target = data['OT'].values.reshape(-1, 1) \n",
    "\n",
    "seq_len = 336\n",
    "pred_len = 96\n",
    "\n",
    "border1s = [0, 12 * 30 * 24 - seq_len, 12 * 30 * 24 + 4 * 30 * 24 - seq_len]\n",
    "border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n",
    "\n",
    "train = target[border1s[0]:border2s[0]]\n",
    "val = target[border1s[1]:border2s[1]]\n",
    "test = target[border1s[2]:border2s[2]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train = scaler.fit_transform(train)\n",
    "val = scaler.transform(val)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "def create_inout_sequences_univariate(data, seq_len, pred_len):\n",
    "    seqs = []\n",
    "    for i in range(len(data) - seq_len - pred_len + 1):\n",
    "        seq_x = data[i:i + seq_len]\n",
    "        seq_y = data[i + seq_len:i + seq_len + pred_len]\n",
    "        seqs.append((seq_x, seq_y))\n",
    "    return seqs\n",
    "\n",
    "train_data = create_inout_sequences_univariate(train, seq_len, pred_len)\n",
    "val_data = create_inout_sequences_univariate(val, seq_len, pred_len)\n",
    "test_data = create_inout_sequences_univariate(test, seq_len, pred_len)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_sequences = torch.tensor([x[0] for x in train_data]).float().to(device)  \n",
    "train_labels = torch.tensor([x[1] for x in train_data]).float().to(device)\n",
    "\n",
    "val_sequences = torch.tensor([x[0] for x in val_data]).float().to(device)\n",
    "val_labels = torch.tensor([x[1] for x in val_data]).float().to(device)\n",
    "\n",
    "test_sequences = torch.tensor([x[0] for x in test_data]).float().to(device)\n",
    "test_labels = torch.tensor([x[1] for x in test_data]).float().to(device)\n",
    "\n",
    "print(\"Train Sequences:\", train_sequences.shape)\n",
    "print(\"Train Labels:\", train_labels.shape)\n",
    "print(\"Val Sequences:\", val_sequences.shape)\n",
    "print(\"Val Labels:\", val_labels.shape)\n",
    "print(\"Test Sequences:\", test_sequences.shape)\n",
    "print(\"Test Labels:\", test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcea2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(train_sequences, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(val_sequences, val_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(test_sequences, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c6e00",
   "metadata": {},
   "source": [
    "# 1st Model Training (Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f789f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c86b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 0.01\n",
      "Epoch 0 | Train Loss: 0.30040217370722544 | Val Loss: 0.11426130796528676\n",
      "Validation loss decreased (inf --> 0.114261).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "Epoch 1 | Train Loss: 0.17064307238689191 | Val Loss: 0.09608124113980342\n",
      "Validation loss decreased (0.114261 --> 0.096081).  Saving model ...\n",
      "Updating learning rate to 0.0025\n",
      "Epoch 2 | Train Loss: 0.15661819447339276 | Val Loss: 0.09316110746427016\n",
      "Validation loss decreased (0.096081 --> 0.093161).  Saving model ...\n",
      "Updating learning rate to 0.00125\n",
      "Epoch 3 | Train Loss: 0.1454292857287459 | Val Loss: 0.09058153639886189\n",
      "Validation loss decreased (0.093161 --> 0.090582).  Saving model ...\n",
      "Updating learning rate to 0.000625\n",
      "Epoch 4 | Train Loss: 0.14348975209526516 | Val Loss: 0.09158864863936535\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0003125\n",
      "Epoch 5 | Train Loss: 0.1414206839141215 | Val Loss: 0.09052422563333741\n",
      "Validation loss decreased (0.090582 --> 0.090524).  Saving model ...\n",
      "Updating learning rate to 0.00015625\n",
      "Epoch 6 | Train Loss: 0.13986682657021957 | Val Loss: 0.09137056405994702\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-05\n",
      "Epoch 7 | Train Loss: 0.13898427175640596 | Val Loss: 0.09004747948016632\n",
      "Validation loss decreased (0.090524 --> 0.090047).  Saving model ...\n",
      "Updating learning rate to 3.90625e-05\n",
      "Epoch 8 | Train Loss: 0.13877471991550133 | Val Loss: 0.08990554817401888\n",
      "Validation loss decreased (0.090047 --> 0.089906).  Saving model ...\n",
      "Updating learning rate to 1.953125e-05\n",
      "Epoch 9 | Train Loss: 0.13861218752912047 | Val Loss: 0.08988658820321276\n",
      "Validation loss decreased (0.089906 --> 0.089887).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "\n",
    "seq_len = 336\n",
    "pred_len = 96  \n",
    "\n",
    "class Configs:\n",
    "    def __init__(self, seq_len, pred_len, enc_in, individual, learning_rate, lradj, patience, save_path, attach_to_trend, attach_to_seasonal):\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len  \n",
    "        self.enc_in = enc_in\n",
    "        self.individual = individual\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lradj = lradj\n",
    "        self.patience = patience\n",
    "        self.save_path = save_path\n",
    "        self.attach_to_trend = attach_to_trend\n",
    "        self.attach_to_seasonal = attach_to_seasonal\n",
    "\n",
    "configs = Configs(seq_len=seq_len, pred_len=pred_len, enc_in=1, individual=False,\n",
    "                   learning_rate=0.005, lradj='type1',  patience=3, save_path=\"./model_ETTh1\", attach_to_trend=False , attach_to_seasonal=False) \n",
    "\n",
    "\n",
    "model = Model(configs).to(device)\n",
    "loss_function = nn.MSELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=configs.learning_rate)\n",
    "\n",
    "if not os.path.exists(configs.save_path):\n",
    "    os.makedirs(configs.save_path)\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), os.path.join(path, 'checkpoint_ES.pth'))\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    if args.lradj == 'type1':\n",
    "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "    elif args.lradj == 'type2':\n",
    "        lr_adjust = {\n",
    "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "        }\n",
    "    elif args.lradj == '3':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 10 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '4':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 15 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '5':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 25 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '6':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 5 else args.learning_rate*0.1}  \n",
    "\n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        print(f'Updating learning rate to {lr}')\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, val_dataloader, optimizer, loss_function, epochs, pred_len, args):\n",
    "    early_stopping = EarlyStopping(patience=args.patience, verbose=True)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        \n",
    "        for batch_sequences, batch_labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(batch_sequences)\n",
    "            \n",
    "            output = output[:, -pred_len:, :]  \n",
    "            batch_labels = batch_labels[:, -pred_len:, :]  \n",
    "            \n",
    "            assert output.shape == batch_labels.shape, f\"Output shape {output.shape} and batch_labels shape {batch_labels.shape} must match\"\n",
    "            \n",
    "            loss = loss_function(output, batch_labels)  \n",
    "            \n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_batch_sequences, val_batch_labels in val_dataloader:\n",
    "                val_output = model(val_batch_sequences)\n",
    "                val_output = val_output[:, -args.pred_len:, :]\n",
    "                val_batch_labels = val_batch_labels[:, -args.pred_len:, :]\n",
    "                \n",
    "                val_loss += loss_function(val_output, val_batch_labels).item()\n",
    "        \n",
    "        val_loss /= len(val_dataloader)\n",
    "\n",
    "        print(f'Epoch {epoch} | Train Loss: {total_loss / len(dataloader)} | Val Loss: {val_loss}')\n",
    "        \n",
    "        early_stopping(val_loss, model, args.save_path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "train_model(model, train_dataloader, val_dataloader, optimizer, loss_function, epochs=10, pred_len=configs.pred_len, args=configs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c3124",
   "metadata": {},
   "source": [
    "# 1st Model Training (1 Epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f891da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 0.2955037660114032 | Val Loss: 0.10543158390051262\n",
      "Model saved at epoch 1\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "seq_len = 336\n",
    "pred_len = 96  \n",
    "\n",
    "class Configs:\n",
    "    def __init__(self, seq_len, pred_len, enc_in, individual, learning_rate, lradj, patience, save_path, attach_to_trend, attach_to_seasonal):\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len  \n",
    "        self.enc_in = enc_in\n",
    "        self.individual = individual\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lradj = lradj\n",
    "        self.patience = patience\n",
    "        self.save_path = save_path\n",
    "        self.attach_to_trend = attach_to_trend\n",
    "        self.attach_to_seasonal = attach_to_seasonal\n",
    "\n",
    "configs = Configs(seq_len=seq_len, pred_len=pred_len, enc_in=1, individual=False,\n",
    "                   learning_rate=0.005, lradj='type1',  patience=3, save_path=\"./model_ETTh1\", attach_to_trend=False , attach_to_seasonal=False) \n",
    "\n",
    "\n",
    "model = Model(configs).to(device)\n",
    "loss_function = nn.MSELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=configs.learning_rate)\n",
    "\n",
    "if not os.path.exists(configs.save_path):\n",
    "    os.makedirs(configs.save_path)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    if args.lradj == 'type1':\n",
    "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "    elif args.lradj == 'type2':\n",
    "        lr_adjust = {\n",
    "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "        }\n",
    "    elif args.lradj == '3':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 10 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '4':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 15 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '5':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 25 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '6':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 5 else args.learning_rate*0.1}  \n",
    "\n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        print(f'Updating learning rate to {lr}')\n",
    "\n",
    "def train_model(model, dataloader, val_dataloader, optimizer, loss_function, epochs, pred_len, args):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        if epoch == 1:\n",
    "            torch.save(model.state_dict(), os.path.join(args.save_path, 'checkpoint_1.pth'))\n",
    "            print(\"Model saved at epoch 1\")\n",
    "\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        \n",
    "        for batch_sequences, batch_labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(batch_sequences)\n",
    "\n",
    "            output = output[:, -pred_len:, :]  \n",
    "            batch_labels = batch_labels[:, -pred_len:, :] \n",
    "            \n",
    "            assert output.shape == batch_labels.shape, f\"Output shape {output.shape} and batch_labels shape {batch_labels.shape} must match\"\n",
    "            \n",
    "            loss = loss_function(output, batch_labels) \n",
    "            \n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_batch_sequences, val_batch_labels in val_dataloader:\n",
    "                val_output = model(val_batch_sequences)\n",
    "                val_output = val_output[:, -args.pred_len:, :]\n",
    "                val_batch_labels = val_batch_labels[:, -args.pred_len:, :]\n",
    "                \n",
    "                val_loss += loss_function(val_output, val_batch_labels).item()\n",
    "        \n",
    "        val_loss /= len(val_dataloader)\n",
    "\n",
    "        print(f'Epoch {epoch} | Train Loss: {total_loss / len(dataloader)} | Val Loss: {val_loss}')\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(args.save_path, f'checkpoint_{epochs}.pth'))\n",
    "    print(f\"Model saved at epoch {epochs}\")\n",
    "\n",
    "train_model(model, train_dataloader, val_dataloader, optimizer, loss_function, epochs=1, pred_len=configs.pred_len, args=configs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442edf18",
   "metadata": {},
   "source": [
    "# Perform Inference and Form Predicted Value (PV) Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef2d510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Predicting with model from epoch ES]\n",
      "Epoch ES | MSE: 0.05794966220855713 | MAE: 0.18053586781024933\n",
      "\n",
      "[Predicting with model from epoch 1]\n",
      "Epoch 1 | MSE: 0.09254826605319977 | MAE: 0.23211471736431122\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "\n",
    "checkpoint_epochs = ['ES', 1]\n",
    "\n",
    "test_1st_results = {}\n",
    "\n",
    "for ckpt_epoch in checkpoint_epochs:\n",
    "    print(f'\\n[Predicting with model from epoch {ckpt_epoch}]')\n",
    "\n",
    "    model_path = os.path.join(configs.save_path, f'checkpoint_{ckpt_epoch}.pth')\n",
    "    \n",
    "    model = Model(configs).to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_sequence, batch_label in test_dataloader:\n",
    "            batch_sequence = batch_sequence.float().to(device)\n",
    "            batch_label = batch_label.float().to(device)\n",
    "\n",
    "            output = model(batch_sequence)\n",
    "            output = output[:, -configs.pred_len:, :]\n",
    "            batch_label = batch_label[:, -configs.pred_len:, :]\n",
    "\n",
    "            predictions.append(output.detach().cpu().numpy())\n",
    "            actuals.append(batch_label.detach().cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)  \n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "\n",
    "    predictions_flatten = predictions.flatten()\n",
    "    actuals_flatten = actuals.flatten()\n",
    "\n",
    "    mse = mean_squared_error(actuals_flatten, predictions_flatten)\n",
    "    mae = mean_absolute_error(actuals_flatten, predictions_flatten)\n",
    "\n",
    "    print(f'Epoch {ckpt_epoch} | MSE: {mse} | MAE: {mae}')\n",
    "\n",
    "    \n",
    "    num_samples = len(predictions_flatten) // configs.pred_len\n",
    "    test_pv = torch.tensor(\n",
    "        predictions_flatten.reshape(num_samples, configs.pred_len, 1),\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    \n",
    "    test_1st_results[ckpt_epoch] = {\n",
    "        f'mse_test_E{ckpt_epoch}': mse,\n",
    "        f'mae_test_E{ckpt_epoch}': mae,\n",
    "        f'test_pv_E{ckpt_epoch}': test_pv\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48a9acba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ES', 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1st_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab46a84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Predicting with model from epoch ES]\n",
      "Epoch ES | MSE: 0.09066066890954971 | MAE: 0.23351846635341644\n",
      "\n",
      "[Predicting with model from epoch 1]\n",
      "Epoch 1 | MSE: 0.10638295114040375 | MAE: 0.2526181638240814\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "\n",
    "\n",
    "checkpoint_epochs = ['ES', 1]\n",
    "\n",
    "\n",
    "val_1st_results = {}\n",
    "\n",
    "for ckpt_epoch in checkpoint_epochs:\n",
    "    print(f'\\n[Predicting with model from epoch {ckpt_epoch}]')\n",
    "\n",
    "    \n",
    "    model_path = os.path.join(configs.save_path, f'checkpoint_{ckpt_epoch}.pth')\n",
    "    \n",
    "    model = Model(configs).to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_sequence, batch_label in val_dataloader:\n",
    "            batch_sequence = batch_sequence.float().to(device)\n",
    "            batch_label = batch_label.float().to(device)\n",
    "\n",
    "            output = model(batch_sequence)\n",
    "            output = output[:, -configs.pred_len:, :]\n",
    "            batch_label = batch_label[:, -configs.pred_len:, :]\n",
    "\n",
    "            predictions.append(output.detach().cpu().numpy())\n",
    "            actuals.append(batch_label.detach().cpu().numpy())\n",
    "\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)  \n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "\n",
    "    \n",
    "    predictions_flatten = predictions.flatten()\n",
    "    actuals_flatten = actuals.flatten()\n",
    "\n",
    "    \n",
    "    mse = mean_squared_error(actuals_flatten, predictions_flatten)\n",
    "    mae = mean_absolute_error(actuals_flatten, predictions_flatten)\n",
    "\n",
    "    print(f'Epoch {ckpt_epoch} | MSE: {mse} | MAE: {mae}')\n",
    "\n",
    "    \n",
    "    num_samples = len(predictions_flatten) // configs.pred_len\n",
    "    val_pv = torch.tensor(\n",
    "        predictions_flatten.reshape(num_samples, configs.pred_len, 1),\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    \n",
    "    val_1st_results[ckpt_epoch] = {\n",
    "        f'mse_val_E{ckpt_epoch}': mse,\n",
    "        f'mae_val_E{ckpt_epoch}': mae,\n",
    "        f'val_pv_E{ckpt_epoch}': val_pv\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Predicting with model from epoch ES]\n",
      "Epoch ES | MSE: 0.13843832910060883 | MAE: 0.28185057640075684\n",
      "\n",
      "[Predicting with model from epoch 1]\n",
      "Epoch 1 | MSE: 0.1669788658618927 | MAE: 0.31184351444244385\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "\n",
    "\n",
    "train_dataloader_infer = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "checkpoint_epochs = ['ES', 1]\n",
    "\n",
    "\n",
    "train_1st_results = {}\n",
    "\n",
    "for ckpt_epoch in checkpoint_epochs:\n",
    "    print(f'\\n[Predicting with model from epoch {ckpt_epoch}]')\n",
    "\n",
    "    \n",
    "    model_path = os.path.join(configs.save_path, f'checkpoint_{ckpt_epoch}.pth')\n",
    "    \n",
    "    model = Model(configs).to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_sequence, batch_label in train_dataloader_infer:\n",
    "            batch_sequence = batch_sequence.float().to(device)\n",
    "            batch_label = batch_label.float().to(device)\n",
    "\n",
    "            output = model(batch_sequence)\n",
    "            output = output[:, -configs.pred_len:, :]\n",
    "            batch_label = batch_label[:, -configs.pred_len:, :]\n",
    "\n",
    "            predictions.append(output.detach().cpu().numpy())\n",
    "            actuals.append(batch_label.detach().cpu().numpy())\n",
    "\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)  \n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "\n",
    "    \n",
    "    predictions_flatten = predictions.flatten()\n",
    "    actuals_flatten = actuals.flatten()\n",
    "\n",
    "    \n",
    "    mse = mean_squared_error(actuals_flatten, predictions_flatten)\n",
    "    mae = mean_absolute_error(actuals_flatten, predictions_flatten)\n",
    "\n",
    "    print(f'Epoch {ckpt_epoch} | MSE: {mse} | MAE: {mae}')\n",
    "\n",
    "    \n",
    "    num_samples = len(predictions_flatten) // configs.pred_len\n",
    "    train_pv = torch.tensor(\n",
    "        predictions_flatten.reshape(num_samples, configs.pred_len, 1),\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    \n",
    "    train_1st_results[ckpt_epoch] = {\n",
    "        f'mse_train_E{ckpt_epoch}': mse,\n",
    "        f'mae_train_E{ckpt_epoch}': mae,\n",
    "        f'train_pv_E{ckpt_epoch}': train_pv\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c540062e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sequences shape:  torch.Size([8209, 336, 1])\n",
      "val_sequences shape:  torch.Size([2785, 336, 1])\n",
      "test_sequences shape:  torch.Size([2785, 336, 1])\n",
      "---------------------------------------------\n",
      "train_pv shape:  torch.Size([8209, 96, 1])\n",
      "val_pv shape:  torch.Size([2785, 96, 1])\n",
      "test_pv shape:  torch.Size([2785, 96, 1])\n",
      "---------------------------------------------\n",
      "train_labels shape:  torch.Size([8209, 96, 1])\n",
      "val_labels shape:  torch.Size([2785, 96, 1])\n",
      "test_labels shape:  torch.Size([2785, 96, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"train_sequences shape: \",train_sequences.shape)\n",
    "print(\"val_sequences shape: \",val_sequences.shape)\n",
    "print(\"test_sequences shape: \",test_sequences.shape)\n",
    "\n",
    "print('---------------------------------------------')\n",
    "\n",
    "print(\"train_pv shape: \",train_pv.shape)\n",
    "print(\"val_pv shape: \",val_pv.shape)\n",
    "print(\"test_pv shape: \",test_pv.shape)\n",
    "\n",
    "\n",
    "print('---------------------------------------------')\n",
    "\n",
    "print(\"train_labels shape: \",train_labels.shape)\n",
    "print(\"val_labels shape: \",val_labels.shape)\n",
    "print(\"test_labels shape: \",test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc6bbe",
   "metadata": {},
   "source": [
    "# Segment Generation Function (sliding window-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd66a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_segments(predictions, labels, segment_length, stride=1):\n",
    "    B, T, C = predictions.shape\n",
    "    num_segments = (T - segment_length) // stride + 1\n",
    "\n",
    "    pred_segments = []\n",
    "    label_segments = []\n",
    "\n",
    "    for i in range(0, T - segment_length + 1, stride):\n",
    "        pred_seg = predictions[:, i:i+segment_length, :]  \n",
    "        label_seg = labels[:, i:i+segment_length, :]      \n",
    "        pred_segments.append(pred_seg.unsqueeze(1))       \n",
    "        label_segments.append(label_seg.unsqueeze(1))\n",
    "\n",
    "    pred_segments = torch.cat(pred_segments, dim=1)       \n",
    "    label_segments = torch.cat(label_segments, dim=1)     \n",
    "\n",
    "    return pred_segments, label_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb39ee",
   "metadata": {},
   "source": [
    "# Segmentation for PV Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d6195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs.pred_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa110891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch ES] Train seg shape: torch.Size([8209, 65, 32, 1]), Val: torch.Size([2785, 65, 32, 1]), Test: torch.Size([2785, 65, 32, 1])\n",
      "[Epoch 1] Train seg shape: torch.Size([8209, 65, 32, 1]), Val: torch.Size([2785, 65, 32, 1]), Test: torch.Size([2785, 65, 32, 1])\n"
     ]
    }
   ],
   "source": [
    "segment_len = configs.pred_len // 3\n",
    "stride = 1  \n",
    "\n",
    "train_segments_by_epoch = {}\n",
    "val_segments_by_epoch = {}\n",
    "test_segments_by_epoch = {}\n",
    "\n",
    "for epoch in checkpoint_epochs:\n",
    "    train_pv = train_1st_results[epoch][f'train_pv_E{epoch}']  \n",
    "    val_pv = val_1st_results[epoch][f'val_pv_E{epoch}']        \n",
    "    test_pv = test_1st_results[epoch][f'test_pv_E{epoch}']     \n",
    "\n",
    "    \n",
    "    train_pred_segments, train_label_segments = create_sliding_segments(train_pv, train_labels, segment_len,stride)\n",
    "    val_pred_segments, val_label_segments = create_sliding_segments(val_pv, val_labels, segment_len, stride)\n",
    "    test_pred_segments, test_label_segments = create_sliding_segments(test_pv, test_labels, segment_len, stride)\n",
    "\n",
    "    train_segments_by_epoch[epoch] = {\n",
    "        f'train_pred_segments_E{epoch}': train_pred_segments,  \n",
    "        f'train_label_segments_E{epoch}': train_label_segments\n",
    "    }\n",
    "\n",
    "    val_segments_by_epoch[epoch] = {\n",
    "        f'val_pred_segments_E{epoch}': val_pred_segments,\n",
    "        f'val_label_segments_E{epoch}': val_label_segments\n",
    "    }\n",
    "\n",
    "    test_segments_by_epoch[epoch] = {\n",
    "        f'test_pred_segments_E{epoch}': test_pred_segments,\n",
    "        f'test_label_segments_E{epoch}': test_label_segments\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f\"[Epoch {epoch}] Train seg shape: {train_pred_segments.shape}, Val: {val_pred_segments.shape}, Test: {test_pred_segments.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b2b0a8",
   "metadata": {},
   "source": [
    "## Save 1st Performance Metrics, PV, and Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53bf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All dictionaries saved successfully in ./saved_results\n"
     ]
    }
   ],
   "source": [
    "save_dir = './1st_results_ETTh1'  \n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "torch.save(train_1st_results, os.path.join(save_dir, 'train_1st_results.pt'))\n",
    "torch.save(val_1st_results, os.path.join(save_dir, 'val_1st_results.pt'))\n",
    "torch.save(test_1st_results, os.path.join(save_dir, 'test_1st_results.pt'))\n",
    "\n",
    "\n",
    "torch.save(train_segments_by_epoch, os.path.join(save_dir, 'train_segments_by_epoch.pt'))\n",
    "torch.save(val_segments_by_epoch, os.path.join(save_dir, 'val_segments_by_epoch.pt'))\n",
    "torch.save(test_segments_by_epoch, os.path.join(save_dir, 'test_segments_by_epoch.pt'))\n",
    "\n",
    "print(\"✅ All dictionaries saved successfully in ./saved_results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d17e1",
   "metadata": {},
   "source": [
    "# 2nd Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25c69b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_segments_by_epoch = torch.load('./1st_results_ETTh1/train_segments_by_epoch.pt', weights_only=False)\n",
    "val_segments_by_epoch = torch.load('./1st_results_ETTh1/val_segments_by_epoch.pt', weights_only=False)\n",
    "test_segments_by_epoch = torch.load('./1st_results_ETTh1/test_segments_by_epoch.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c6f826b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ES', 1])\n",
      "dict_keys(['train_pred_segments_EES', 'train_label_segments_EES'])\n",
      "dict_keys(['train_pred_segments_E1', 'train_label_segments_E1'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_segments_by_epoch.keys())\n",
    "print(train_segments_by_epoch['ES'].keys())\n",
    "print(train_segments_by_epoch[1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da81e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_pred_segments_ES = train_segments_by_epoch['ES']['train_pred_segments_EES']  \n",
    "train_label_segments_ES = train_segments_by_epoch['ES']['train_label_segments_EES']\n",
    "val_pred_segments_ES = val_segments_by_epoch['ES']['val_pred_segments_EES']  \n",
    "val_label_segments_ES = val_segments_by_epoch['ES']['val_label_segments_EES']\n",
    "test_pred_segments_ES = test_segments_by_epoch['ES']['test_pred_segments_EES']  \n",
    "test_label_segments_ES = test_segments_by_epoch['ES']['test_label_segments_EES']\n",
    "\n",
    "\n",
    "train_pred_segments_E1 = train_segments_by_epoch[1]['train_pred_segments_E1']  \n",
    "train_label_segments_E1 = train_segments_by_epoch[1]['train_label_segments_E1']\n",
    "val_pred_segments_E1 = val_segments_by_epoch[1]['val_pred_segments_E1']  \n",
    "val_label_segments_E1 = val_segments_by_epoch[1]['val_label_segments_E1']\n",
    "test_pred_segments_E1 = test_segments_by_epoch[1]['test_pred_segments_E1']  \n",
    "test_label_segments_E1 = test_segments_by_epoch[1]['test_label_segments_E1']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49859470",
   "metadata": {},
   "source": [
    "## ES->1E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5423b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== Segment 0 ===========\n",
      "[Segment 0] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 0] Training Time = 0.65 sec\n",
      "[Segment 0] GPU Memory Start = 371.98 MB\n",
      "[Segment 0] GPU Memory End   = 372.86 MB\n",
      "[Segment 0] GPU Peak Memory  = 374.04 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 1 ===========\n",
      "[Segment 1] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 1] Training Time = 1.38 sec\n",
      "[Segment 1] GPU Memory Start = 372.86 MB\n",
      "[Segment 1] GPU Memory End   = 372.86 MB\n",
      "[Segment 1] GPU Peak Memory  = 374.30 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 2 ===========\n",
      "[Segment 2] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 2] Training Time = 1.11 sec\n",
      "[Segment 2] GPU Memory Start = 372.86 MB\n",
      "[Segment 2] GPU Memory End   = 373.13 MB\n",
      "[Segment 2] GPU Peak Memory  = 374.31 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 3 ===========\n",
      "[Segment 3] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 3] Training Time = 0.54 sec\n",
      "[Segment 3] GPU Memory Start = 373.13 MB\n",
      "[Segment 3] GPU Memory End   = 373.40 MB\n",
      "[Segment 3] GPU Peak Memory  = 374.58 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 4 ===========\n",
      "[Segment 4] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 4] Training Time = 1.32 sec\n",
      "[Segment 4] GPU Memory Start = 373.40 MB\n",
      "[Segment 4] GPU Memory End   = 373.67 MB\n",
      "[Segment 4] GPU Peak Memory  = 374.86 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 5 ===========\n",
      "[Segment 5] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 5] Training Time = 1.27 sec\n",
      "[Segment 5] GPU Memory Start = 373.67 MB\n",
      "[Segment 5] GPU Memory End   = 373.94 MB\n",
      "[Segment 5] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 6 ===========\n",
      "[Segment 6] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 6] Training Time = 1.19 sec\n",
      "[Segment 6] GPU Memory Start = 373.94 MB\n",
      "[Segment 6] GPU Memory End   = 374.22 MB\n",
      "[Segment 6] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 7 ===========\n",
      "[Segment 7] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 7] Training Time = 1.32 sec\n",
      "[Segment 7] GPU Memory Start = 374.22 MB\n",
      "[Segment 7] GPU Memory End   = 374.49 MB\n",
      "[Segment 7] GPU Peak Memory  = 375.67 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 8 ===========\n",
      "[Segment 8] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 8] Training Time = 1.33 sec\n",
      "[Segment 8] GPU Memory Start = 374.49 MB\n",
      "[Segment 8] GPU Memory End   = 374.76 MB\n",
      "[Segment 8] GPU Peak Memory  = 375.94 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 9 ===========\n",
      "[Segment 9] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 9] Training Time = 0.78 sec\n",
      "[Segment 9] GPU Memory Start = 374.76 MB\n",
      "[Segment 9] GPU Memory End   = 375.03 MB\n",
      "[Segment 9] GPU Peak Memory  = 376.21 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 10 ===========\n",
      "[Segment 10] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 10] Training Time = 1.41 sec\n",
      "[Segment 10] GPU Memory Start = 375.03 MB\n",
      "[Segment 10] GPU Memory End   = 375.30 MB\n",
      "[Segment 10] GPU Peak Memory  = 376.48 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 11 ===========\n",
      "[Segment 11] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 11] Training Time = 1.42 sec\n",
      "[Segment 11] GPU Memory Start = 372.86 MB\n",
      "[Segment 11] GPU Memory End   = 373.13 MB\n",
      "[Segment 11] GPU Peak Memory  = 374.31 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 12 ===========\n",
      "[Segment 12] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 12] Training Time = 0.57 sec\n",
      "[Segment 12] GPU Memory Start = 373.13 MB\n",
      "[Segment 12] GPU Memory End   = 373.40 MB\n",
      "[Segment 12] GPU Peak Memory  = 374.58 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 13 ===========\n",
      "[Segment 13] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 13] Training Time = 1.41 sec\n",
      "[Segment 13] GPU Memory Start = 373.40 MB\n",
      "[Segment 13] GPU Memory End   = 373.67 MB\n",
      "[Segment 13] GPU Peak Memory  = 374.86 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 14 ===========\n",
      "[Segment 14] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 14] Training Time = 1.42 sec\n",
      "[Segment 14] GPU Memory Start = 373.67 MB\n",
      "[Segment 14] GPU Memory End   = 373.94 MB\n",
      "[Segment 14] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 15 ===========\n",
      "[Segment 15] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 15] Training Time = 1.42 sec\n",
      "[Segment 15] GPU Memory Start = 373.94 MB\n",
      "[Segment 15] GPU Memory End   = 374.22 MB\n",
      "[Segment 15] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 16 ===========\n",
      "[Segment 16] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 16] Training Time = 1.31 sec\n",
      "[Segment 16] GPU Memory Start = 374.22 MB\n",
      "[Segment 16] GPU Memory End   = 374.49 MB\n",
      "[Segment 16] GPU Peak Memory  = 375.67 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 17 ===========\n",
      "[Segment 17] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 17] Training Time = 1.43 sec\n",
      "[Segment 17] GPU Memory Start = 374.49 MB\n",
      "[Segment 17] GPU Memory End   = 374.76 MB\n",
      "[Segment 17] GPU Peak Memory  = 375.94 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 18 ===========\n",
      "[Segment 18] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 18] Training Time = 1.42 sec\n",
      "[Segment 18] GPU Memory Start = 374.76 MB\n",
      "[Segment 18] GPU Memory End   = 375.03 MB\n",
      "[Segment 18] GPU Peak Memory  = 376.21 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 19 ===========\n",
      "[Segment 19] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 19] Training Time = 1.40 sec\n",
      "[Segment 19] GPU Memory Start = 375.03 MB\n",
      "[Segment 19] GPU Memory End   = 375.30 MB\n",
      "[Segment 19] GPU Peak Memory  = 376.48 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 20 ===========\n",
      "[Segment 20] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 20] Training Time = 1.41 sec\n",
      "[Segment 20] GPU Memory Start = 375.30 MB\n",
      "[Segment 20] GPU Memory End   = 373.13 MB\n",
      "[Segment 20] GPU Peak Memory  = 375.57 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 21 ===========\n",
      "[Segment 21] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 21] Training Time = 1.44 sec\n",
      "[Segment 21] GPU Memory Start = 373.13 MB\n",
      "[Segment 21] GPU Memory End   = 373.40 MB\n",
      "[Segment 21] GPU Peak Memory  = 374.58 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 22 ===========\n",
      "[Segment 22] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 22] Training Time = 1.42 sec\n",
      "[Segment 22] GPU Memory Start = 373.40 MB\n",
      "[Segment 22] GPU Memory End   = 373.67 MB\n",
      "[Segment 22] GPU Peak Memory  = 374.86 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 23 ===========\n",
      "[Segment 23] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 23] Training Time = 1.30 sec\n",
      "[Segment 23] GPU Memory Start = 373.67 MB\n",
      "[Segment 23] GPU Memory End   = 373.94 MB\n",
      "[Segment 23] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 24 ===========\n",
      "[Segment 24] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 24] Training Time = 1.00 sec\n",
      "[Segment 24] GPU Memory Start = 373.94 MB\n",
      "[Segment 24] GPU Memory End   = 374.22 MB\n",
      "[Segment 24] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 25 ===========\n",
      "[Segment 25] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 25] Training Time = 1.38 sec\n",
      "[Segment 25] GPU Memory Start = 374.22 MB\n",
      "[Segment 25] GPU Memory End   = 374.49 MB\n",
      "[Segment 25] GPU Peak Memory  = 375.67 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 26 ===========\n",
      "[Segment 26] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 26] Training Time = 1.42 sec\n",
      "[Segment 26] GPU Memory Start = 374.49 MB\n",
      "[Segment 26] GPU Memory End   = 374.76 MB\n",
      "[Segment 26] GPU Peak Memory  = 375.94 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 27 ===========\n",
      "[Segment 27] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 27] Training Time = 1.36 sec\n",
      "[Segment 27] GPU Memory Start = 374.76 MB\n",
      "[Segment 27] GPU Memory End   = 375.03 MB\n",
      "[Segment 27] GPU Peak Memory  = 376.21 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 28 ===========\n",
      "[Segment 28] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 28] Training Time = 1.20 sec\n",
      "[Segment 28] GPU Memory Start = 375.03 MB\n",
      "[Segment 28] GPU Memory End   = 375.30 MB\n",
      "[Segment 28] GPU Peak Memory  = 376.48 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 29 ===========\n",
      "[Segment 29] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 29] Training Time = 1.31 sec\n",
      "[Segment 29] GPU Memory Start = 375.30 MB\n",
      "[Segment 29] GPU Memory End   = 375.57 MB\n",
      "[Segment 29] GPU Peak Memory  = 376.75 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 30 ===========\n",
      "[Segment 30] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 30] Training Time = 0.92 sec\n",
      "[Segment 30] GPU Memory Start = 373.13 MB\n",
      "[Segment 30] GPU Memory End   = 373.40 MB\n",
      "[Segment 30] GPU Peak Memory  = 374.58 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 31 ===========\n",
      "[Segment 31] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 31] Training Time = 0.43 sec\n",
      "[Segment 31] GPU Memory Start = 373.40 MB\n",
      "[Segment 31] GPU Memory End   = 373.67 MB\n",
      "[Segment 31] GPU Peak Memory  = 374.86 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 32 ===========\n",
      "[Segment 32] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 32] Training Time = 0.40 sec\n",
      "[Segment 32] GPU Memory Start = 373.67 MB\n",
      "[Segment 32] GPU Memory End   = 373.94 MB\n",
      "[Segment 32] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 33 ===========\n",
      "[Segment 33] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 33] Training Time = 0.42 sec\n",
      "[Segment 33] GPU Memory Start = 373.94 MB\n",
      "[Segment 33] GPU Memory End   = 374.22 MB\n",
      "[Segment 33] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 34 ===========\n",
      "[Segment 34] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 34] Training Time = 0.47 sec\n",
      "[Segment 34] GPU Memory Start = 374.22 MB\n",
      "[Segment 34] GPU Memory End   = 374.49 MB\n",
      "[Segment 34] GPU Peak Memory  = 375.67 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 35 ===========\n",
      "[Segment 35] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 35] Training Time = 0.40 sec\n",
      "[Segment 35] GPU Memory Start = 374.49 MB\n",
      "[Segment 35] GPU Memory End   = 374.76 MB\n",
      "[Segment 35] GPU Peak Memory  = 375.94 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 36 ===========\n",
      "[Segment 36] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 36] Training Time = 0.46 sec\n",
      "[Segment 36] GPU Memory Start = 374.76 MB\n",
      "[Segment 36] GPU Memory End   = 375.03 MB\n",
      "[Segment 36] GPU Peak Memory  = 376.21 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 37 ===========\n",
      "[Segment 37] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 37] Training Time = 1.41 sec\n",
      "[Segment 37] GPU Memory Start = 375.03 MB\n",
      "[Segment 37] GPU Memory End   = 375.30 MB\n",
      "[Segment 37] GPU Peak Memory  = 376.48 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 38 ===========\n",
      "[Segment 38] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 38] Training Time = 1.36 sec\n",
      "[Segment 38] GPU Memory Start = 375.30 MB\n",
      "[Segment 38] GPU Memory End   = 375.57 MB\n",
      "[Segment 38] GPU Peak Memory  = 376.75 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 39 ===========\n",
      "[Segment 39] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 39] Training Time = 1.85 sec\n",
      "[Segment 39] GPU Memory Start = 375.57 MB\n",
      "[Segment 39] GPU Memory End   = 373.40 MB\n",
      "[Segment 39] GPU Peak Memory  = 375.84 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 40 ===========\n",
      "[Segment 40] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 40] Training Time = 0.70 sec\n",
      "[Segment 40] GPU Memory Start = 373.40 MB\n",
      "[Segment 40] GPU Memory End   = 373.67 MB\n",
      "[Segment 40] GPU Peak Memory  = 374.86 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 41 ===========\n",
      "[Segment 41] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 41] Training Time = 1.25 sec\n",
      "[Segment 41] GPU Memory Start = 373.67 MB\n",
      "[Segment 41] GPU Memory End   = 373.94 MB\n",
      "[Segment 41] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 42 ===========\n",
      "[Segment 42] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 42] Training Time = 1.10 sec\n",
      "[Segment 42] GPU Memory Start = 373.94 MB\n",
      "[Segment 42] GPU Memory End   = 374.22 MB\n",
      "[Segment 42] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 43 ===========\n",
      "[Segment 43] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 43] Training Time = 0.98 sec\n",
      "[Segment 43] GPU Memory Start = 374.22 MB\n",
      "[Segment 43] GPU Memory End   = 374.49 MB\n",
      "[Segment 43] GPU Peak Memory  = 375.67 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 44 ===========\n",
      "[Segment 44] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 44] Training Time = 0.54 sec\n",
      "[Segment 44] GPU Memory Start = 374.49 MB\n",
      "[Segment 44] GPU Memory End   = 374.76 MB\n",
      "[Segment 44] GPU Peak Memory  = 375.94 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 45 ===========\n",
      "[Segment 45] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 45] Training Time = 0.41 sec\n",
      "[Segment 45] GPU Memory Start = 374.76 MB\n",
      "[Segment 45] GPU Memory End   = 375.03 MB\n",
      "[Segment 45] GPU Peak Memory  = 376.21 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 46 ===========\n",
      "[Segment 46] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 46] Training Time = 0.44 sec\n",
      "[Segment 46] GPU Memory Start = 375.03 MB\n",
      "[Segment 46] GPU Memory End   = 375.30 MB\n",
      "[Segment 46] GPU Peak Memory  = 376.48 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 47 ===========\n",
      "[Segment 47] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 47] Training Time = 0.50 sec\n",
      "[Segment 47] GPU Memory Start = 375.30 MB\n",
      "[Segment 47] GPU Memory End   = 375.57 MB\n",
      "[Segment 47] GPU Peak Memory  = 376.75 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 48 ===========\n",
      "[Segment 48] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 48] Training Time = 0.43 sec\n",
      "[Segment 48] GPU Memory Start = 375.57 MB\n",
      "[Segment 48] GPU Memory End   = 375.84 MB\n",
      "[Segment 48] GPU Peak Memory  = 377.02 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 49 ===========\n",
      "[Segment 49] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 49] Training Time = 1.35 sec\n",
      "[Segment 49] GPU Memory Start = 373.40 MB\n",
      "[Segment 49] GPU Memory End   = 373.67 MB\n",
      "[Segment 49] GPU Peak Memory  = 374.86 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 50 ===========\n",
      "[Segment 50] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 50] Training Time = 1.69 sec\n",
      "[Segment 50] GPU Memory Start = 373.67 MB\n",
      "[Segment 50] GPU Memory End   = 373.94 MB\n",
      "[Segment 50] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 51 ===========\n",
      "[Segment 51] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 51] Training Time = 1.67 sec\n",
      "[Segment 51] GPU Memory Start = 373.94 MB\n",
      "[Segment 51] GPU Memory End   = 374.22 MB\n",
      "[Segment 51] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 52 ===========\n",
      "[Segment 52] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 52] Training Time = 1.56 sec\n",
      "[Segment 52] GPU Memory Start = 374.22 MB\n",
      "[Segment 52] GPU Memory End   = 374.49 MB\n",
      "[Segment 52] GPU Peak Memory  = 375.67 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 53 ===========\n",
      "[Segment 53] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 53] Training Time = 1.54 sec\n",
      "[Segment 53] GPU Memory Start = 374.49 MB\n",
      "[Segment 53] GPU Memory End   = 374.76 MB\n",
      "[Segment 53] GPU Peak Memory  = 375.94 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 54 ===========\n",
      "[Segment 54] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 54] Training Time = 1.63 sec\n",
      "[Segment 54] GPU Memory Start = 374.76 MB\n",
      "[Segment 54] GPU Memory End   = 375.03 MB\n",
      "[Segment 54] GPU Peak Memory  = 376.21 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 55 ===========\n",
      "[Segment 55] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 55] Training Time = 1.62 sec\n",
      "[Segment 55] GPU Memory Start = 375.03 MB\n",
      "[Segment 55] GPU Memory End   = 375.30 MB\n",
      "[Segment 55] GPU Peak Memory  = 376.48 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 56 ===========\n",
      "[Segment 56] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 56] Training Time = 1.61 sec\n",
      "[Segment 56] GPU Memory Start = 375.30 MB\n",
      "[Segment 56] GPU Memory End   = 375.57 MB\n",
      "[Segment 56] GPU Peak Memory  = 376.75 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 57 ===========\n",
      "[Segment 57] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 57] Training Time = 1.56 sec\n",
      "[Segment 57] GPU Memory Start = 375.57 MB\n",
      "[Segment 57] GPU Memory End   = 375.84 MB\n",
      "[Segment 57] GPU Peak Memory  = 377.02 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 58 ===========\n",
      "[Segment 58] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 58] Training Time = 1.43 sec\n",
      "[Segment 58] GPU Memory Start = 375.84 MB\n",
      "[Segment 58] GPU Memory End   = 373.67 MB\n",
      "[Segment 58] GPU Peak Memory  = 376.11 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 59 ===========\n",
      "[Segment 59] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 59] Training Time = 1.52 sec\n",
      "[Segment 59] GPU Memory Start = 373.67 MB\n",
      "[Segment 59] GPU Memory End   = 373.94 MB\n",
      "[Segment 59] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 60 ===========\n",
      "[Segment 60] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 60] Training Time = 1.57 sec\n",
      "[Segment 60] GPU Memory Start = 373.94 MB\n",
      "[Segment 60] GPU Memory End   = 374.22 MB\n",
      "[Segment 60] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 61 ===========\n",
      "[Segment 61] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 61] Training Time = 1.19 sec\n",
      "[Segment 61] GPU Memory Start = 374.22 MB\n",
      "[Segment 61] GPU Memory End   = 374.49 MB\n",
      "[Segment 61] GPU Peak Memory  = 375.67 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 62 ===========\n",
      "[Segment 62] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 62] Training Time = 1.56 sec\n",
      "[Segment 62] GPU Memory Start = 374.49 MB\n",
      "[Segment 62] GPU Memory End   = 374.76 MB\n",
      "[Segment 62] GPU Peak Memory  = 375.94 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 63 ===========\n",
      "[Segment 63] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 63] Training Time = 1.61 sec\n",
      "[Segment 63] GPU Memory Start = 374.76 MB\n",
      "[Segment 63] GPU Memory End   = 375.03 MB\n",
      "[Segment 63] GPU Peak Memory  = 376.21 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 64 ===========\n",
      "[Segment 64] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 64] Training Time = 0.77 sec\n",
      "[Segment 64] GPU Memory Start = 375.03 MB\n",
      "[Segment 64] GPU Memory End   = 375.30 MB\n",
      "[Segment 64] GPU Peak Memory  = 376.48 MB\n",
      "\n",
      "============= Done =============\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np, os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "epo = 'ES'\n",
    "\n",
    "class Configs:\n",
    "    def __init__(self, seq_len, pred_len, enc_in, individual,\n",
    "                 learning_rate, lradj, patience,\n",
    "                 save_path, attach_to_trend, attach_to_seasonal):\n",
    "        self.seq_len  = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.enc_in   = enc_in          \n",
    "        self.individual = individual\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lradj = lradj\n",
    "        self.patience = patience\n",
    "        self.save_path = save_path\n",
    "        self.attach_to_trend    = attach_to_trend\n",
    "        self.attach_to_seasonal = attach_to_seasonal\n",
    "\n",
    "\n",
    "\n",
    "base_cfg = Configs(\n",
    "    seq_len          = 336 + pred_len//3,\n",
    "    pred_len         = 96,\n",
    "    enc_in           = 1,\n",
    "    individual       = True,\n",
    "    learning_rate    = 0.005,\n",
    "    lradj            = 'type1',\n",
    "    patience         = 3,\n",
    "    save_path        = f\"./model2_ETTh1_E{epo}_1E\",\n",
    "    attach_to_trend  = True,\n",
    "    attach_to_seasonal = True\n",
    ")\n",
    "Path(base_cfg.save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "for seg in range(len(train_pred_segments_ES[0])):\n",
    "    print(f\"\\n=========== Segment {seg} ===========\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    mem_start = torch.cuda.memory_allocated(device)\n",
    "\n",
    "    \n",
    "    tr_pv = train_pred_segments_ES[:, seg, :, :]   \n",
    "    va_pv = val_pred_segments_ES[:,  seg, :, :]\n",
    "\n",
    "    tr_ds = TensorDataset(train_sequences, train_labels, tr_pv)\n",
    "    va_ds = TensorDataset(val_sequences,   val_labels,   va_pv)\n",
    "    tr_dl = DataLoader(tr_ds, batch_size=32, shuffle=True)\n",
    "    va_dl = DataLoader(va_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "    \n",
    "    model = Model(base_cfg).to(device)\n",
    "    C     = model.channels                                    \n",
    "\n",
    "    opts = []\n",
    "    for c in range(C):\n",
    "        params = list(model.Linear_Seasonal[c].parameters()) + \\\n",
    "                 list(model.Linear_Trend[c].parameters())\n",
    "        opts.append(optim.Adam(params, lr=base_cfg.learning_rate))\n",
    "        path = Path(base_cfg.save_path) / f\"seg{seg}\" / f\"ch{c}\"\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    active = set(range(C))            \n",
    "\n",
    "    \n",
    "    for epoch in range(1, 2):\n",
    "        if not active: break          \n",
    "\n",
    "        \n",
    "        model.train()\n",
    "        for c in active:                              \n",
    "            if base_cfg.lradj=='type1':\n",
    "                lr = base_cfg.learning_rate * 0.5**((epoch-1)//1)\n",
    "                for g in opts[c].param_groups: g['lr'] = lr\n",
    "\n",
    "        printed_tensor_info = False  \n",
    "\n",
    "        for xb, yb, pv in tr_dl:\n",
    "            xb, yb, pv = xb.float().to(device), yb.float().to(device), pv.float().to(device)\n",
    "\n",
    "            if not printed_tensor_info:\n",
    "                \n",
    "                xb_size = xb.numel() * xb.element_size() / 1024**2\n",
    "                yb_size = yb.numel() * yb.element_size() / 1024**2\n",
    "                pv_size = pv.numel() * pv.element_size() / 1024**2\n",
    "\n",
    "                \n",
    "                \n",
    "                num_batches = len(tr_dl)\n",
    "\n",
    "                \n",
    "                total_xb_MB = num_batches * xb_size\n",
    "                total_yb_MB = num_batches * yb_size\n",
    "                total_pv_MB = num_batches * pv_size\n",
    "                \n",
    "\n",
    "\n",
    "                print(f\"[Segment {seg}] Batch Size Info:\")\n",
    "                print(f\"  xb per batch = {xb_size:.4f} MB, total = {total_xb_MB:.4f} MB\")\n",
    "                print(f\"  yb per batch = {yb_size:.4f} MB, total = {total_yb_MB:.4f} MB\")\n",
    "                print(f\"  pv per batch = {pv_size:.4f} MB, total = {total_pv_MB:.4f} MB\")\n",
    "\n",
    "                printed_tensor_info = True\n",
    "\n",
    "\n",
    "            out = model(xb, pv)                       \n",
    "\n",
    "            loss_list = []\n",
    "            for c in active:\n",
    "                pred = out[:, -base_cfg.pred_len:, c]\n",
    "                tgt  = yb[:, -base_cfg.pred_len:, c]\n",
    "                loss_list.append(loss_fn(pred, tgt))\n",
    "\n",
    "            total = torch.stack(loss_list).sum()\n",
    "            for opt in opts:\n",
    "                if opt: opt.zero_grad()\n",
    "            total.backward()\n",
    "            for c in active:\n",
    "                opts[c].step()\n",
    "        \n",
    "    elapsed = time.time() - start_time\n",
    "    mem_end = torch.cuda.memory_allocated(device)\n",
    "    peak_mem = torch.cuda.max_memory_allocated(device)\n",
    "\n",
    "\n",
    "    print(f\"[Segment {seg}] Training Time = {elapsed:.2f} sec\")\n",
    "    print(f\"[Segment {seg}] GPU Memory Start = {mem_start/1024**2:.2f} MB\")\n",
    "    print(f\"[Segment {seg}] GPU Memory End   = {mem_end/1024**2:.2f} MB\")\n",
    "    print(f\"[Segment {seg}] GPU Peak Memory  = {peak_mem/1024**2:.2f} MB\")\n",
    "\n",
    "    print(f\"\\n============= Done =============\")\n",
    "\n",
    "    \n",
    "    torch.save(model.state_dict(),\n",
    "                Path(base_cfg.save_path) / f\"seg{seg}\" / f\"ch{c}\" / \"checkpoint.pth\")\n",
    "    del model, opts; torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06cec21",
   "metadata": {},
   "source": [
    "## 1E->1E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab371e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== Segment 0 ===========\n",
      "[Segment 0] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 0] Training Time = 1.66 sec\n",
      "[Segment 0] GPU Memory Start = 373.67 MB\n",
      "[Segment 0] GPU Memory End   = 373.67 MB\n",
      "[Segment 0] GPU Peak Memory  = 375.11 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 1 ===========\n",
      "[Segment 1] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 1] Training Time = 1.40 sec\n",
      "[Segment 1] GPU Memory Start = 373.67 MB\n",
      "[Segment 1] GPU Memory End   = 373.94 MB\n",
      "[Segment 1] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 2 ===========\n",
      "[Segment 2] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 2] Training Time = 1.38 sec\n",
      "[Segment 2] GPU Memory Start = 373.94 MB\n",
      "[Segment 2] GPU Memory End   = 374.22 MB\n",
      "[Segment 2] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 3 ===========\n",
      "[Segment 3] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 3] Training Time = 0.76 sec\n",
      "[Segment 3] GPU Memory Start = 374.22 MB\n",
      "[Segment 3] GPU Memory End   = 373.67 MB\n",
      "[Segment 3] GPU Peak Memory  = 375.11 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 4 ===========\n",
      "[Segment 4] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 4] Training Time = 1.58 sec\n",
      "[Segment 4] GPU Memory Start = 373.67 MB\n",
      "[Segment 4] GPU Memory End   = 373.94 MB\n",
      "[Segment 4] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 5 ===========\n",
      "[Segment 5] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 5] Training Time = 1.48 sec\n",
      "[Segment 5] GPU Memory Start = 373.94 MB\n",
      "[Segment 5] GPU Memory End   = 374.22 MB\n",
      "[Segment 5] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 6 ===========\n",
      "[Segment 6] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 6] Training Time = 1.37 sec\n",
      "[Segment 6] GPU Memory Start = 374.22 MB\n",
      "[Segment 6] GPU Memory End   = 374.49 MB\n",
      "[Segment 6] GPU Peak Memory  = 375.67 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 7 ===========\n",
      "[Segment 7] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 7] Training Time = 1.52 sec\n",
      "[Segment 7] GPU Memory Start = 374.49 MB\n",
      "[Segment 7] GPU Memory End   = 374.76 MB\n",
      "[Segment 7] GPU Peak Memory  = 375.94 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 8 ===========\n",
      "[Segment 8] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 8] Training Time = 1.56 sec\n",
      "[Segment 8] GPU Memory Start = 374.76 MB\n",
      "[Segment 8] GPU Memory End   = 375.03 MB\n",
      "[Segment 8] GPU Peak Memory  = 376.21 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 9 ===========\n",
      "[Segment 9] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 9] Training Time = 0.60 sec\n",
      "[Segment 9] GPU Memory Start = 375.03 MB\n",
      "[Segment 9] GPU Memory End   = 375.30 MB\n",
      "[Segment 9] GPU Peak Memory  = 376.48 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 10 ===========\n",
      "[Segment 10] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 10] Training Time = 1.34 sec\n",
      "[Segment 10] GPU Memory Start = 375.30 MB\n",
      "[Segment 10] GPU Memory End   = 375.57 MB\n",
      "[Segment 10] GPU Peak Memory  = 376.75 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 11 ===========\n",
      "[Segment 11] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 11] Training Time = 1.44 sec\n",
      "[Segment 11] GPU Memory Start = 375.57 MB\n",
      "[Segment 11] GPU Memory End   = 375.84 MB\n",
      "[Segment 11] GPU Peak Memory  = 377.02 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 12 ===========\n",
      "[Segment 12] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 12] Training Time = 1.19 sec\n",
      "[Segment 12] GPU Memory Start = 375.84 MB\n",
      "[Segment 12] GPU Memory End   = 376.11 MB\n",
      "[Segment 12] GPU Peak Memory  = 377.29 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 13 ===========\n",
      "[Segment 13] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 13] Training Time = 1.34 sec\n",
      "[Segment 13] GPU Memory Start = 372.86 MB\n",
      "[Segment 13] GPU Memory End   = 373.13 MB\n",
      "[Segment 13] GPU Peak Memory  = 374.31 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 14 ===========\n",
      "[Segment 14] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 14] Training Time = 1.31 sec\n",
      "[Segment 14] GPU Memory Start = 373.13 MB\n",
      "[Segment 14] GPU Memory End   = 373.40 MB\n",
      "[Segment 14] GPU Peak Memory  = 374.58 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 15 ===========\n",
      "[Segment 15] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 15] Training Time = 1.36 sec\n",
      "[Segment 15] GPU Memory Start = 373.40 MB\n",
      "[Segment 15] GPU Memory End   = 373.67 MB\n",
      "[Segment 15] GPU Peak Memory  = 374.86 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 16 ===========\n",
      "[Segment 16] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 16] Training Time = 1.37 sec\n",
      "[Segment 16] GPU Memory Start = 373.67 MB\n",
      "[Segment 16] GPU Memory End   = 373.94 MB\n",
      "[Segment 16] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 17 ===========\n",
      "[Segment 17] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 17] Training Time = 0.59 sec\n",
      "[Segment 17] GPU Memory Start = 373.94 MB\n",
      "[Segment 17] GPU Memory End   = 374.22 MB\n",
      "[Segment 17] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 18 ===========\n",
      "[Segment 18] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 18] Training Time = 0.58 sec\n",
      "[Segment 18] GPU Memory Start = 374.22 MB\n",
      "[Segment 18] GPU Memory End   = 374.49 MB\n",
      "[Segment 18] GPU Peak Memory  = 375.67 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 19 ===========\n",
      "[Segment 19] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 19] Training Time = 0.58 sec\n",
      "[Segment 19] GPU Memory Start = 374.49 MB\n",
      "[Segment 19] GPU Memory End   = 374.76 MB\n",
      "[Segment 19] GPU Peak Memory  = 375.94 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 20 ===========\n",
      "[Segment 20] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 20] Training Time = 0.64 sec\n",
      "[Segment 20] GPU Memory Start = 374.76 MB\n",
      "[Segment 20] GPU Memory End   = 375.03 MB\n",
      "[Segment 20] GPU Peak Memory  = 376.21 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 21 ===========\n",
      "[Segment 21] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 21] Training Time = 1.35 sec\n",
      "[Segment 21] GPU Memory Start = 375.03 MB\n",
      "[Segment 21] GPU Memory End   = 375.30 MB\n",
      "[Segment 21] GPU Peak Memory  = 376.48 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 22 ===========\n",
      "[Segment 22] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 22] Training Time = 1.42 sec\n",
      "[Segment 22] GPU Memory Start = 375.30 MB\n",
      "[Segment 22] GPU Memory End   = 373.13 MB\n",
      "[Segment 22] GPU Peak Memory  = 375.57 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 23 ===========\n",
      "[Segment 23] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 23] Training Time = 0.57 sec\n",
      "[Segment 23] GPU Memory Start = 373.13 MB\n",
      "[Segment 23] GPU Memory End   = 373.40 MB\n",
      "[Segment 23] GPU Peak Memory  = 374.58 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 24 ===========\n",
      "[Segment 24] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 24] Training Time = 1.42 sec\n",
      "[Segment 24] GPU Memory Start = 373.40 MB\n",
      "[Segment 24] GPU Memory End   = 373.67 MB\n",
      "[Segment 24] GPU Peak Memory  = 374.86 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 25 ===========\n",
      "[Segment 25] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 25] Training Time = 1.39 sec\n",
      "[Segment 25] GPU Memory Start = 373.67 MB\n",
      "[Segment 25] GPU Memory End   = 373.94 MB\n",
      "[Segment 25] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 26 ===========\n",
      "[Segment 26] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 26] Training Time = 0.58 sec\n",
      "[Segment 26] GPU Memory Start = 373.94 MB\n",
      "[Segment 26] GPU Memory End   = 374.22 MB\n",
      "[Segment 26] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 27 ===========\n",
      "[Segment 27] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 27] Training Time = 1.22 sec\n",
      "[Segment 27] GPU Memory Start = 374.22 MB\n",
      "[Segment 27] GPU Memory End   = 374.49 MB\n",
      "[Segment 27] GPU Peak Memory  = 375.67 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 28 ===========\n",
      "[Segment 28] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 28] Training Time = 1.22 sec\n",
      "[Segment 28] GPU Memory Start = 374.49 MB\n",
      "[Segment 28] GPU Memory End   = 374.76 MB\n",
      "[Segment 28] GPU Peak Memory  = 375.94 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 29 ===========\n",
      "[Segment 29] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 29] Training Time = 0.58 sec\n",
      "[Segment 29] GPU Memory Start = 374.76 MB\n",
      "[Segment 29] GPU Memory End   = 375.03 MB\n",
      "[Segment 29] GPU Peak Memory  = 376.21 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 30 ===========\n",
      "[Segment 30] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 30] Training Time = 1.37 sec\n",
      "[Segment 30] GPU Memory Start = 375.03 MB\n",
      "[Segment 30] GPU Memory End   = 375.30 MB\n",
      "[Segment 30] GPU Peak Memory  = 376.48 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 31 ===========\n",
      "[Segment 31] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 31] Training Time = 0.92 sec\n",
      "[Segment 31] GPU Memory Start = 375.30 MB\n",
      "[Segment 31] GPU Memory End   = 375.57 MB\n",
      "[Segment 31] GPU Peak Memory  = 376.75 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 32 ===========\n",
      "[Segment 32] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 32] Training Time = 0.59 sec\n",
      "[Segment 32] GPU Memory Start = 373.13 MB\n",
      "[Segment 32] GPU Memory End   = 373.40 MB\n",
      "[Segment 32] GPU Peak Memory  = 374.58 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 33 ===========\n",
      "[Segment 33] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 33] Training Time = 0.58 sec\n",
      "[Segment 33] GPU Memory Start = 373.40 MB\n",
      "[Segment 33] GPU Memory End   = 373.67 MB\n",
      "[Segment 33] GPU Peak Memory  = 374.86 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 34 ===========\n",
      "[Segment 34] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 34] Training Time = 0.58 sec\n",
      "[Segment 34] GPU Memory Start = 373.67 MB\n",
      "[Segment 34] GPU Memory End   = 373.94 MB\n",
      "[Segment 34] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 35 ===========\n",
      "[Segment 35] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 35] Training Time = 1.33 sec\n",
      "[Segment 35] GPU Memory Start = 373.94 MB\n",
      "[Segment 35] GPU Memory End   = 374.22 MB\n",
      "[Segment 35] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 36 ===========\n",
      "[Segment 36] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 36] Training Time = 1.35 sec\n",
      "[Segment 36] GPU Memory Start = 374.22 MB\n",
      "[Segment 36] GPU Memory End   = 374.49 MB\n",
      "[Segment 36] GPU Peak Memory  = 375.67 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 37 ===========\n",
      "[Segment 37] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 37] Training Time = 0.58 sec\n",
      "[Segment 37] GPU Memory Start = 374.49 MB\n",
      "[Segment 37] GPU Memory End   = 374.76 MB\n",
      "[Segment 37] GPU Peak Memory  = 375.94 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 38 ===========\n",
      "[Segment 38] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 38] Training Time = 1.32 sec\n",
      "[Segment 38] GPU Memory Start = 374.76 MB\n",
      "[Segment 38] GPU Memory End   = 375.03 MB\n",
      "[Segment 38] GPU Peak Memory  = 376.21 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 39 ===========\n",
      "[Segment 39] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 39] Training Time = 0.58 sec\n",
      "[Segment 39] GPU Memory Start = 375.03 MB\n",
      "[Segment 39] GPU Memory End   = 375.30 MB\n",
      "[Segment 39] GPU Peak Memory  = 376.48 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 40 ===========\n",
      "[Segment 40] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 40] Training Time = 0.59 sec\n",
      "[Segment 40] GPU Memory Start = 375.30 MB\n",
      "[Segment 40] GPU Memory End   = 375.57 MB\n",
      "[Segment 40] GPU Peak Memory  = 376.75 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 41 ===========\n",
      "[Segment 41] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 41] Training Time = 1.28 sec\n",
      "[Segment 41] GPU Memory Start = 375.57 MB\n",
      "[Segment 41] GPU Memory End   = 373.40 MB\n",
      "[Segment 41] GPU Peak Memory  = 375.84 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 42 ===========\n",
      "[Segment 42] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 42] Training Time = 1.37 sec\n",
      "[Segment 42] GPU Memory Start = 373.40 MB\n",
      "[Segment 42] GPU Memory End   = 373.67 MB\n",
      "[Segment 42] GPU Peak Memory  = 374.86 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 43 ===========\n",
      "[Segment 43] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 43] Training Time = 0.58 sec\n",
      "[Segment 43] GPU Memory Start = 373.67 MB\n",
      "[Segment 43] GPU Memory End   = 373.94 MB\n",
      "[Segment 43] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 44 ===========\n",
      "[Segment 44] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 44] Training Time = 0.97 sec\n",
      "[Segment 44] GPU Memory Start = 373.94 MB\n",
      "[Segment 44] GPU Memory End   = 374.22 MB\n",
      "[Segment 44] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 45 ===========\n",
      "[Segment 45] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 45] Training Time = 0.58 sec\n",
      "[Segment 45] GPU Memory Start = 374.22 MB\n",
      "[Segment 45] GPU Memory End   = 374.49 MB\n",
      "[Segment 45] GPU Peak Memory  = 375.67 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 46 ===========\n",
      "[Segment 46] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 46] Training Time = 1.03 sec\n",
      "[Segment 46] GPU Memory Start = 374.49 MB\n",
      "[Segment 46] GPU Memory End   = 374.76 MB\n",
      "[Segment 46] GPU Peak Memory  = 375.94 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 47 ===========\n",
      "[Segment 47] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 47] Training Time = 1.37 sec\n",
      "[Segment 47] GPU Memory Start = 374.76 MB\n",
      "[Segment 47] GPU Memory End   = 375.03 MB\n",
      "[Segment 47] GPU Peak Memory  = 376.21 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 48 ===========\n",
      "[Segment 48] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 48] Training Time = 1.37 sec\n",
      "[Segment 48] GPU Memory Start = 375.03 MB\n",
      "[Segment 48] GPU Memory End   = 375.30 MB\n",
      "[Segment 48] GPU Peak Memory  = 376.48 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 49 ===========\n",
      "[Segment 49] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 49] Training Time = 1.41 sec\n",
      "[Segment 49] GPU Memory Start = 375.30 MB\n",
      "[Segment 49] GPU Memory End   = 375.57 MB\n",
      "[Segment 49] GPU Peak Memory  = 376.75 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 50 ===========\n",
      "[Segment 50] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 50] Training Time = 0.95 sec\n",
      "[Segment 50] GPU Memory Start = 375.57 MB\n",
      "[Segment 50] GPU Memory End   = 375.84 MB\n",
      "[Segment 50] GPU Peak Memory  = 377.02 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 51 ===========\n",
      "[Segment 51] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 51] Training Time = 1.40 sec\n",
      "[Segment 51] GPU Memory Start = 373.40 MB\n",
      "[Segment 51] GPU Memory End   = 373.67 MB\n",
      "[Segment 51] GPU Peak Memory  = 374.86 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 52 ===========\n",
      "[Segment 52] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 52] Training Time = 1.42 sec\n",
      "[Segment 52] GPU Memory Start = 373.67 MB\n",
      "[Segment 52] GPU Memory End   = 373.94 MB\n",
      "[Segment 52] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 53 ===========\n",
      "[Segment 53] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 53] Training Time = 0.58 sec\n",
      "[Segment 53] GPU Memory Start = 373.94 MB\n",
      "[Segment 53] GPU Memory End   = 374.22 MB\n",
      "[Segment 53] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 54 ===========\n",
      "[Segment 54] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 54] Training Time = 0.67 sec\n",
      "[Segment 54] GPU Memory Start = 374.22 MB\n",
      "[Segment 54] GPU Memory End   = 374.49 MB\n",
      "[Segment 54] GPU Peak Memory  = 375.67 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 55 ===========\n",
      "[Segment 55] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 55] Training Time = 1.05 sec\n",
      "[Segment 55] GPU Memory Start = 374.49 MB\n",
      "[Segment 55] GPU Memory End   = 374.76 MB\n",
      "[Segment 55] GPU Peak Memory  = 375.94 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 56 ===========\n",
      "[Segment 56] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 56] Training Time = 1.30 sec\n",
      "[Segment 56] GPU Memory Start = 374.76 MB\n",
      "[Segment 56] GPU Memory End   = 375.03 MB\n",
      "[Segment 56] GPU Peak Memory  = 376.21 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 57 ===========\n",
      "[Segment 57] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 57] Training Time = 0.59 sec\n",
      "[Segment 57] GPU Memory Start = 375.03 MB\n",
      "[Segment 57] GPU Memory End   = 375.30 MB\n",
      "[Segment 57] GPU Peak Memory  = 376.48 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 58 ===========\n",
      "[Segment 58] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 58] Training Time = 1.38 sec\n",
      "[Segment 58] GPU Memory Start = 375.30 MB\n",
      "[Segment 58] GPU Memory End   = 375.57 MB\n",
      "[Segment 58] GPU Peak Memory  = 376.75 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 59 ===========\n",
      "[Segment 59] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 59] Training Time = 1.31 sec\n",
      "[Segment 59] GPU Memory Start = 375.57 MB\n",
      "[Segment 59] GPU Memory End   = 375.84 MB\n",
      "[Segment 59] GPU Peak Memory  = 377.02 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 60 ===========\n",
      "[Segment 60] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 60] Training Time = 1.40 sec\n",
      "[Segment 60] GPU Memory Start = 375.84 MB\n",
      "[Segment 60] GPU Memory End   = 373.67 MB\n",
      "[Segment 60] GPU Peak Memory  = 376.11 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 61 ===========\n",
      "[Segment 61] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 61] Training Time = 0.58 sec\n",
      "[Segment 61] GPU Memory Start = 373.67 MB\n",
      "[Segment 61] GPU Memory End   = 373.94 MB\n",
      "[Segment 61] GPU Peak Memory  = 375.13 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 62 ===========\n",
      "[Segment 62] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 62] Training Time = 0.57 sec\n",
      "[Segment 62] GPU Memory Start = 373.94 MB\n",
      "[Segment 62] GPU Memory End   = 374.22 MB\n",
      "[Segment 62] GPU Peak Memory  = 375.40 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 63 ===========\n",
      "[Segment 63] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 63] Training Time = 0.57 sec\n",
      "[Segment 63] GPU Memory Start = 374.22 MB\n",
      "[Segment 63] GPU Memory End   = 374.49 MB\n",
      "[Segment 63] GPU Peak Memory  = 375.67 MB\n",
      "\n",
      "============= Done =============\n",
      "\n",
      "=========== Segment 64 ===========\n",
      "[Segment 64] Batch Size Info:\n",
      "  xb per batch = 0.0410 MB, total = 10.5410 MB\n",
      "  yb per batch = 0.0117 MB, total = 3.0117 MB\n",
      "  pv per batch = 0.0039 MB, total = 1.0039 MB\n",
      "[Segment 64] Training Time = 0.57 sec\n",
      "[Segment 64] GPU Memory Start = 374.49 MB\n",
      "[Segment 64] GPU Memory End   = 374.76 MB\n",
      "[Segment 64] GPU Peak Memory  = 375.94 MB\n",
      "\n",
      "============= Done =============\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np, os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "epo = 1\n",
    "\n",
    "\n",
    "class Configs:\n",
    "    def __init__(self, seq_len, pred_len, enc_in, individual,\n",
    "                 learning_rate, lradj, patience,\n",
    "                 save_path, attach_to_trend, attach_to_seasonal):\n",
    "        self.seq_len  = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.enc_in   = enc_in          \n",
    "        self.individual = individual\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lradj = lradj\n",
    "        self.patience = patience\n",
    "        self.save_path = save_path\n",
    "        self.attach_to_trend    = attach_to_trend\n",
    "        self.attach_to_seasonal = attach_to_seasonal\n",
    "\n",
    "\n",
    "\n",
    "base_cfg = Configs(\n",
    "    seq_len          = 336 + pred_len//3,\n",
    "    pred_len         = 96,\n",
    "    enc_in           = 1,\n",
    "    individual       = True,\n",
    "    learning_rate    = 0.005,\n",
    "    lradj            = 'type1',\n",
    "    patience         = 3,\n",
    "    save_path        = f\"./model2_ETTh1_E{epo}_1E/\",\n",
    "    attach_to_trend  = True,\n",
    "    attach_to_seasonal = True\n",
    ")\n",
    "Path(base_cfg.save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "for seg in range(len(train_pred_segments_E1[0])):\n",
    "    print(f\"\\n=========== Segment {seg} ===========\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    mem_start = torch.cuda.memory_allocated(device)\n",
    "\n",
    "    \n",
    "    tr_pv = train_pred_segments_E1[:, seg, :, :]   \n",
    "    va_pv = val_pred_segments_E1[:,  seg, :, :]\n",
    "\n",
    "    tr_ds = TensorDataset(train_sequences, train_labels, tr_pv)\n",
    "    va_ds = TensorDataset(val_sequences,   val_labels,   va_pv)\n",
    "    tr_dl = DataLoader(tr_ds, batch_size=32, shuffle=True)\n",
    "    va_dl = DataLoader(va_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "    \n",
    "    model = Model(base_cfg).to(device)\n",
    "    C     = model.channels                                    \n",
    "\n",
    "    opts = []\n",
    "    for c in range(C):\n",
    "        params = list(model.Linear_Seasonal[c].parameters()) + \\\n",
    "                 list(model.Linear_Trend[c].parameters())\n",
    "        opts.append(optim.Adam(params, lr=base_cfg.learning_rate))\n",
    "        path = Path(base_cfg.save_path) / f\"seg{seg}\" / f\"ch{c}\"\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    active = set(range(C))            \n",
    "\n",
    "    \n",
    "    for epoch in range(1, 2):\n",
    "        if not active: break          \n",
    "\n",
    "        \n",
    "        model.train()\n",
    "        for c in active:                              \n",
    "            if base_cfg.lradj=='type1':\n",
    "                lr = base_cfg.learning_rate * 0.5**((epoch-1)//1)\n",
    "                for g in opts[c].param_groups: g['lr'] = lr\n",
    "\n",
    "        printed_tensor_info = False\n",
    "        \n",
    "        for xb, yb, pv in tr_dl:\n",
    "            xb, yb, pv = xb.float().to(device), yb.float().to(device), pv.float().to(device)\n",
    "\n",
    "            if not printed_tensor_info:\n",
    "                \n",
    "                xb_size = xb.numel() * xb.element_size() / 1024**2\n",
    "                yb_size = yb.numel() * yb.element_size() / 1024**2\n",
    "                pv_size = pv.numel() * pv.element_size() / 1024**2\n",
    "\n",
    "                \n",
    "                \n",
    "                num_batches = len(tr_dl)\n",
    "\n",
    "                \n",
    "                total_xb_MB = num_batches * xb_size\n",
    "                total_yb_MB = num_batches * yb_size\n",
    "                total_pv_MB = num_batches * pv_size\n",
    "                \n",
    "\n",
    "\n",
    "                print(f\"[Segment {seg}] Batch Size Info:\")\n",
    "                print(f\"  xb per batch = {xb_size:.4f} MB, total = {total_xb_MB:.4f} MB\")\n",
    "                print(f\"  yb per batch = {yb_size:.4f} MB, total = {total_yb_MB:.4f} MB\")\n",
    "                print(f\"  pv per batch = {pv_size:.4f} MB, total = {total_pv_MB:.4f} MB\")\n",
    "\n",
    "                printed_tensor_info = True\n",
    "\n",
    "\n",
    "\n",
    "            out = model(xb, pv)                       \n",
    "\n",
    "            loss_list = []\n",
    "            for c in active:\n",
    "                pred = out[:, -base_cfg.pred_len:, c]\n",
    "                tgt  = yb[:, -base_cfg.pred_len:, c]\n",
    "                loss_list.append(loss_fn(pred, tgt))\n",
    "\n",
    "            total = torch.stack(loss_list).sum()\n",
    "            for opt in opts:\n",
    "                if opt: opt.zero_grad()\n",
    "            total.backward()\n",
    "            for c in active:\n",
    "                opts[c].step()\n",
    "\n",
    "        \n",
    "    elapsed = time.time() - start_time\n",
    "    mem_end = torch.cuda.memory_allocated(device)\n",
    "    peak_mem = torch.cuda.max_memory_allocated(device)\n",
    "\n",
    "\n",
    "    print(f\"[Segment {seg}] Training Time = {elapsed:.2f} sec\")\n",
    "    print(f\"[Segment {seg}] GPU Memory Start = {mem_start/1024**2:.2f} MB\")\n",
    "    print(f\"[Segment {seg}] GPU Memory End   = {mem_end/1024**2:.2f} MB\")\n",
    "    print(f\"[Segment {seg}] GPU Peak Memory  = {peak_mem/1024**2:.2f} MB\")\n",
    "\n",
    "    print(f\"\\n============= Done =============\")\n",
    "\n",
    "    \n",
    "    torch.save(model.state_dict(),\n",
    "                Path(base_cfg.save_path) / f\"seg{seg}\" / f\"ch{c}\" / \"checkpoint.pth\")\n",
    "    del model, opts; torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441a975",
   "metadata": {},
   "source": [
    "# 2nd Inference for Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "812dd2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_segments_by_epoch = torch.load('./1st_results_ETTh1/train_segments_by_epoch.pt', weights_only=False)\n",
    "val_segments_by_epoch = torch.load('./1st_results_ETTh1/val_segments_by_epoch.pt', weights_only=False)\n",
    "test_segments_by_epoch = torch.load('./1st_results_ETTh1/test_segments_by_epoch.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e8be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_segments_ES = train_segments_by_epoch['ES']['train_pred_segments_EES']  \n",
    "train_label_segments_ES = train_segments_by_epoch['ES']['train_label_segments_EES']\n",
    "val_pred_segments_ES = val_segments_by_epoch['ES']['val_pred_segments_EES']  \n",
    "val_label_segments_ES = val_segments_by_epoch['ES']['val_label_segments_EES']\n",
    "test_pred_segments_ES = test_segments_by_epoch['ES']['test_pred_segments_EES']  \n",
    "test_label_segments_ES = test_segments_by_epoch['ES']['test_label_segments_EES']\n",
    "\n",
    "\n",
    "train_pred_segments_E1 = train_segments_by_epoch[1]['train_pred_segments_E1']  \n",
    "train_label_segments_E1 = train_segments_by_epoch[1]['train_label_segments_E1']\n",
    "val_pred_segments_E1 = val_segments_by_epoch[1]['val_pred_segments_E1']  \n",
    "val_label_segments_E1 = val_segments_by_epoch[1]['val_label_segments_E1']\n",
    "test_pred_segments_E1 = test_segments_by_epoch[1]['test_pred_segments_E1']  \n",
    "test_label_segments_E1 = test_segments_by_epoch[1]['test_label_segments_E1']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea95428",
   "metadata": {},
   "source": [
    "## ES->1E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b36281",
   "metadata": {},
   "source": [
    "### configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d778b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np, os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class Configs:\n",
    "    def __init__(self, seq_len, pred_len, enc_in, individual,\n",
    "                 learning_rate, lradj, patience,\n",
    "                 save_path, attach_to_trend, attach_to_seasonal):\n",
    "        self.seq_len  = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.enc_in   = enc_in          \n",
    "        self.individual = individual\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lradj = lradj\n",
    "        self.patience = patience\n",
    "        self.save_path = save_path\n",
    "        self.attach_to_trend    = attach_to_trend\n",
    "        self.attach_to_seasonal = attach_to_seasonal\n",
    "\n",
    "\n",
    "base_cfg = Configs(\n",
    "    pred_len         = 96,\n",
    "    seq_len          = 336 + pred_len//3,\n",
    "    enc_in           = 1,\n",
    "    individual       = True,\n",
    "    learning_rate    = 0.005,\n",
    "    lradj            = 'type1',\n",
    "    patience         = 3,\n",
    "    save_path        = \"./model2_ETTh1_EES_1E\",\n",
    "    attach_to_trend  = True,\n",
    "    attach_to_seasonal = True\n",
    ")\n",
    "Path(base_cfg.save_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368013d3",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011493bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "―――― Inference  |  Segment 0 ――――\n",
      "  seg0-ch0  MSE:0.10040894150733948  MAE:0.24522291123867035\n",
      "▶ seg0  mean  MSE:0.10040894150733948  MAE:0.24522291123867035\n",
      "\n",
      "―――― Inference  |  Segment 1 ――――\n",
      "  seg1-ch0  MSE:0.10000165551900864  MAE:0.24709229171276093\n",
      "▶ seg1  mean  MSE:0.10000165551900864  MAE:0.24709229171276093\n",
      "\n",
      "―――― Inference  |  Segment 2 ――――\n",
      "  seg2-ch0  MSE:0.0982840284705162  MAE:0.24378301203250885\n",
      "▶ seg2  mean  MSE:0.0982840284705162  MAE:0.24378301203250885\n",
      "\n",
      "―――― Inference  |  Segment 3 ――――\n",
      "  seg3-ch0  MSE:0.10482393950223923  MAE:0.2550000548362732\n",
      "▶ seg3  mean  MSE:0.10482393950223923  MAE:0.2550000548362732\n",
      "\n",
      "―――― Inference  |  Segment 4 ――――\n",
      "  seg4-ch0  MSE:0.10246667265892029  MAE:0.24980740249156952\n",
      "▶ seg4  mean  MSE:0.10246667265892029  MAE:0.24980740249156952\n",
      "\n",
      "―――― Inference  |  Segment 5 ――――\n",
      "  seg5-ch0  MSE:0.10249224305152893  MAE:0.24976851046085358\n",
      "▶ seg5  mean  MSE:0.10249224305152893  MAE:0.24976851046085358\n",
      "\n",
      "―――― Inference  |  Segment 6 ――――\n",
      "  seg6-ch0  MSE:0.10605795681476593  MAE:0.250335693359375\n",
      "▶ seg6  mean  MSE:0.10605795681476593  MAE:0.250335693359375\n",
      "\n",
      "―――― Inference  |  Segment 7 ――――\n",
      "  seg7-ch0  MSE:0.1020646020770073  MAE:0.24967926740646362\n",
      "▶ seg7  mean  MSE:0.1020646020770073  MAE:0.24967926740646362\n",
      "\n",
      "―――― Inference  |  Segment 8 ――――\n",
      "  seg8-ch0  MSE:0.09757068008184433  MAE:0.2451428472995758\n",
      "▶ seg8  mean  MSE:0.09757068008184433  MAE:0.2451428472995758\n",
      "\n",
      "―――― Inference  |  Segment 9 ――――\n",
      "  seg9-ch0  MSE:0.104825958609581  MAE:0.25732359290122986\n",
      "▶ seg9  mean  MSE:0.104825958609581  MAE:0.25732359290122986\n",
      "\n",
      "―――― Inference  |  Segment 10 ――――\n",
      "  seg10-ch0  MSE:0.10982119292020798  MAE:0.2607320547103882\n",
      "▶ seg10  mean  MSE:0.10982119292020798  MAE:0.2607320547103882\n",
      "\n",
      "―――― Inference  |  Segment 11 ――――\n",
      "  seg11-ch0  MSE:0.09695421904325485  MAE:0.24337179958820343\n",
      "▶ seg11  mean  MSE:0.09695421904325485  MAE:0.24337179958820343\n",
      "\n",
      "―――― Inference  |  Segment 12 ――――\n",
      "  seg12-ch0  MSE:0.10582569241523743  MAE:0.2525569796562195\n",
      "▶ seg12  mean  MSE:0.10582569241523743  MAE:0.2525569796562195\n",
      "\n",
      "―――― Inference  |  Segment 13 ――――\n",
      "  seg13-ch0  MSE:0.10027351975440979  MAE:0.24919593334197998\n",
      "▶ seg13  mean  MSE:0.10027351975440979  MAE:0.24919593334197998\n",
      "\n",
      "―――― Inference  |  Segment 14 ――――\n",
      "  seg14-ch0  MSE:0.1072990745306015  MAE:0.25714483857154846\n",
      "▶ seg14  mean  MSE:0.1072990745306015  MAE:0.25714483857154846\n",
      "\n",
      "―――― Inference  |  Segment 15 ――――\n",
      "  seg15-ch0  MSE:0.10121683031320572  MAE:0.24795643985271454\n",
      "▶ seg15  mean  MSE:0.10121683031320572  MAE:0.24795643985271454\n",
      "\n",
      "―――― Inference  |  Segment 16 ――――\n",
      "  seg16-ch0  MSE:0.10294037312269211  MAE:0.24914652109146118\n",
      "▶ seg16  mean  MSE:0.10294037312269211  MAE:0.24914652109146118\n",
      "\n",
      "―――― Inference  |  Segment 17 ――――\n",
      "  seg17-ch0  MSE:0.10118814557790756  MAE:0.2486097812652588\n",
      "▶ seg17  mean  MSE:0.10118814557790756  MAE:0.2486097812652588\n",
      "\n",
      "―――― Inference  |  Segment 18 ――――\n",
      "  seg18-ch0  MSE:0.09876192361116409  MAE:0.24519900977611542\n",
      "▶ seg18  mean  MSE:0.09876192361116409  MAE:0.24519900977611542\n",
      "\n",
      "―――― Inference  |  Segment 19 ――――\n",
      "  seg19-ch0  MSE:0.10083352774381638  MAE:0.24956192076206207\n",
      "▶ seg19  mean  MSE:0.10083352774381638  MAE:0.24956192076206207\n",
      "\n",
      "―――― Inference  |  Segment 20 ――――\n",
      "  seg20-ch0  MSE:0.10669728368520737  MAE:0.25502344965934753\n",
      "▶ seg20  mean  MSE:0.10669728368520737  MAE:0.25502344965934753\n",
      "\n",
      "―――― Inference  |  Segment 21 ――――\n",
      "  seg21-ch0  MSE:0.10586080700159073  MAE:0.25159916281700134\n",
      "▶ seg21  mean  MSE:0.10586080700159073  MAE:0.25159916281700134\n",
      "\n",
      "―――― Inference  |  Segment 22 ――――\n",
      "  seg22-ch0  MSE:0.10227012634277344  MAE:0.24942244589328766\n",
      "▶ seg22  mean  MSE:0.10227012634277344  MAE:0.24942244589328766\n",
      "\n",
      "―――― Inference  |  Segment 23 ――――\n",
      "  seg23-ch0  MSE:0.10593875497579575  MAE:0.2575676143169403\n",
      "▶ seg23  mean  MSE:0.10593875497579575  MAE:0.2575676143169403\n",
      "\n",
      "―――― Inference  |  Segment 24 ――――\n",
      "  seg24-ch0  MSE:0.10667866468429565  MAE:0.254271000623703\n",
      "▶ seg24  mean  MSE:0.10667866468429565  MAE:0.254271000623703\n",
      "\n",
      "―――― Inference  |  Segment 25 ――――\n",
      "  seg25-ch0  MSE:0.10118165612220764  MAE:0.24751046299934387\n",
      "▶ seg25  mean  MSE:0.10118165612220764  MAE:0.24751046299934387\n",
      "\n",
      "―――― Inference  |  Segment 26 ――――\n",
      "  seg26-ch0  MSE:0.10244166105985641  MAE:0.24756388366222382\n",
      "▶ seg26  mean  MSE:0.10244166105985641  MAE:0.24756388366222382\n",
      "\n",
      "―――― Inference  |  Segment 27 ――――\n",
      "  seg27-ch0  MSE:0.10891342908143997  MAE:0.2577330470085144\n",
      "▶ seg27  mean  MSE:0.10891342908143997  MAE:0.2577330470085144\n",
      "\n",
      "―――― Inference  |  Segment 28 ――――\n",
      "  seg28-ch0  MSE:0.11215667426586151  MAE:0.25829628109931946\n",
      "▶ seg28  mean  MSE:0.11215667426586151  MAE:0.25829628109931946\n",
      "\n",
      "―――― Inference  |  Segment 29 ――――\n",
      "  seg29-ch0  MSE:0.13687077164649963  MAE:0.2881716787815094\n",
      "▶ seg29  mean  MSE:0.13687077164649963  MAE:0.2881716787815094\n",
      "\n",
      "―――― Inference  |  Segment 30 ――――\n",
      "  seg30-ch0  MSE:0.09746410697698593  MAE:0.24339880049228668\n",
      "▶ seg30  mean  MSE:0.09746410697698593  MAE:0.24339880049228668\n",
      "\n",
      "―――― Inference  |  Segment 31 ――――\n",
      "  seg31-ch0  MSE:0.09941302239894867  MAE:0.24664515256881714\n",
      "▶ seg31  mean  MSE:0.09941302239894867  MAE:0.24664515256881714\n",
      "\n",
      "―――― Inference  |  Segment 32 ――――\n",
      "  seg32-ch0  MSE:0.10503704100847244  MAE:0.25221434235572815\n",
      "▶ seg32  mean  MSE:0.10503704100847244  MAE:0.25221434235572815\n",
      "\n",
      "―――― Inference  |  Segment 33 ――――\n",
      "  seg33-ch0  MSE:0.103067547082901  MAE:0.25062623620033264\n",
      "▶ seg33  mean  MSE:0.103067547082901  MAE:0.25062623620033264\n",
      "\n",
      "―――― Inference  |  Segment 34 ――――\n",
      "  seg34-ch0  MSE:0.10107072442770004  MAE:0.24808670580387115\n",
      "▶ seg34  mean  MSE:0.10107072442770004  MAE:0.24808670580387115\n",
      "\n",
      "―――― Inference  |  Segment 35 ――――\n",
      "  seg35-ch0  MSE:0.10294955968856812  MAE:0.250457763671875\n",
      "▶ seg35  mean  MSE:0.10294955968856812  MAE:0.250457763671875\n",
      "\n",
      "―――― Inference  |  Segment 36 ――――\n",
      "  seg36-ch0  MSE:0.11421968042850494  MAE:0.26257163286209106\n",
      "▶ seg36  mean  MSE:0.11421968042850494  MAE:0.26257163286209106\n",
      "\n",
      "―――― Inference  |  Segment 37 ――――\n",
      "  seg37-ch0  MSE:0.1021203100681305  MAE:0.24984033405780792\n",
      "▶ seg37  mean  MSE:0.1021203100681305  MAE:0.24984033405780792\n",
      "\n",
      "―――― Inference  |  Segment 38 ――――\n",
      "  seg38-ch0  MSE:0.10731532424688339  MAE:0.25655391812324524\n",
      "▶ seg38  mean  MSE:0.10731532424688339  MAE:0.25655391812324524\n",
      "\n",
      "―――― Inference  |  Segment 39 ――――\n",
      "  seg39-ch0  MSE:0.10207455605268478  MAE:0.24433210492134094\n",
      "▶ seg39  mean  MSE:0.10207455605268478  MAE:0.24433210492134094\n",
      "\n",
      "―――― Inference  |  Segment 40 ――――\n",
      "  seg40-ch0  MSE:0.13281948864459991  MAE:0.2849007844924927\n",
      "▶ seg40  mean  MSE:0.13281948864459991  MAE:0.2849007844924927\n",
      "\n",
      "―――― Inference  |  Segment 41 ――――\n",
      "  seg41-ch0  MSE:0.1139293909072876  MAE:0.2625635862350464\n",
      "▶ seg41  mean  MSE:0.1139293909072876  MAE:0.2625635862350464\n",
      "\n",
      "―――― Inference  |  Segment 42 ――――\n",
      "  seg42-ch0  MSE:0.10905306041240692  MAE:0.25708329677581787\n",
      "▶ seg42  mean  MSE:0.10905306041240692  MAE:0.25708329677581787\n",
      "\n",
      "―――― Inference  |  Segment 43 ――――\n",
      "  seg43-ch0  MSE:0.10125130414962769  MAE:0.24801938235759735\n",
      "▶ seg43  mean  MSE:0.10125130414962769  MAE:0.24801938235759735\n",
      "\n",
      "―――― Inference  |  Segment 44 ――――\n",
      "  seg44-ch0  MSE:0.1017405316233635  MAE:0.24958164989948273\n",
      "▶ seg44  mean  MSE:0.1017405316233635  MAE:0.24958164989948273\n",
      "\n",
      "―――― Inference  |  Segment 45 ――――\n",
      "  seg45-ch0  MSE:0.09986026585102081  MAE:0.2461068481206894\n",
      "▶ seg45  mean  MSE:0.09986026585102081  MAE:0.2461068481206894\n",
      "\n",
      "―――― Inference  |  Segment 46 ――――\n",
      "  seg46-ch0  MSE:0.1143714189529419  MAE:0.26250630617141724\n",
      "▶ seg46  mean  MSE:0.1143714189529419  MAE:0.26250630617141724\n",
      "\n",
      "―――― Inference  |  Segment 47 ――――\n",
      "  seg47-ch0  MSE:0.10202231258153915  MAE:0.2507578134536743\n",
      "▶ seg47  mean  MSE:0.10202231258153915  MAE:0.2507578134536743\n",
      "\n",
      "―――― Inference  |  Segment 48 ――――\n",
      "  seg48-ch0  MSE:0.10488168895244598  MAE:0.2553381025791168\n",
      "▶ seg48  mean  MSE:0.10488168895244598  MAE:0.2553381025791168\n",
      "\n",
      "―――― Inference  |  Segment 49 ――――\n",
      "  seg49-ch0  MSE:0.09902525693178177  MAE:0.24624003469944\n",
      "▶ seg49  mean  MSE:0.09902525693178177  MAE:0.24624003469944\n",
      "\n",
      "―――― Inference  |  Segment 50 ――――\n",
      "  seg50-ch0  MSE:0.10909704118967056  MAE:0.2554640769958496\n",
      "▶ seg50  mean  MSE:0.10909704118967056  MAE:0.2554640769958496\n",
      "\n",
      "―――― Inference  |  Segment 51 ――――\n",
      "  seg51-ch0  MSE:0.09960300475358963  MAE:0.24627910554409027\n",
      "▶ seg51  mean  MSE:0.09960300475358963  MAE:0.24627910554409027\n",
      "\n",
      "―――― Inference  |  Segment 52 ――――\n",
      "  seg52-ch0  MSE:0.10387272387742996  MAE:0.24994486570358276\n",
      "▶ seg52  mean  MSE:0.10387272387742996  MAE:0.24994486570358276\n",
      "\n",
      "―――― Inference  |  Segment 53 ――――\n",
      "  seg53-ch0  MSE:0.11618121713399887  MAE:0.2662147283554077\n",
      "▶ seg53  mean  MSE:0.11618121713399887  MAE:0.2662147283554077\n",
      "\n",
      "―――― Inference  |  Segment 54 ――――\n",
      "  seg54-ch0  MSE:0.10424327850341797  MAE:0.25309523940086365\n",
      "▶ seg54  mean  MSE:0.10424327850341797  MAE:0.25309523940086365\n",
      "\n",
      "―――― Inference  |  Segment 55 ――――\n",
      "  seg55-ch0  MSE:0.10022757202386856  MAE:0.24560905992984772\n",
      "▶ seg55  mean  MSE:0.10022757202386856  MAE:0.24560905992984772\n",
      "\n",
      "―――― Inference  |  Segment 56 ――――\n",
      "  seg56-ch0  MSE:0.1055951789021492  MAE:0.25316786766052246\n",
      "▶ seg56  mean  MSE:0.1055951789021492  MAE:0.25316786766052246\n",
      "\n",
      "―――― Inference  |  Segment 57 ――――\n",
      "  seg57-ch0  MSE:0.10322112590074539  MAE:0.24836362898349762\n",
      "▶ seg57  mean  MSE:0.10322112590074539  MAE:0.24836362898349762\n",
      "\n",
      "―――― Inference  |  Segment 58 ――――\n",
      "  seg58-ch0  MSE:0.10437248647212982  MAE:0.2511475384235382\n",
      "▶ seg58  mean  MSE:0.10437248647212982  MAE:0.2511475384235382\n",
      "\n",
      "―――― Inference  |  Segment 59 ――――\n",
      "  seg59-ch0  MSE:0.10078898072242737  MAE:0.24852776527404785\n",
      "▶ seg59  mean  MSE:0.10078898072242737  MAE:0.24852776527404785\n",
      "\n",
      "―――― Inference  |  Segment 60 ――――\n",
      "  seg60-ch0  MSE:0.10487966239452362  MAE:0.2524483799934387\n",
      "▶ seg60  mean  MSE:0.10487966239452362  MAE:0.2524483799934387\n",
      "\n",
      "―――― Inference  |  Segment 61 ――――\n",
      "  seg61-ch0  MSE:0.1069721058011055  MAE:0.25573697686195374\n",
      "▶ seg61  mean  MSE:0.1069721058011055  MAE:0.25573697686195374\n",
      "\n",
      "―――― Inference  |  Segment 62 ――――\n",
      "  seg62-ch0  MSE:0.10554715991020203  MAE:0.25683996081352234\n",
      "▶ seg62  mean  MSE:0.10554715991020203  MAE:0.25683996081352234\n",
      "\n",
      "―――― Inference  |  Segment 63 ――――\n",
      "  seg63-ch0  MSE:0.10388510674238205  MAE:0.24947835505008698\n",
      "▶ seg63  mean  MSE:0.10388510674238205  MAE:0.24947835505008698\n",
      "\n",
      "―――― Inference  |  Segment 64 ――――\n",
      "  seg64-ch0  MSE:0.09875477105379105  MAE:0.2482941448688507\n",
      "▶ seg64  mean  MSE:0.09875477105379105  MAE:0.2482941448688507\n",
      "\n",
      "seg10 ch0 MSE : 0.10982119\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "C        = 1                                \n",
    "P        = base_cfg.pred_len                \n",
    "seg_cnt  = 65\n",
    "batch_sz = 32\n",
    "\n",
    "results2_ETTh1_ES_val_1E = {}  \n",
    "\n",
    "for seg in range(seg_cnt):\n",
    "    print(f\"\\n―――― Inference  |  Segment {seg} ――――\")\n",
    "\n",
    "    \n",
    "    pv = val_pred_segments_ES[:, seg, :, :]         \n",
    "    ds = TensorDataset(val_sequences, val_labels, pv) \n",
    "    dl = DataLoader(ds, batch_size=batch_sz, shuffle=False)\n",
    "\n",
    "    \n",
    "    pred_ch, true_ch, metrics = [], [], {}\n",
    "\n",
    "    \n",
    "    for ch in range(C):\n",
    "        ckpt = Path(base_cfg.save_path) / f\"seg{seg}\" / f\"ch{ch}\" / \"checkpoint.pth\"\n",
    "        if not ckpt.exists():\n",
    "            print(f\"[Warn] seg{seg}-ch{ch} ckpt 없음 → skip\");  continue\n",
    "\n",
    "        \n",
    "        model = Model(base_cfg).to(device)\n",
    "        model.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, pv_batch in dl:                 \n",
    "                xb  = xb.float().to(device)\n",
    "                yb  = yb.float()      \n",
    "                pv_ = pv_batch.float().to(device)\n",
    "\n",
    "                out = model(xb, pv_)                     \n",
    "                preds.append(out[:, -P:, ch].cpu())\n",
    "                trues.append(yb[:, -P:, ch])\n",
    "\n",
    "        preds = torch.cat(preds, 0)     \n",
    "        trues = torch.cat(trues, 0)\n",
    "        pred_ch.append(preds)\n",
    "        true_ch.append(trues)\n",
    "\n",
    "        \n",
    "        p, t = preds.reshape(-1).cpu().numpy(), trues.reshape(-1).cpu().numpy()\n",
    "        mse  = np.mean((p - t)**2)\n",
    "        mae  = np.mean(np.abs(p - t))\n",
    "        metrics[ch] = (mse, mae)\n",
    "\n",
    "        print(f\"  seg{seg}-ch{ch}  MSE:{mse}  MAE:{mae}\")\n",
    "\n",
    "        del model; torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    pred_all = torch.stack(pred_ch,  dim=2)   \n",
    "    true_all = torch.stack(true_ch,  dim=2)\n",
    "\n",
    "    \n",
    "    mse_avg = np.mean([m for m,_ in metrics.values()])\n",
    "    mae_avg = np.mean([a for _,a in metrics.values()])\n",
    "    print(f\"▶ seg{seg}  mean  MSE:{mse_avg}  MAE:{mae_avg}\")\n",
    "\n",
    "    \n",
    "    results2_ETTh1_ES_val_1E[seg] = {\n",
    "        \"pred2_ES_val_1E\"    : pred_all,   \n",
    "        \"true2_ES_val_1E\"    : true_all,   \n",
    "        \"metrics2_ES_val_1E\" : metrics     \n",
    "    }\n",
    "\n",
    "\n",
    "seg10_ch0_mse = results2_ETTh1_ES_val_1E[10][\"metrics2_ES_val_1E\"][0][0]\n",
    "print(\"\\nseg10 ch0 MSE :\", seg10_ch0_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee265f8",
   "metadata": {},
   "source": [
    "## 1E->1E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2f12b",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d5080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np, os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class Configs:\n",
    "    def __init__(self, seq_len, pred_len, enc_in, individual,\n",
    "                 learning_rate, lradj, patience,\n",
    "                 save_path, attach_to_trend, attach_to_seasonal):\n",
    "        self.seq_len  = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.enc_in   = enc_in          \n",
    "        self.individual = individual\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lradj = lradj\n",
    "        self.patience = patience\n",
    "        self.save_path = save_path\n",
    "        self.attach_to_trend    = attach_to_trend\n",
    "        self.attach_to_seasonal = attach_to_seasonal\n",
    "\n",
    "\n",
    "base_cfg = Configs(\n",
    "    pred_len         = 96,\n",
    "    seq_len          = 336 + pred_len//3,\n",
    "    enc_in           = 1,\n",
    "    individual       = True,\n",
    "    learning_rate    = 0.005,\n",
    "    lradj            = 'type1',\n",
    "    patience         = 3,\n",
    "    save_path        = \"./model2_ETTh1_E1_1E\",\n",
    "    attach_to_trend  = True,\n",
    "    attach_to_seasonal = True\n",
    ")\n",
    "Path(base_cfg.save_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa5601",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae3d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "―――― Inference  |  Segment 0 ――――\n",
      "  seg0-ch0  MSE:0.10546677559614182  MAE:0.24991127848625183\n",
      "▶ seg0  mean  MSE:0.10546677559614182  MAE:0.24991127848625183\n",
      "\n",
      "―――― Inference  |  Segment 1 ――――\n",
      "  seg1-ch0  MSE:0.1027875691652298  MAE:0.25060567259788513\n",
      "▶ seg1  mean  MSE:0.1027875691652298  MAE:0.25060567259788513\n",
      "\n",
      "―――― Inference  |  Segment 2 ――――\n",
      "  seg2-ch0  MSE:0.09951791167259216  MAE:0.24287378787994385\n",
      "▶ seg2  mean  MSE:0.09951791167259216  MAE:0.24287378787994385\n",
      "\n",
      "―――― Inference  |  Segment 3 ――――\n",
      "  seg3-ch0  MSE:0.10075636953115463  MAE:0.24639277160167694\n",
      "▶ seg3  mean  MSE:0.10075636953115463  MAE:0.24639277160167694\n",
      "\n",
      "―――― Inference  |  Segment 4 ――――\n",
      "  seg4-ch0  MSE:0.09673481434583664  MAE:0.24248972535133362\n",
      "▶ seg4  mean  MSE:0.09673481434583664  MAE:0.24248972535133362\n",
      "\n",
      "―――― Inference  |  Segment 5 ――――\n",
      "  seg5-ch0  MSE:0.10485834628343582  MAE:0.25548338890075684\n",
      "▶ seg5  mean  MSE:0.10485834628343582  MAE:0.25548338890075684\n",
      "\n",
      "―――― Inference  |  Segment 6 ――――\n",
      "  seg6-ch0  MSE:0.10343682020902634  MAE:0.2487345039844513\n",
      "▶ seg6  mean  MSE:0.10343682020902634  MAE:0.2487345039844513\n",
      "\n",
      "―――― Inference  |  Segment 7 ――――\n",
      "  seg7-ch0  MSE:0.10304708778858185  MAE:0.252415269613266\n",
      "▶ seg7  mean  MSE:0.10304708778858185  MAE:0.252415269613266\n",
      "\n",
      "―――― Inference  |  Segment 8 ――――\n",
      "  seg8-ch0  MSE:0.10088671743869781  MAE:0.2493313103914261\n",
      "▶ seg8  mean  MSE:0.10088671743869781  MAE:0.2493313103914261\n",
      "\n",
      "―――― Inference  |  Segment 9 ――――\n",
      "  seg9-ch0  MSE:0.10080719739198685  MAE:0.25108274817466736\n",
      "▶ seg9  mean  MSE:0.10080719739198685  MAE:0.25108274817466736\n",
      "\n",
      "―――― Inference  |  Segment 10 ――――\n",
      "  seg10-ch0  MSE:0.09824685007333755  MAE:0.2457636147737503\n",
      "▶ seg10  mean  MSE:0.09824685007333755  MAE:0.2457636147737503\n",
      "\n",
      "―――― Inference  |  Segment 11 ――――\n",
      "  seg11-ch0  MSE:0.09932579845190048  MAE:0.24666151404380798\n",
      "▶ seg11  mean  MSE:0.09932579845190048  MAE:0.24666151404380798\n",
      "\n",
      "―――― Inference  |  Segment 12 ――――\n",
      "  seg12-ch0  MSE:0.12204258143901825  MAE:0.2747768759727478\n",
      "▶ seg12  mean  MSE:0.12204258143901825  MAE:0.2747768759727478\n",
      "\n",
      "―――― Inference  |  Segment 13 ――――\n",
      "  seg13-ch0  MSE:0.1051376461982727  MAE:0.2524086534976959\n",
      "▶ seg13  mean  MSE:0.1051376461982727  MAE:0.2524086534976959\n",
      "\n",
      "―――― Inference  |  Segment 14 ――――\n",
      "  seg14-ch0  MSE:0.10084760189056396  MAE:0.2435898780822754\n",
      "▶ seg14  mean  MSE:0.10084760189056396  MAE:0.2435898780822754\n",
      "\n",
      "―――― Inference  |  Segment 15 ――――\n",
      "  seg15-ch0  MSE:0.0980081781744957  MAE:0.2427823841571808\n",
      "▶ seg15  mean  MSE:0.0980081781744957  MAE:0.2427823841571808\n",
      "\n",
      "―――― Inference  |  Segment 16 ――――\n",
      "  seg16-ch0  MSE:0.09871431440114975  MAE:0.2456219494342804\n",
      "▶ seg16  mean  MSE:0.09871431440114975  MAE:0.2456219494342804\n",
      "\n",
      "―――― Inference  |  Segment 17 ――――\n",
      "  seg17-ch0  MSE:0.10827138274908066  MAE:0.2535623013973236\n",
      "▶ seg17  mean  MSE:0.10827138274908066  MAE:0.2535623013973236\n",
      "\n",
      "―――― Inference  |  Segment 18 ――――\n",
      "  seg18-ch0  MSE:0.0972338542342186  MAE:0.24408526718616486\n",
      "▶ seg18  mean  MSE:0.0972338542342186  MAE:0.24408526718616486\n",
      "\n",
      "―――― Inference  |  Segment 19 ――――\n",
      "  seg19-ch0  MSE:0.10942258685827255  MAE:0.2553006708621979\n",
      "▶ seg19  mean  MSE:0.10942258685827255  MAE:0.2553006708621979\n",
      "\n",
      "―――― Inference  |  Segment 20 ――――\n",
      "  seg20-ch0  MSE:0.1024388000369072  MAE:0.2480945587158203\n",
      "▶ seg20  mean  MSE:0.1024388000369072  MAE:0.2480945587158203\n",
      "\n",
      "―――― Inference  |  Segment 21 ――――\n",
      "  seg21-ch0  MSE:0.13059598207473755  MAE:0.28146374225616455\n",
      "▶ seg21  mean  MSE:0.13059598207473755  MAE:0.28146374225616455\n",
      "\n",
      "―――― Inference  |  Segment 22 ――――\n",
      "  seg22-ch0  MSE:0.09839922934770584  MAE:0.24512794613838196\n",
      "▶ seg22  mean  MSE:0.09839922934770584  MAE:0.24512794613838196\n",
      "\n",
      "―――― Inference  |  Segment 23 ――――\n",
      "  seg23-ch0  MSE:0.10393308103084564  MAE:0.2517150938510895\n",
      "▶ seg23  mean  MSE:0.10393308103084564  MAE:0.2517150938510895\n",
      "\n",
      "―――― Inference  |  Segment 24 ――――\n",
      "  seg24-ch0  MSE:0.10297811031341553  MAE:0.24807846546173096\n",
      "▶ seg24  mean  MSE:0.10297811031341553  MAE:0.24807846546173096\n",
      "\n",
      "―――― Inference  |  Segment 25 ――――\n",
      "  seg25-ch0  MSE:0.10637862980365753  MAE:0.25927358865737915\n",
      "▶ seg25  mean  MSE:0.10637862980365753  MAE:0.25927358865737915\n",
      "\n",
      "―――― Inference  |  Segment 26 ――――\n",
      "  seg26-ch0  MSE:0.09653577953577042  MAE:0.24457873404026031\n",
      "▶ seg26  mean  MSE:0.09653577953577042  MAE:0.24457873404026031\n",
      "\n",
      "―――― Inference  |  Segment 27 ――――\n",
      "  seg27-ch0  MSE:0.10093272477388382  MAE:0.24769394099712372\n",
      "▶ seg27  mean  MSE:0.10093272477388382  MAE:0.24769394099712372\n",
      "\n",
      "―――― Inference  |  Segment 28 ――――\n",
      "  seg28-ch0  MSE:0.09732916951179504  MAE:0.2454044222831726\n",
      "▶ seg28  mean  MSE:0.09732916951179504  MAE:0.2454044222831726\n",
      "\n",
      "―――― Inference  |  Segment 29 ――――\n",
      "  seg29-ch0  MSE:0.09711740911006927  MAE:0.24456548690795898\n",
      "▶ seg29  mean  MSE:0.09711740911006927  MAE:0.24456548690795898\n",
      "\n",
      "―――― Inference  |  Segment 30 ――――\n",
      "  seg30-ch0  MSE:0.09893790632486343  MAE:0.24860551953315735\n",
      "▶ seg30  mean  MSE:0.09893790632486343  MAE:0.24860551953315735\n",
      "\n",
      "―――― Inference  |  Segment 31 ――――\n",
      "  seg31-ch0  MSE:0.10496786236763  MAE:0.252948522567749\n",
      "▶ seg31  mean  MSE:0.10496786236763  MAE:0.252948522567749\n",
      "\n",
      "―――― Inference  |  Segment 32 ――――\n",
      "  seg32-ch0  MSE:0.09797030687332153  MAE:0.24489359557628632\n",
      "▶ seg32  mean  MSE:0.09797030687332153  MAE:0.24489359557628632\n",
      "\n",
      "―――― Inference  |  Segment 33 ――――\n",
      "  seg33-ch0  MSE:0.11126738786697388  MAE:0.2595351040363312\n",
      "▶ seg33  mean  MSE:0.11126738786697388  MAE:0.2595351040363312\n",
      "\n",
      "―――― Inference  |  Segment 34 ――――\n",
      "  seg34-ch0  MSE:0.11011943966150284  MAE:0.2583726644515991\n",
      "▶ seg34  mean  MSE:0.11011943966150284  MAE:0.2583726644515991\n",
      "\n",
      "―――― Inference  |  Segment 35 ――――\n",
      "  seg35-ch0  MSE:0.10228073596954346  MAE:0.24747326970100403\n",
      "▶ seg35  mean  MSE:0.10228073596954346  MAE:0.24747326970100403\n",
      "\n",
      "―――― Inference  |  Segment 36 ――――\n",
      "  seg36-ch0  MSE:0.12292194366455078  MAE:0.2754319906234741\n",
      "▶ seg36  mean  MSE:0.12292194366455078  MAE:0.2754319906234741\n",
      "\n",
      "―――― Inference  |  Segment 37 ――――\n",
      "  seg37-ch0  MSE:0.10285114496946335  MAE:0.25006788969039917\n",
      "▶ seg37  mean  MSE:0.10285114496946335  MAE:0.25006788969039917\n",
      "\n",
      "―――― Inference  |  Segment 38 ――――\n",
      "  seg38-ch0  MSE:0.09913505613803864  MAE:0.24713365733623505\n",
      "▶ seg38  mean  MSE:0.09913505613803864  MAE:0.24713365733623505\n",
      "\n",
      "―――― Inference  |  Segment 39 ――――\n",
      "  seg39-ch0  MSE:0.10343901067972183  MAE:0.2500137984752655\n",
      "▶ seg39  mean  MSE:0.10343901067972183  MAE:0.2500137984752655\n",
      "\n",
      "―――― Inference  |  Segment 40 ――――\n",
      "  seg40-ch0  MSE:0.09953075647354126  MAE:0.24636702239513397\n",
      "▶ seg40  mean  MSE:0.09953075647354126  MAE:0.24636702239513397\n",
      "\n",
      "―――― Inference  |  Segment 41 ――――\n",
      "  seg41-ch0  MSE:0.13080015778541565  MAE:0.2810104191303253\n",
      "▶ seg41  mean  MSE:0.13080015778541565  MAE:0.2810104191303253\n",
      "\n",
      "―――― Inference  |  Segment 42 ――――\n",
      "  seg42-ch0  MSE:0.10118081420660019  MAE:0.248872309923172\n",
      "▶ seg42  mean  MSE:0.10118081420660019  MAE:0.248872309923172\n",
      "\n",
      "―――― Inference  |  Segment 43 ――――\n",
      "  seg43-ch0  MSE:0.09712150692939758  MAE:0.24517013132572174\n",
      "▶ seg43  mean  MSE:0.09712150692939758  MAE:0.24517013132572174\n",
      "\n",
      "―――― Inference  |  Segment 44 ――――\n",
      "  seg44-ch0  MSE:0.10572493821382523  MAE:0.25505125522613525\n",
      "▶ seg44  mean  MSE:0.10572493821382523  MAE:0.25505125522613525\n",
      "\n",
      "―――― Inference  |  Segment 45 ――――\n",
      "  seg45-ch0  MSE:0.10581734776496887  MAE:0.2548588812351227\n",
      "▶ seg45  mean  MSE:0.10581734776496887  MAE:0.2548588812351227\n",
      "\n",
      "―――― Inference  |  Segment 46 ――――\n",
      "  seg46-ch0  MSE:0.0976860523223877  MAE:0.24526575207710266\n",
      "▶ seg46  mean  MSE:0.0976860523223877  MAE:0.24526575207710266\n",
      "\n",
      "―――― Inference  |  Segment 47 ――――\n",
      "  seg47-ch0  MSE:0.10173961520195007  MAE:0.2496495544910431\n",
      "▶ seg47  mean  MSE:0.10173961520195007  MAE:0.2496495544910431\n",
      "\n",
      "―――― Inference  |  Segment 48 ――――\n",
      "  seg48-ch0  MSE:0.09626753628253937  MAE:0.2431519776582718\n",
      "▶ seg48  mean  MSE:0.09626753628253937  MAE:0.2431519776582718\n",
      "\n",
      "―――― Inference  |  Segment 49 ――――\n",
      "  seg49-ch0  MSE:0.09717956185340881  MAE:0.2444097399711609\n",
      "▶ seg49  mean  MSE:0.09717956185340881  MAE:0.2444097399711609\n",
      "\n",
      "―――― Inference  |  Segment 50 ――――\n",
      "  seg50-ch0  MSE:0.10499503463506699  MAE:0.2508309781551361\n",
      "▶ seg50  mean  MSE:0.10499503463506699  MAE:0.2508309781551361\n",
      "\n",
      "―――― Inference  |  Segment 51 ――――\n",
      "  seg51-ch0  MSE:0.1071670800447464  MAE:0.2584686577320099\n",
      "▶ seg51  mean  MSE:0.1071670800447464  MAE:0.2584686577320099\n",
      "\n",
      "―――― Inference  |  Segment 52 ――――\n",
      "  seg52-ch0  MSE:0.09762750566005707  MAE:0.24632170796394348\n",
      "▶ seg52  mean  MSE:0.09762750566005707  MAE:0.24632170796394348\n",
      "\n",
      "―――― Inference  |  Segment 53 ――――\n",
      "  seg53-ch0  MSE:0.10844047367572784  MAE:0.25700849294662476\n",
      "▶ seg53  mean  MSE:0.10844047367572784  MAE:0.25700849294662476\n",
      "\n",
      "―――― Inference  |  Segment 54 ――――\n",
      "  seg54-ch0  MSE:0.10505121946334839  MAE:0.2532620429992676\n",
      "▶ seg54  mean  MSE:0.10505121946334839  MAE:0.2532620429992676\n",
      "\n",
      "―――― Inference  |  Segment 55 ――――\n",
      "  seg55-ch0  MSE:0.10377229750156403  MAE:0.24946428835391998\n",
      "▶ seg55  mean  MSE:0.10377229750156403  MAE:0.24946428835391998\n",
      "\n",
      "―――― Inference  |  Segment 56 ――――\n",
      "  seg56-ch0  MSE:0.09947850555181503  MAE:0.24431118369102478\n",
      "▶ seg56  mean  MSE:0.09947850555181503  MAE:0.24431118369102478\n",
      "\n",
      "―――― Inference  |  Segment 57 ――――\n",
      "  seg57-ch0  MSE:0.09725220501422882  MAE:0.24287916719913483\n",
      "▶ seg57  mean  MSE:0.09725220501422882  MAE:0.24287916719913483\n",
      "\n",
      "―――― Inference  |  Segment 58 ――――\n",
      "  seg58-ch0  MSE:0.1037098839879036  MAE:0.25152334570884705\n",
      "▶ seg58  mean  MSE:0.1037098839879036  MAE:0.25152334570884705\n",
      "\n",
      "―――― Inference  |  Segment 59 ――――\n",
      "  seg59-ch0  MSE:0.09903589636087418  MAE:0.2457023411989212\n",
      "▶ seg59  mean  MSE:0.09903589636087418  MAE:0.2457023411989212\n",
      "\n",
      "―――― Inference  |  Segment 60 ――――\n",
      "  seg60-ch0  MSE:0.1041395515203476  MAE:0.25275421142578125\n",
      "▶ seg60  mean  MSE:0.1041395515203476  MAE:0.25275421142578125\n",
      "\n",
      "―――― Inference  |  Segment 61 ――――\n",
      "  seg61-ch0  MSE:0.09883738309144974  MAE:0.24680443108081818\n",
      "▶ seg61  mean  MSE:0.09883738309144974  MAE:0.24680443108081818\n",
      "\n",
      "―――― Inference  |  Segment 62 ――――\n",
      "  seg62-ch0  MSE:0.10211889445781708  MAE:0.2523290514945984\n",
      "▶ seg62  mean  MSE:0.10211889445781708  MAE:0.2523290514945984\n",
      "\n",
      "―――― Inference  |  Segment 63 ――――\n",
      "  seg63-ch0  MSE:0.10329776257276535  MAE:0.24997223913669586\n",
      "▶ seg63  mean  MSE:0.10329776257276535  MAE:0.24997223913669586\n",
      "\n",
      "―――― Inference  |  Segment 64 ――――\n",
      "  seg64-ch0  MSE:0.09818682074546814  MAE:0.2440996766090393\n",
      "▶ seg64  mean  MSE:0.09818682074546814  MAE:0.2440996766090393\n",
      "\n",
      "seg10 ch0 MSE : 0.09824685\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "C        = 1                                \n",
    "P        = base_cfg.pred_len                \n",
    "seg_cnt  = 65\n",
    "batch_sz = 32\n",
    "\n",
    "results2_ETTh1_E1_val_1E = {}  \n",
    "\n",
    "for seg in range(seg_cnt):\n",
    "    print(f\"\\n―――― Inference  |  Segment {seg} ――――\")\n",
    "\n",
    "    \n",
    "    pv = val_pred_segments_E1[:, seg, :, :]         \n",
    "    ds = TensorDataset(val_sequences, val_labels, pv) \n",
    "    dl = DataLoader(ds, batch_size=batch_sz, shuffle=False)\n",
    "\n",
    "    \n",
    "    pred_ch, true_ch, metrics = [], [], {}\n",
    "\n",
    "    \n",
    "    for ch in range(C):\n",
    "        ckpt = Path(base_cfg.save_path) / f\"seg{seg}\" / f\"ch{ch}\" / \"checkpoint.pth\"\n",
    "        if not ckpt.exists():\n",
    "            print(f\"[Warn] seg{seg}-ch{ch} ckpt 없음 → skip\");  continue\n",
    "\n",
    "        \n",
    "        model = Model(base_cfg).to(device)\n",
    "        model.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, pv_batch in dl:                 \n",
    "                xb  = xb.float().to(device)\n",
    "                yb  = yb.float()        \n",
    "                pv_ = pv_batch.float().to(device)\n",
    "\n",
    "                out = model(xb, pv_)                     \n",
    "                preds.append(out[:, -P:, ch].cpu())\n",
    "                trues.append(yb[:, -P:, ch])\n",
    "\n",
    "        preds = torch.cat(preds, 0)     \n",
    "        trues = torch.cat(trues, 0)\n",
    "        pred_ch.append(preds)\n",
    "        true_ch.append(trues)\n",
    "\n",
    "        \n",
    "        p, t = preds.reshape(-1).cpu().numpy(), trues.reshape(-1).cpu().numpy()\n",
    "        mse  = np.mean((p - t)**2)\n",
    "        mae  = np.mean(np.abs(p - t))\n",
    "        metrics[ch] = (mse, mae)\n",
    "\n",
    "        print(f\"  seg{seg}-ch{ch}  MSE:{mse}  MAE:{mae}\")\n",
    "\n",
    "        del model; torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    pred_all = torch.stack(pred_ch,  dim=2)   \n",
    "    true_all = torch.stack(true_ch,  dim=2)\n",
    "\n",
    "    \n",
    "    mse_avg = np.mean([m for m,_ in metrics.values()])\n",
    "    mae_avg = np.mean([a for _,a in metrics.values()])\n",
    "    print(f\"▶ seg{seg}  mean  MSE:{mse_avg}  MAE:{mae_avg}\")\n",
    "\n",
    "    \n",
    "    results2_ETTh1_E1_val_1E[seg] = {\n",
    "        \"pred2_E1_val_1E\"    : pred_all,   \n",
    "        \"true2_E1_val_1E\"    : true_all,   \n",
    "        \"metrics2_E1_val_1E\" : metrics     \n",
    "    }\n",
    "\n",
    "\n",
    "seg10_ch0_mse = results2_ETTh1_E1_val_1E[10][\"metrics2_E1_val_1E\"][0][0]\n",
    "print(\"\\nseg10 ch0 MSE :\", seg10_ch0_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69834e70",
   "metadata": {},
   "source": [
    "# 2nd Inference for Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebf5f35",
   "metadata": {},
   "source": [
    "## ES->1E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c81e19",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc98fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np, os\n",
    "from pathlib import Path\n",
    "\n",
    "pred_len = 96\n",
    "seq_len = 336 + pred_len//3\n",
    "\n",
    "\n",
    "class Configs:\n",
    "    def __init__(self, seq_len, pred_len, enc_in, individual,\n",
    "                 learning_rate, lradj, patience,\n",
    "                 save_path, attach_to_trend, attach_to_seasonal):\n",
    "        self.seq_len  = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.enc_in   = enc_in          \n",
    "        self.individual = individual\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lradj = lradj\n",
    "        self.patience = patience\n",
    "        self.save_path = save_path\n",
    "        self.attach_to_trend    = attach_to_trend\n",
    "        self.attach_to_seasonal = attach_to_seasonal\n",
    "\n",
    "\n",
    "base_cfg = Configs(\n",
    "    pred_len         = 96,\n",
    "    seq_len          = 336 + pred_len//3,\n",
    "    enc_in           = 1,\n",
    "    individual       = True,\n",
    "    learning_rate    = 0.005,\n",
    "    lradj            = 'type1',\n",
    "    patience         = 3,\n",
    "    save_path        = \"./model2_ETTh1_EES_1E\",\n",
    "    attach_to_trend  = True,\n",
    "    attach_to_seasonal = True\n",
    ")\n",
    "Path(base_cfg.save_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c38e1e",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557bded1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "―――― Inference  |  Segment 0 ――――\n",
      "  seg0-ch0  MSE:0.09696110337972641  MAE:0.2395492047071457\n",
      "▶ seg0  mean  MSE:0.09696110337972641  MAE:0.2395492047071457\n",
      "\n",
      "―――― Inference  |  Segment 1 ――――\n",
      "  seg1-ch0  MSE:0.08905763924121857  MAE:0.2339210957288742\n",
      "▶ seg1  mean  MSE:0.08905763924121857  MAE:0.2339210957288742\n",
      "\n",
      "―――― Inference  |  Segment 2 ――――\n",
      "  seg2-ch0  MSE:0.08044449985027313  MAE:0.2212306559085846\n",
      "▶ seg2  mean  MSE:0.08044449985027313  MAE:0.2212306559085846\n",
      "\n",
      "―――― Inference  |  Segment 3 ――――\n",
      "  seg3-ch0  MSE:0.12422502040863037  MAE:0.2844632565975189\n",
      "▶ seg3  mean  MSE:0.12422502040863037  MAE:0.2844632565975189\n",
      "\n",
      "―――― Inference  |  Segment 4 ――――\n",
      "  seg4-ch0  MSE:0.09962159395217896  MAE:0.24565236270427704\n",
      "▶ seg4  mean  MSE:0.09962159395217896  MAE:0.24565236270427704\n",
      "\n",
      "―――― Inference  |  Segment 5 ――――\n",
      "  seg5-ch0  MSE:0.1129877120256424  MAE:0.26492974162101746\n",
      "▶ seg5  mean  MSE:0.1129877120256424  MAE:0.26492974162101746\n",
      "\n",
      "―――― Inference  |  Segment 6 ――――\n",
      "  seg6-ch0  MSE:0.12304482609033585  MAE:0.2754412591457367\n",
      "▶ seg6  mean  MSE:0.12304482609033585  MAE:0.2754412591457367\n",
      "\n",
      "―――― Inference  |  Segment 7 ――――\n",
      "  seg7-ch0  MSE:0.10293632745742798  MAE:0.24955864250659943\n",
      "▶ seg7  mean  MSE:0.10293632745742798  MAE:0.24955864250659943\n",
      "\n",
      "―――― Inference  |  Segment 8 ――――\n",
      "  seg8-ch0  MSE:0.0928056612610817  MAE:0.24100609123706818\n",
      "▶ seg8  mean  MSE:0.0928056612610817  MAE:0.24100609123706818\n",
      "\n",
      "―――― Inference  |  Segment 9 ――――\n",
      "  seg9-ch0  MSE:0.13088977336883545  MAE:0.28654512763023376\n",
      "▶ seg9  mean  MSE:0.13088977336883545  MAE:0.28654512763023376\n",
      "\n",
      "―――― Inference  |  Segment 10 ――――\n",
      "  seg10-ch0  MSE:0.22340208292007446  MAE:0.3822936415672302\n",
      "▶ seg10  mean  MSE:0.22340208292007446  MAE:0.3822936415672302\n",
      "\n",
      "―――― Inference  |  Segment 11 ――――\n",
      "  seg11-ch0  MSE:0.08119607716798782  MAE:0.2260066270828247\n",
      "▶ seg11  mean  MSE:0.08119607716798782  MAE:0.2260066270828247\n",
      "\n",
      "―――― Inference  |  Segment 12 ――――\n",
      "  seg12-ch0  MSE:0.14105254411697388  MAE:0.30191054940223694\n",
      "▶ seg12  mean  MSE:0.14105254411697388  MAE:0.30191054940223694\n",
      "\n",
      "―――― Inference  |  Segment 13 ――――\n",
      "  seg13-ch0  MSE:0.12055607885122299  MAE:0.2673202455043793\n",
      "▶ seg13  mean  MSE:0.12055607885122299  MAE:0.2673202455043793\n",
      "\n",
      "―――― Inference  |  Segment 14 ――――\n",
      "  seg14-ch0  MSE:0.15579769015312195  MAE:0.31260377168655396\n",
      "▶ seg14  mean  MSE:0.15579769015312195  MAE:0.31260377168655396\n",
      "\n",
      "―――― Inference  |  Segment 15 ――――\n",
      "  seg15-ch0  MSE:0.09399870783090591  MAE:0.2345532476902008\n",
      "▶ seg15  mean  MSE:0.09399870783090591  MAE:0.2345532476902008\n",
      "\n",
      "―――― Inference  |  Segment 16 ――――\n",
      "  seg16-ch0  MSE:0.08746776729822159  MAE:0.2293141484260559\n",
      "▶ seg16  mean  MSE:0.08746776729822159  MAE:0.2293141484260559\n",
      "\n",
      "―――― Inference  |  Segment 17 ――――\n",
      "  seg17-ch0  MSE:0.11091656237840652  MAE:0.2640749216079712\n",
      "▶ seg17  mean  MSE:0.11091656237840652  MAE:0.2640749216079712\n",
      "\n",
      "―――― Inference  |  Segment 18 ――――\n",
      "  seg18-ch0  MSE:0.09586343914270401  MAE:0.23872250318527222\n",
      "▶ seg18  mean  MSE:0.09586343914270401  MAE:0.23872250318527222\n",
      "\n",
      "―――― Inference  |  Segment 19 ――――\n",
      "  seg19-ch0  MSE:0.09805746376514435  MAE:0.24230167269706726\n",
      "▶ seg19  mean  MSE:0.09805746376514435  MAE:0.24230167269706726\n",
      "\n",
      "―――― Inference  |  Segment 20 ――――\n",
      "  seg20-ch0  MSE:0.14811718463897705  MAE:0.31070685386657715\n",
      "▶ seg20  mean  MSE:0.14811718463897705  MAE:0.31070685386657715\n",
      "\n",
      "―――― Inference  |  Segment 21 ――――\n",
      "  seg21-ch0  MSE:0.11654523760080338  MAE:0.2694587707519531\n",
      "▶ seg21  mean  MSE:0.11654523760080338  MAE:0.2694587707519531\n",
      "\n",
      "―――― Inference  |  Segment 22 ――――\n",
      "  seg22-ch0  MSE:0.09333893656730652  MAE:0.2360830307006836\n",
      "▶ seg22  mean  MSE:0.09333893656730652  MAE:0.2360830307006836\n",
      "\n",
      "―――― Inference  |  Segment 23 ――――\n",
      "  seg23-ch0  MSE:0.14177531003952026  MAE:0.3039972484111786\n",
      "▶ seg23  mean  MSE:0.14177531003952026  MAE:0.3039972484111786\n",
      "\n",
      "―――― Inference  |  Segment 24 ――――\n",
      "  seg24-ch0  MSE:0.12960366904735565  MAE:0.28639307618141174\n",
      "▶ seg24  mean  MSE:0.12960366904735565  MAE:0.28639307618141174\n",
      "\n",
      "―――― Inference  |  Segment 25 ――――\n",
      "  seg25-ch0  MSE:0.09504969418048859  MAE:0.24433931708335876\n",
      "▶ seg25  mean  MSE:0.09504969418048859  MAE:0.24433931708335876\n",
      "\n",
      "―――― Inference  |  Segment 26 ――――\n",
      "  seg26-ch0  MSE:0.09984264522790909  MAE:0.24628674983978271\n",
      "▶ seg26  mean  MSE:0.09984264522790909  MAE:0.24628674983978271\n",
      "\n",
      "―――― Inference  |  Segment 27 ――――\n",
      "  seg27-ch0  MSE:0.14751140773296356  MAE:0.3050989508628845\n",
      "▶ seg27  mean  MSE:0.14751140773296356  MAE:0.3050989508628845\n",
      "\n",
      "―――― Inference  |  Segment 28 ――――\n",
      "  seg28-ch0  MSE:0.17037229239940643  MAE:0.32146552205085754\n",
      "▶ seg28  mean  MSE:0.17037229239940643  MAE:0.32146552205085754\n",
      "\n",
      "―――― Inference  |  Segment 29 ――――\n",
      "  seg29-ch0  MSE:0.3543519079685211  MAE:0.5081201195716858\n",
      "▶ seg29  mean  MSE:0.3543519079685211  MAE:0.5081201195716858\n",
      "\n",
      "―――― Inference  |  Segment 30 ――――\n",
      "  seg30-ch0  MSE:0.06327260285615921  MAE:0.19474516808986664\n",
      "▶ seg30  mean  MSE:0.06327260285615921  MAE:0.19474516808986664\n",
      "\n",
      "―――― Inference  |  Segment 31 ――――\n",
      "  seg31-ch0  MSE:0.07930325716733932  MAE:0.22159136831760406\n",
      "▶ seg31  mean  MSE:0.07930325716733932  MAE:0.22159136831760406\n",
      "\n",
      "―――― Inference  |  Segment 32 ――――\n",
      "  seg32-ch0  MSE:0.12234503775835037  MAE:0.2777874767780304\n",
      "▶ seg32  mean  MSE:0.12234503775835037  MAE:0.2777874767780304\n",
      "\n",
      "―――― Inference  |  Segment 33 ――――\n",
      "  seg33-ch0  MSE:0.10327006876468658  MAE:0.24627302587032318\n",
      "▶ seg33  mean  MSE:0.10327006876468658  MAE:0.24627302587032318\n",
      "\n",
      "―――― Inference  |  Segment 34 ――――\n",
      "  seg34-ch0  MSE:0.08646072447299957  MAE:0.23031863570213318\n",
      "▶ seg34  mean  MSE:0.08646072447299957  MAE:0.23031863570213318\n",
      "\n",
      "―――― Inference  |  Segment 35 ――――\n",
      "  seg35-ch0  MSE:0.14475451409816742  MAE:0.2999459207057953\n",
      "▶ seg35  mean  MSE:0.14475451409816742  MAE:0.2999459207057953\n",
      "\n",
      "―――― Inference  |  Segment 36 ――――\n",
      "  seg36-ch0  MSE:0.23946134746074677  MAE:0.38614144921302795\n",
      "▶ seg36  mean  MSE:0.23946134746074677  MAE:0.38614144921302795\n",
      "\n",
      "―――― Inference  |  Segment 37 ――――\n",
      "  seg37-ch0  MSE:0.12094878405332565  MAE:0.27253350615501404\n",
      "▶ seg37  mean  MSE:0.12094878405332565  MAE:0.27253350615501404\n",
      "\n",
      "―――― Inference  |  Segment 38 ――――\n",
      "  seg38-ch0  MSE:0.16610348224639893  MAE:0.3265628218650818\n",
      "▶ seg38  mean  MSE:0.16610348224639893  MAE:0.3265628218650818\n",
      "\n",
      "―――― Inference  |  Segment 39 ――――\n",
      "  seg39-ch0  MSE:0.08611872792243958  MAE:0.2268650382757187\n",
      "▶ seg39  mean  MSE:0.08611872792243958  MAE:0.2268650382757187\n",
      "\n",
      "―――― Inference  |  Segment 40 ――――\n",
      "  seg40-ch0  MSE:0.39743345975875854  MAE:0.5283740162849426\n",
      "▶ seg40  mean  MSE:0.39743345975875854  MAE:0.5283740162849426\n",
      "\n",
      "―――― Inference  |  Segment 41 ――――\n",
      "  seg41-ch0  MSE:0.200505331158638  MAE:0.3567332327365875\n",
      "▶ seg41  mean  MSE:0.200505331158638  MAE:0.3567332327365875\n",
      "\n",
      "―――― Inference  |  Segment 42 ――――\n",
      "  seg42-ch0  MSE:0.17381590604782104  MAE:0.3339276909828186\n",
      "▶ seg42  mean  MSE:0.17381590604782104  MAE:0.3339276909828186\n",
      "\n",
      "―――― Inference  |  Segment 43 ――――\n",
      "  seg43-ch0  MSE:0.09083577990531921  MAE:0.23684430122375488\n",
      "▶ seg43  mean  MSE:0.09083577990531921  MAE:0.23684430122375488\n",
      "\n",
      "―――― Inference  |  Segment 44 ――――\n",
      "  seg44-ch0  MSE:0.11022859811782837  MAE:0.26478198170661926\n",
      "▶ seg44  mean  MSE:0.11022859811782837  MAE:0.26478198170661926\n",
      "\n",
      "―――― Inference  |  Segment 45 ――――\n",
      "  seg45-ch0  MSE:0.0875205248594284  MAE:0.22856579720973969\n",
      "▶ seg45  mean  MSE:0.0875205248594284  MAE:0.22856579720973969\n",
      "\n",
      "―――― Inference  |  Segment 46 ――――\n",
      "  seg46-ch0  MSE:0.22338972985744476  MAE:0.3750814199447632\n",
      "▶ seg46  mean  MSE:0.22338972985744476  MAE:0.3750814199447632\n",
      "\n",
      "―――― Inference  |  Segment 47 ――――\n",
      "  seg47-ch0  MSE:0.11219377815723419  MAE:0.2666373550891876\n",
      "▶ seg47  mean  MSE:0.11219377815723419  MAE:0.2666373550891876\n",
      "\n",
      "―――― Inference  |  Segment 48 ――――\n",
      "  seg48-ch0  MSE:0.1535552740097046  MAE:0.32197305560112\n",
      "▶ seg48  mean  MSE:0.1535552740097046  MAE:0.32197305560112\n",
      "\n",
      "―――― Inference  |  Segment 49 ――――\n",
      "  seg49-ch0  MSE:0.07393457740545273  MAE:0.21222904324531555\n",
      "▶ seg49  mean  MSE:0.07393457740545273  MAE:0.21222904324531555\n",
      "\n",
      "―――― Inference  |  Segment 50 ――――\n",
      "  seg50-ch0  MSE:0.15915632247924805  MAE:0.31500253081321716\n",
      "▶ seg50  mean  MSE:0.15915632247924805  MAE:0.31500253081321716\n",
      "\n",
      "―――― Inference  |  Segment 51 ――――\n",
      "  seg51-ch0  MSE:0.09283777326345444  MAE:0.2375389039516449\n",
      "▶ seg51  mean  MSE:0.09283777326345444  MAE:0.2375389039516449\n",
      "\n",
      "―――― Inference  |  Segment 52 ――――\n",
      "  seg52-ch0  MSE:0.092010498046875  MAE:0.23557403683662415\n",
      "▶ seg52  mean  MSE:0.092010498046875  MAE:0.23557403683662415\n",
      "\n",
      "―――― Inference  |  Segment 53 ――――\n",
      "  seg53-ch0  MSE:0.24240174889564514  MAE:0.4045684337615967\n",
      "▶ seg53  mean  MSE:0.24240174889564514  MAE:0.4045684337615967\n",
      "\n",
      "―――― Inference  |  Segment 54 ――――\n",
      "  seg54-ch0  MSE:0.1228058785200119  MAE:0.26715531945228577\n",
      "▶ seg54  mean  MSE:0.1228058785200119  MAE:0.26715531945228577\n",
      "\n",
      "―――― Inference  |  Segment 55 ――――\n",
      "  seg55-ch0  MSE:0.08007021248340607  MAE:0.22383949160575867\n",
      "▶ seg55  mean  MSE:0.08007021248340607  MAE:0.22383949160575867\n",
      "\n",
      "―――― Inference  |  Segment 56 ――――\n",
      "  seg56-ch0  MSE:0.14995671808719635  MAE:0.3029978573322296\n",
      "▶ seg56  mean  MSE:0.14995671808719635  MAE:0.3029978573322296\n",
      "\n",
      "―――― Inference  |  Segment 57 ――――\n",
      "  seg57-ch0  MSE:0.11698838323354721  MAE:0.2680231034755707\n",
      "▶ seg57  mean  MSE:0.11698838323354721  MAE:0.2680231034755707\n",
      "\n",
      "―――― Inference  |  Segment 58 ――――\n",
      "  seg58-ch0  MSE:0.1385352909564972  MAE:0.29252684116363525\n",
      "▶ seg58  mean  MSE:0.1385352909564972  MAE:0.29252684116363525\n",
      "\n",
      "―――― Inference  |  Segment 59 ――――\n",
      "  seg59-ch0  MSE:0.07872951030731201  MAE:0.22076080739498138\n",
      "▶ seg59  mean  MSE:0.07872951030731201  MAE:0.22076080739498138\n",
      "\n",
      "―――― Inference  |  Segment 60 ――――\n",
      "  seg60-ch0  MSE:0.13202030956745148  MAE:0.2843652665615082\n",
      "▶ seg60  mean  MSE:0.13202030956745148  MAE:0.2843652665615082\n",
      "\n",
      "―――― Inference  |  Segment 61 ――――\n",
      "  seg61-ch0  MSE:0.14403803646564484  MAE:0.2938876152038574\n",
      "▶ seg61  mean  MSE:0.14403803646564484  MAE:0.2938876152038574\n",
      "\n",
      "―――― Inference  |  Segment 62 ――――\n",
      "  seg62-ch0  MSE:0.1758306920528412  MAE:0.34159189462661743\n",
      "▶ seg62  mean  MSE:0.1758306920528412  MAE:0.34159189462661743\n",
      "\n",
      "―――― Inference  |  Segment 63 ――――\n",
      "  seg63-ch0  MSE:0.10418631136417389  MAE:0.2559477686882019\n",
      "▶ seg63  mean  MSE:0.10418631136417389  MAE:0.2559477686882019\n",
      "\n",
      "―――― Inference  |  Segment 64 ――――\n",
      "  seg64-ch0  MSE:0.08854786306619644  MAE:0.238117977976799\n",
      "▶ seg64  mean  MSE:0.08854786306619644  MAE:0.238117977976799\n",
      "\n",
      "seg10 ch0 MSE : 0.22340208\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from pathlib import Path\n",
    "\n",
    "C        = 1                                \n",
    "P        = base_cfg.pred_len                \n",
    "seg_cnt  = 65\n",
    "batch_sz = 32\n",
    "\n",
    "results2_ETTh1_ES_test_1E = {}  \n",
    "\n",
    "for seg in range(seg_cnt):\n",
    "    print(f\"\\n―――― Inference  |  Segment {seg} ――――\")\n",
    "\n",
    "    \n",
    "    pv = test_pred_segments_ES[:, seg, :, :]         \n",
    "    ds = TensorDataset(test_sequences, test_labels, pv) \n",
    "    dl = DataLoader(ds, batch_size=batch_sz, shuffle=False)\n",
    "\n",
    "    \n",
    "    pred_ch, true_ch, metrics = [], [], {}\n",
    "\n",
    "    \n",
    "    for ch in range(C):\n",
    "        ckpt = Path(base_cfg.save_path) / f\"seg{seg}\" / f\"ch{ch}\" / \"checkpoint.pth\"\n",
    "        if not ckpt.exists():\n",
    "            print(f\"[Warn] seg{seg}-ch{ch} ckpt 없음 → skip\");  continue\n",
    "\n",
    "        \n",
    "        model = Model(base_cfg).to(device)\n",
    "        model.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, pv_batch in dl:                 \n",
    "                xb  = xb.float().to(device)\n",
    "                yb  = yb.float()        \n",
    "                pv_ = pv_batch.float().to(device)\n",
    "\n",
    "                out = model(xb, pv_)                     \n",
    "                preds.append(out[:, -P:, ch].cpu())\n",
    "                trues.append(yb[:, -P:, ch])\n",
    "\n",
    "        preds = torch.cat(preds, 0)     \n",
    "        trues = torch.cat(trues, 0)\n",
    "        pred_ch.append(preds)\n",
    "        true_ch.append(trues)\n",
    "\n",
    "        \n",
    "        p, t = preds.reshape(-1).cpu().numpy(), trues.reshape(-1).cpu().numpy()\n",
    "        mse  = np.mean((p - t)**2)\n",
    "        mae  = np.mean(np.abs(p - t))\n",
    "        metrics[ch] = (mse, mae)\n",
    "\n",
    "        print(f\"  seg{seg}-ch{ch}  MSE:{mse}  MAE:{mae}\")\n",
    "\n",
    "        del model; torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    pred_all = torch.stack(pred_ch,  dim=2)   \n",
    "    true_all = torch.stack(true_ch,  dim=2)\n",
    "\n",
    "    \n",
    "    mse_avg = np.mean([m for m,_ in metrics.values()])\n",
    "    mae_avg = np.mean([a for _,a in metrics.values()])\n",
    "    print(f\"▶ seg{seg}  mean  MSE:{mse_avg}  MAE:{mae_avg}\")\n",
    "\n",
    "    \n",
    "    results2_ETTh1_ES_test_1E[seg] = {\n",
    "        \"pred2_ES_test_1E\"    : pred_all,   \n",
    "        \"true2_ES_test_1E\"    : true_all,   \n",
    "        \"metrics2_ES_test_1E\" : metrics     \n",
    "    }\n",
    "\n",
    "\n",
    "seg10_ch0_mse = results2_ETTh1_ES_test_1E[10][\"metrics2_ES_test_1E\"][0][0]\n",
    "print(\"\\nseg10 ch0 MSE :\", seg10_ch0_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ecd38c",
   "metadata": {},
   "source": [
    "## 1E->1E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6346f36",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np, os\n",
    "from pathlib import Path\n",
    "\n",
    "pred_len = 96\n",
    "seq_len = 336 + pred_len//3\n",
    "\n",
    "\n",
    "class Configs:\n",
    "    def __init__(self, seq_len, pred_len, enc_in, individual,\n",
    "                 learning_rate, lradj, patience,\n",
    "                 save_path, attach_to_trend, attach_to_seasonal):\n",
    "        self.seq_len  = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.enc_in   = enc_in          \n",
    "        self.individual = individual\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lradj = lradj\n",
    "        self.patience = patience\n",
    "        self.save_path = save_path\n",
    "        self.attach_to_trend    = attach_to_trend\n",
    "        self.attach_to_seasonal = attach_to_seasonal\n",
    "\n",
    "\n",
    "base_cfg = Configs(\n",
    "    pred_len         = 96,\n",
    "    seq_len          = 336 + pred_len//3,\n",
    "    enc_in           = 1,\n",
    "    individual       = True,\n",
    "    learning_rate    = 0.005,\n",
    "    lradj            = 'type1',\n",
    "    patience         = 3,\n",
    "    save_path        = \"./model2_ETTh1_E1_1E\",\n",
    "    attach_to_trend  = True,\n",
    "    attach_to_seasonal = True\n",
    ")\n",
    "Path(base_cfg.save_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df334612",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e212421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "―――― Inference  |  Segment 0 ――――\n",
      "  seg0-ch0  MSE:0.10943467170000076  MAE:0.2597741484642029\n",
      "▶ seg0  mean  MSE:0.10943467170000076  MAE:0.2597741484642029\n",
      "\n",
      "―――― Inference  |  Segment 1 ――――\n",
      "  seg1-ch0  MSE:0.11361920833587646  MAE:0.2676355242729187\n",
      "▶ seg1  mean  MSE:0.11361920833587646  MAE:0.2676355242729187\n",
      "\n",
      "―――― Inference  |  Segment 2 ――――\n",
      "  seg2-ch0  MSE:0.07685666531324387  MAE:0.2143278568983078\n",
      "▶ seg2  mean  MSE:0.07685666531324387  MAE:0.2143278568983078\n",
      "\n",
      "―――― Inference  |  Segment 3 ――――\n",
      "  seg3-ch0  MSE:0.08964385092258453  MAE:0.2300041913986206\n",
      "▶ seg3  mean  MSE:0.08964385092258453  MAE:0.2300041913986206\n",
      "\n",
      "―――― Inference  |  Segment 4 ――――\n",
      "  seg4-ch0  MSE:0.07381153851747513  MAE:0.21185554563999176\n",
      "▶ seg4  mean  MSE:0.07381153851747513  MAE:0.21185554563999176\n",
      "\n",
      "―――― Inference  |  Segment 5 ――――\n",
      "  seg5-ch0  MSE:0.09934082627296448  MAE:0.25032854080200195\n",
      "▶ seg5  mean  MSE:0.09934082627296448  MAE:0.25032854080200195\n",
      "\n",
      "―――― Inference  |  Segment 6 ――――\n",
      "  seg6-ch0  MSE:0.11915169656276703  MAE:0.26935914158821106\n",
      "▶ seg6  mean  MSE:0.11915169656276703  MAE:0.26935914158821106\n",
      "\n",
      "―――― Inference  |  Segment 7 ――――\n",
      "  seg7-ch0  MSE:0.09391681849956512  MAE:0.2419242113828659\n",
      "▶ seg7  mean  MSE:0.09391681849956512  MAE:0.2419242113828659\n",
      "\n",
      "―――― Inference  |  Segment 8 ――――\n",
      "  seg8-ch0  MSE:0.10012219846248627  MAE:0.25128015875816345\n",
      "▶ seg8  mean  MSE:0.10012219846248627  MAE:0.25128015875816345\n",
      "\n",
      "―――― Inference  |  Segment 9 ――――\n",
      "  seg9-ch0  MSE:0.09355492144823074  MAE:0.2408188283443451\n",
      "▶ seg9  mean  MSE:0.09355492144823074  MAE:0.2408188283443451\n",
      "\n",
      "―――― Inference  |  Segment 10 ――――\n",
      "  seg10-ch0  MSE:0.0707215890288353  MAE:0.20956449210643768\n",
      "▶ seg10  mean  MSE:0.0707215890288353  MAE:0.20956449210643768\n",
      "\n",
      "―――― Inference  |  Segment 11 ――――\n",
      "  seg11-ch0  MSE:0.11186360567808151  MAE:0.2639288902282715\n",
      "▶ seg11  mean  MSE:0.11186360567808151  MAE:0.2639288902282715\n",
      "\n",
      "―――― Inference  |  Segment 12 ――――\n",
      "  seg12-ch0  MSE:0.28352075815200806  MAE:0.4478987753391266\n",
      "▶ seg12  mean  MSE:0.28352075815200806  MAE:0.4478987753391266\n",
      "\n",
      "―――― Inference  |  Segment 13 ――――\n",
      "  seg13-ch0  MSE:0.12295392155647278  MAE:0.27447736263275146\n",
      "▶ seg13  mean  MSE:0.12295392155647278  MAE:0.27447736263275146\n",
      "\n",
      "―――― Inference  |  Segment 14 ――――\n",
      "  seg14-ch0  MSE:0.0985235646367073  MAE:0.24215415120124817\n",
      "▶ seg14  mean  MSE:0.0985235646367073  MAE:0.24215415120124817\n",
      "\n",
      "―――― Inference  |  Segment 15 ――――\n",
      "  seg15-ch0  MSE:0.07114917039871216  MAE:0.2076728492975235\n",
      "▶ seg15  mean  MSE:0.07114917039871216  MAE:0.2076728492975235\n",
      "\n",
      "―――― Inference  |  Segment 16 ――――\n",
      "  seg16-ch0  MSE:0.0872272476553917  MAE:0.2327665537595749\n",
      "▶ seg16  mean  MSE:0.0872272476553917  MAE:0.2327665537595749\n",
      "\n",
      "―――― Inference  |  Segment 17 ――――\n",
      "  seg17-ch0  MSE:0.1442442685365677  MAE:0.2924130856990814\n",
      "▶ seg17  mean  MSE:0.1442442685365677  MAE:0.2924130856990814\n",
      "\n",
      "―――― Inference  |  Segment 18 ――――\n",
      "  seg18-ch0  MSE:0.07694658637046814  MAE:0.21601608395576477\n",
      "▶ seg18  mean  MSE:0.07694658637046814  MAE:0.21601608395576477\n",
      "\n",
      "―――― Inference  |  Segment 19 ――――\n",
      "  seg19-ch0  MSE:0.1579154133796692  MAE:0.3107892572879791\n",
      "▶ seg19  mean  MSE:0.1579154133796692  MAE:0.3107892572879791\n",
      "\n",
      "―――― Inference  |  Segment 20 ――――\n",
      "  seg20-ch0  MSE:0.0852375477552414  MAE:0.2280568778514862\n",
      "▶ seg20  mean  MSE:0.0852375477552414  MAE:0.2280568778514862\n",
      "\n",
      "―――― Inference  |  Segment 21 ――――\n",
      "  seg21-ch0  MSE:0.35756707191467285  MAE:0.4861197769641876\n",
      "▶ seg21  mean  MSE:0.35756707191467285  MAE:0.4861197769641876\n",
      "\n",
      "―――― Inference  |  Segment 22 ――――\n",
      "  seg22-ch0  MSE:0.09398023784160614  MAE:0.24160057306289673\n",
      "▶ seg22  mean  MSE:0.09398023784160614  MAE:0.24160057306289673\n",
      "\n",
      "―――― Inference  |  Segment 23 ――――\n",
      "  seg23-ch0  MSE:0.13223141431808472  MAE:0.27924537658691406\n",
      "▶ seg23  mean  MSE:0.13223141431808472  MAE:0.27924537658691406\n",
      "\n",
      "―――― Inference  |  Segment 24 ――――\n",
      "  seg24-ch0  MSE:0.1136656180024147  MAE:0.26322072744369507\n",
      "▶ seg24  mean  MSE:0.1136656180024147  MAE:0.26322072744369507\n",
      "\n",
      "―――― Inference  |  Segment 25 ――――\n",
      "  seg25-ch0  MSE:0.15218253433704376  MAE:0.3198705315589905\n",
      "▶ seg25  mean  MSE:0.15218253433704376  MAE:0.3198705315589905\n",
      "\n",
      "―――― Inference  |  Segment 26 ――――\n",
      "  seg26-ch0  MSE:0.07714924961328506  MAE:0.21939392387866974\n",
      "▶ seg26  mean  MSE:0.07714924961328506  MAE:0.21939392387866974\n",
      "\n",
      "―――― Inference  |  Segment 27 ――――\n",
      "  seg27-ch0  MSE:0.08949056267738342  MAE:0.23228833079338074\n",
      "▶ seg27  mean  MSE:0.08949056267738342  MAE:0.23228833079338074\n",
      "\n",
      "―――― Inference  |  Segment 28 ――――\n",
      "  seg28-ch0  MSE:0.07288138568401337  MAE:0.2127491533756256\n",
      "▶ seg28  mean  MSE:0.07288138568401337  MAE:0.2127491533756256\n",
      "\n",
      "―――― Inference  |  Segment 29 ――――\n",
      "  seg29-ch0  MSE:0.07129518687725067  MAE:0.2095697671175003\n",
      "▶ seg29  mean  MSE:0.07129518687725067  MAE:0.2095697671175003\n",
      "\n",
      "―――― Inference  |  Segment 30 ――――\n",
      "  seg30-ch0  MSE:0.11830732971429825  MAE:0.2729688584804535\n",
      "▶ seg30  mean  MSE:0.11830732971429825  MAE:0.2729688584804535\n",
      "\n",
      "―――― Inference  |  Segment 31 ――――\n",
      "  seg31-ch0  MSE:0.12241383641958237  MAE:0.27870017290115356\n",
      "▶ seg31  mean  MSE:0.12241383641958237  MAE:0.27870017290115356\n",
      "\n",
      "―――― Inference  |  Segment 32 ――――\n",
      "  seg32-ch0  MSE:0.0717102438211441  MAE:0.20768971741199493\n",
      "▶ seg32  mean  MSE:0.0717102438211441  MAE:0.20768971741199493\n",
      "\n",
      "―――― Inference  |  Segment 33 ――――\n",
      "  seg33-ch0  MSE:0.1688752919435501  MAE:0.3296889662742615\n",
      "▶ seg33  mean  MSE:0.1688752919435501  MAE:0.3296889662742615\n",
      "\n",
      "―――― Inference  |  Segment 34 ――――\n",
      "  seg34-ch0  MSE:0.1407419592142105  MAE:0.30467894673347473\n",
      "▶ seg34  mean  MSE:0.1407419592142105  MAE:0.30467894673347473\n",
      "\n",
      "―――― Inference  |  Segment 35 ――――\n",
      "  seg35-ch0  MSE:0.12591266632080078  MAE:0.27183037996292114\n",
      "▶ seg35  mean  MSE:0.12591266632080078  MAE:0.27183037996292114\n",
      "\n",
      "―――― Inference  |  Segment 36 ――――\n",
      "  seg36-ch0  MSE:0.30152806639671326  MAE:0.4677128791809082\n",
      "▶ seg36  mean  MSE:0.30152806639671326  MAE:0.4677128791809082\n",
      "\n",
      "―――― Inference  |  Segment 37 ――――\n",
      "  seg37-ch0  MSE:0.09554900974035263  MAE:0.2453681081533432\n",
      "▶ seg37  mean  MSE:0.09554900974035263  MAE:0.2453681081533432\n",
      "\n",
      "―――― Inference  |  Segment 38 ――――\n",
      "  seg38-ch0  MSE:0.08685626834630966  MAE:0.23110689222812653\n",
      "▶ seg38  mean  MSE:0.08685626834630966  MAE:0.23110689222812653\n",
      "\n",
      "―――― Inference  |  Segment 39 ――――\n",
      "  seg39-ch0  MSE:0.12210192531347275  MAE:0.2723121643066406\n",
      "▶ seg39  mean  MSE:0.12210192531347275  MAE:0.2723121643066406\n",
      "\n",
      "―――― Inference  |  Segment 40 ――――\n",
      "  seg40-ch0  MSE:0.08981167525053024  MAE:0.2399033010005951\n",
      "▶ seg40  mean  MSE:0.08981167525053024  MAE:0.2399033010005951\n",
      "\n",
      "―――― Inference  |  Segment 41 ――――\n",
      "  seg41-ch0  MSE:0.36624675989151  MAE:0.504358172416687\n",
      "▶ seg41  mean  MSE:0.36624675989151  MAE:0.504358172416687\n",
      "\n",
      "―――― Inference  |  Segment 42 ――――\n",
      "  seg42-ch0  MSE:0.12526734173297882  MAE:0.27990469336509705\n",
      "▶ seg42  mean  MSE:0.12526734173297882  MAE:0.27990469336509705\n",
      "\n",
      "―――― Inference  |  Segment 43 ――――\n",
      "  seg43-ch0  MSE:0.08456288278102875  MAE:0.23355308175086975\n",
      "▶ seg43  mean  MSE:0.08456288278102875  MAE:0.23355308175086975\n",
      "\n",
      "―――― Inference  |  Segment 44 ――――\n",
      "  seg44-ch0  MSE:0.14496974647045135  MAE:0.2967328131198883\n",
      "▶ seg44  mean  MSE:0.14496974647045135  MAE:0.2967328131198883\n",
      "\n",
      "―――― Inference  |  Segment 45 ――――\n",
      "  seg45-ch0  MSE:0.1625843644142151  MAE:0.3217807412147522\n",
      "▶ seg45  mean  MSE:0.1625843644142151  MAE:0.3217807412147522\n",
      "\n",
      "―――― Inference  |  Segment 46 ――――\n",
      "  seg46-ch0  MSE:0.08819957822561264  MAE:0.23279061913490295\n",
      "▶ seg46  mean  MSE:0.08819957822561264  MAE:0.23279061913490295\n",
      "\n",
      "―――― Inference  |  Segment 47 ――――\n",
      "  seg47-ch0  MSE:0.11233223229646683  MAE:0.2594875693321228\n",
      "▶ seg47  mean  MSE:0.11233223229646683  MAE:0.2594875693321228\n",
      "\n",
      "―――― Inference  |  Segment 48 ――――\n",
      "  seg48-ch0  MSE:0.079193115234375  MAE:0.22007836401462555\n",
      "▶ seg48  mean  MSE:0.079193115234375  MAE:0.22007836401462555\n",
      "\n",
      "―――― Inference  |  Segment 49 ――――\n",
      "  seg49-ch0  MSE:0.07490980625152588  MAE:0.21423214673995972\n",
      "▶ seg49  mean  MSE:0.07490980625152588  MAE:0.21423214673995972\n",
      "\n",
      "―――― Inference  |  Segment 50 ――――\n",
      "  seg50-ch0  MSE:0.12984798848628998  MAE:0.27997225522994995\n",
      "▶ seg50  mean  MSE:0.12984798848628998  MAE:0.27997225522994995\n",
      "\n",
      "―――― Inference  |  Segment 51 ――――\n",
      "  seg51-ch0  MSE:0.16180160641670227  MAE:0.32907018065452576\n",
      "▶ seg51  mean  MSE:0.16180160641670227  MAE:0.32907018065452576\n",
      "\n",
      "―――― Inference  |  Segment 52 ――――\n",
      "  seg52-ch0  MSE:0.076802559196949  MAE:0.21987606585025787\n",
      "▶ seg52  mean  MSE:0.076802559196949  MAE:0.21987606585025787\n",
      "\n",
      "―――― Inference  |  Segment 53 ――――\n",
      "  seg53-ch0  MSE:0.1496688276529312  MAE:0.30612340569496155\n",
      "▶ seg53  mean  MSE:0.1496688276529312  MAE:0.30612340569496155\n",
      "\n",
      "―――― Inference  |  Segment 54 ――――\n",
      "  seg54-ch0  MSE:0.11992724239826202  MAE:0.2744537889957428\n",
      "▶ seg54  mean  MSE:0.11992724239826202  MAE:0.2744537889957428\n",
      "\n",
      "―――― Inference  |  Segment 55 ――――\n",
      "  seg55-ch0  MSE:0.1261282116174698  MAE:0.2807105481624603\n",
      "▶ seg55  mean  MSE:0.1261282116174698  MAE:0.2807105481624603\n",
      "\n",
      "―――― Inference  |  Segment 56 ――――\n",
      "  seg56-ch0  MSE:0.10327581316232681  MAE:0.2470053881406784\n",
      "▶ seg56  mean  MSE:0.10327581316232681  MAE:0.2470053881406784\n",
      "\n",
      "―――― Inference  |  Segment 57 ――――\n",
      "  seg57-ch0  MSE:0.0907473936676979  MAE:0.23387061059474945\n",
      "▶ seg57  mean  MSE:0.0907473936676979  MAE:0.23387061059474945\n",
      "\n",
      "―――― Inference  |  Segment 58 ――――\n",
      "  seg58-ch0  MSE:0.08706838637590408  MAE:0.23091983795166016\n",
      "▶ seg58  mean  MSE:0.08706838637590408  MAE:0.23091983795166016\n",
      "\n",
      "―――― Inference  |  Segment 59 ――――\n",
      "  seg59-ch0  MSE:0.08345318585634232  MAE:0.22850917279720306\n",
      "▶ seg59  mean  MSE:0.08345318585634232  MAE:0.22850917279720306\n",
      "\n",
      "―――― Inference  |  Segment 60 ――――\n",
      "  seg60-ch0  MSE:0.13758409023284912  MAE:0.29381704330444336\n",
      "▶ seg60  mean  MSE:0.13758409023284912  MAE:0.29381704330444336\n",
      "\n",
      "―――― Inference  |  Segment 61 ――――\n",
      "  seg61-ch0  MSE:0.07934851944446564  MAE:0.22308394312858582\n",
      "▶ seg61  mean  MSE:0.07934851944446564  MAE:0.22308394312858582\n",
      "\n",
      "―――― Inference  |  Segment 62 ――――\n",
      "  seg62-ch0  MSE:0.11095485836267471  MAE:0.2666115462779999\n",
      "▶ seg62  mean  MSE:0.11095485836267471  MAE:0.2666115462779999\n",
      "\n",
      "―――― Inference  |  Segment 63 ――――\n",
      "  seg63-ch0  MSE:0.10731780529022217  MAE:0.25762757658958435\n",
      "▶ seg63  mean  MSE:0.10731780529022217  MAE:0.25762757658958435\n",
      "\n",
      "―――― Inference  |  Segment 64 ――――\n",
      "  seg64-ch0  MSE:0.0955871120095253  MAE:0.24314463138580322\n",
      "▶ seg64  mean  MSE:0.0955871120095253  MAE:0.24314463138580322\n",
      "\n",
      "seg10 ch0 MSE : 0.07072159\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from pathlib import Path\n",
    "\n",
    "C        = 1                                \n",
    "P        = base_cfg.pred_len                \n",
    "seg_cnt  = 65\n",
    "batch_sz = 32\n",
    "\n",
    "results2_ETTh1_E1_test_1E = {}  \n",
    "\n",
    "for seg in range(seg_cnt):\n",
    "    print(f\"\\n―――― Inference  |  Segment {seg} ――――\")\n",
    "\n",
    "    \n",
    "    pv = test_pred_segments_E1[:, seg, :, :]         \n",
    "    ds = TensorDataset(test_sequences, test_labels, pv) \n",
    "    dl = DataLoader(ds, batch_size=batch_sz, shuffle=False)\n",
    "\n",
    "    \n",
    "    pred_ch, true_ch, metrics = [], [], {}\n",
    "\n",
    "    \n",
    "    for ch in range(C):\n",
    "        ckpt = Path(base_cfg.save_path) / f\"seg{seg}\" / f\"ch{ch}\" / \"checkpoint.pth\"\n",
    "        if not ckpt.exists():\n",
    "            print(f\"[Warn] seg{seg}-ch{ch} ckpt 없음 → skip\");  continue\n",
    "\n",
    "        \n",
    "        model = Model(base_cfg).to(device)\n",
    "        model.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, pv_batch in dl:                 \n",
    "                xb  = xb.float().to(device)\n",
    "                yb  = yb.float()        \n",
    "                pv_ = pv_batch.float().to(device)\n",
    "\n",
    "                out = model(xb, pv_)                     \n",
    "                preds.append(out[:, -P:, ch].cpu())\n",
    "                trues.append(yb[:, -P:, ch])\n",
    "\n",
    "        preds = torch.cat(preds, 0)     \n",
    "        trues = torch.cat(trues, 0)\n",
    "        pred_ch.append(preds)\n",
    "        true_ch.append(trues)\n",
    "\n",
    "        \n",
    "        p, t = preds.reshape(-1).cpu().numpy(), trues.reshape(-1).cpu().numpy()\n",
    "        mse  = np.mean((p - t)**2)\n",
    "        mae  = np.mean(np.abs(p - t))\n",
    "        metrics[ch] = (mse, mae)\n",
    "\n",
    "        print(f\"  seg{seg}-ch{ch}  MSE:{mse}  MAE:{mae}\")\n",
    "\n",
    "        del model; torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    pred_all = torch.stack(pred_ch,  dim=2)   \n",
    "    true_all = torch.stack(true_ch,  dim=2)\n",
    "\n",
    "    \n",
    "    mse_avg = np.mean([m for m,_ in metrics.values()])\n",
    "    mae_avg = np.mean([a for _,a in metrics.values()])\n",
    "    print(f\"▶ seg{seg}  mean  MSE:{mse_avg}  MAE:{mae_avg}\")\n",
    "\n",
    "    \n",
    "    results2_ETTh1_E1_test_1E[seg] = {\n",
    "        \"pred2_E1_test_1E\"    : pred_all,   \n",
    "        \"true2_E1_test_1E\"    : true_all,   \n",
    "        \"metrics2_E1_test_1E\" : metrics     \n",
    "    }\n",
    "\n",
    "\n",
    "seg10_ch0_mse = results2_ETTh1_E1_test_1E[10][\"metrics2_E1_test_1E\"][0][0]\n",
    "print(\"\\nseg10 ch0 MSE :\", seg10_ch0_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee1bf9",
   "metadata": {},
   "source": [
    "# Rank Models (by 2nd validation performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b60c2a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "results1_ETTh1_train = torch.load(\"./1st_results_ETTh1/train_1st_results.pt\")\n",
    "results1_ETTh1_val = torch.load(\"./1st_results_ETTh1/val_1st_results.pt\")\n",
    "results1_ETTh1_test = torch.load(\"./1st_results_ETTh1/test_1st_results.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa7661",
   "metadata": {},
   "source": [
    "### ES->1E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f11d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Train-set  Per-Channel  Top-N  MSE\n",
      "\n",
      "=== Channel 0 ===\n",
      "Vanilla MSE : 0.106383\n",
      "\n",
      "  Top  1 | seg 11 | MSE 0.096954 (+0.009429)\n",
      "  Top  2 | seg 30 | MSE 0.097464 (+0.008919)\n",
      "  Top  3 | seg 08 | MSE 0.097571 (+0.008812)\n",
      "  Top  4 | seg 02 | MSE 0.098284 (+0.008099)\n",
      "  Top  5 | seg 64 | MSE 0.098755 (+0.007628)\n",
      "  Top  6 | seg 18 | MSE 0.098762 (+0.007621)\n",
      "  Top  7 | seg 49 | MSE 0.099025 (+0.007358)\n",
      "  Top  8 | seg 31 | MSE 0.099413 (+0.006970)\n",
      "  Top  9 | seg 51 | MSE 0.099603 (+0.006780)\n",
      "  Top 10 | seg 45 | MSE 0.099860 (+0.006523)\n",
      "  Top 11 | seg 01 | MSE 0.100002 (+0.006381)\n",
      "  Top 12 | seg 55 | MSE 0.100228 (+0.006155)\n",
      "  Top 13 | seg 13 | MSE 0.100274 (+0.006109)\n",
      "  Top 14 | seg 00 | MSE 0.100409 (+0.005974)\n",
      "  Top 15 | seg 59 | MSE 0.100789 (+0.005594)\n",
      "  Top 16 | seg 19 | MSE 0.100834 (+0.005549)\n",
      "  Top 17 | seg 34 | MSE 0.101071 (+0.005312)\n",
      "  Top 18 | seg 25 | MSE 0.101182 (+0.005201)\n",
      "  Top 19 | seg 17 | MSE 0.101188 (+0.005195)\n",
      "  Top 20 | seg 15 | MSE 0.101217 (+0.005166)\n",
      "  Top 21 | seg 43 | MSE 0.101251 (+0.005132)\n",
      "  Top 22 | seg 44 | MSE 0.101741 (+0.004642)\n",
      "  Top 23 | seg 47 | MSE 0.102022 (+0.004361)\n",
      "  Top 24 | seg 07 | MSE 0.102065 (+0.004318)\n",
      "  Top 25 | seg 39 | MSE 0.102075 (+0.004308)\n",
      "  Top 26 | seg 37 | MSE 0.102120 (+0.004263)\n",
      "  Top 27 | seg 22 | MSE 0.102270 (+0.004113)\n",
      "  Top 28 | seg 26 | MSE 0.102442 (+0.003941)\n",
      "  Top 29 | seg 04 | MSE 0.102467 (+0.003916)\n",
      "  Top 30 | seg 05 | MSE 0.102492 (+0.003891)\n",
      "  Top 31 | seg 16 | MSE 0.102940 (+0.003443)\n",
      "  Top 32 | seg 35 | MSE 0.102950 (+0.003433)\n",
      "  Top 33 | seg 33 | MSE 0.103068 (+0.003315)\n",
      "  Top 34 | seg 57 | MSE 0.103221 (+0.003162)\n",
      "  Top 35 | seg 52 | MSE 0.103873 (+0.002510)\n",
      "  Top 36 | seg 63 | MSE 0.103885 (+0.002498)\n",
      "  Top 37 | seg 54 | MSE 0.104243 (+0.002140)\n",
      "  Top 38 | seg 58 | MSE 0.104372 (+0.002010)\n",
      "  Top 39 | seg 03 | MSE 0.104824 (+0.001559)\n",
      "  Top 40 | seg 09 | MSE 0.104826 (+0.001557)\n",
      "  Top 41 | seg 60 | MSE 0.104880 (+0.001503)\n",
      "  Top 42 | seg 48 | MSE 0.104882 (+0.001501)\n",
      "  Top 43 | seg 32 | MSE 0.105037 (+0.001346)\n",
      "  Top 44 | seg 62 | MSE 0.105547 (+0.000836)\n",
      "  Top 45 | seg 56 | MSE 0.105595 (+0.000788)\n",
      "  Top 46 | seg 12 | MSE 0.105826 (+0.000557)\n",
      "  Top 47 | seg 21 | MSE 0.105861 (+0.000522)\n",
      "  Top 48 | seg 23 | MSE 0.105939 (+0.000444)\n",
      "  Top 49 | seg 06 | MSE 0.106058 (+0.000325)\n",
      "  Top 50 | seg 24 | MSE 0.106679 (-0.000296)\n",
      "  Top 51 | seg 20 | MSE 0.106697 (-0.000314)\n",
      "  Top 52 | seg 61 | MSE 0.106972 (-0.000589)\n",
      "  Top 53 | seg 14 | MSE 0.107299 (-0.000916)\n",
      "  Top 54 | seg 38 | MSE 0.107315 (-0.000932)\n",
      "  Top 55 | seg 27 | MSE 0.108913 (-0.002530)\n",
      "  Top 56 | seg 42 | MSE 0.109053 (-0.002670)\n",
      "  Top 57 | seg 50 | MSE 0.109097 (-0.002714)\n",
      "  Top 58 | seg 10 | MSE 0.109821 (-0.003438)\n",
      "  Top 59 | seg 28 | MSE 0.112157 (-0.005774)\n",
      "  Top 60 | seg 41 | MSE 0.113929 (-0.007546)\n",
      "  Top 61 | seg 36 | MSE 0.114220 (-0.007837)\n",
      "  Top 62 | seg 46 | MSE 0.114371 (-0.007988)\n",
      "  Top 63 | seg 53 | MSE 0.116181 (-0.009798)\n",
      "  Top 64 | seg 40 | MSE 0.132819 (-0.026437)\n",
      "  Top 65 | seg 29 | MSE 0.136871 (-0.030488)\n",
      "\n",
      "Saved top segment indices per channel:\n",
      "Channel 0: [11, 30, 8, 2, 64, 18, 49, 31, 51, 45, 1, 55, 13, 0, 59, 19, 34, 25, 17, 15, 43, 44, 47, 7, 39, 37, 22, 26, 4, 5, 16, 35, 33, 57, 52, 63, 54, 58, 3, 9, 60, 48, 32, 62, 56, 12, 21, 23, 6, 24, 20, 61, 14, 38, 27, 42, 50, 10, 28, 41, 36, 46, 53, 40, 29]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TOP_N       = 65\n",
    "SEG_CNT     = 65\n",
    "CH_CNT      = 1\n",
    "metric_idx  = 0          \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mse_table = np.full((SEG_CNT, CH_CNT), np.inf)\n",
    "\n",
    "for seg in range(SEG_CNT):\n",
    "    if seg not in results2_ETTh1_ES_val_1E:\n",
    "        continue\n",
    "    for ch in range(CH_CNT):\n",
    "        if ch in results2_ETTh1_ES_val_1E[seg][\"metrics2_ES_val_1E\"]:\n",
    "            mse_table[seg, ch] = results2_ETTh1_ES_val_1E[seg][\"metrics2_ES_val_1E\"][ch][metric_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n▶ Train-set  Per-Channel  Top-N  MSE\\n\")\n",
    "top_seg_idx_ES_val_1E = {}                       \n",
    "\n",
    "for ch in range(CH_CNT):\n",
    "    print(f\"=== Channel {ch} ===\")\n",
    "    v_mse = results1_ETTh1_val[1]['mse_val_E1']\n",
    "    print(f\"Vanilla MSE : {v_mse:.6f}\\n\")\n",
    "\n",
    "    \n",
    "    pairs = [(seg, mse_table[seg, ch]) for seg in range(SEG_CNT)\n",
    "             if np.isfinite(mse_table[seg, ch])]\n",
    "    top   = sorted(pairs, key=lambda x: x[1])[:TOP_N]\n",
    "\n",
    "    \n",
    "    top_seg_idx_ES_val_1E[ch] = [seg for seg, _ in top]   \n",
    "\n",
    "    \n",
    "    for rank, (seg_id, mse_val) in enumerate(top, 1):\n",
    "        delta = v_mse - mse_val\n",
    "        sign  = \"+\" if delta > 0 else \"-\"\n",
    "        print(f\"  Top {rank:2d} | seg {seg_id:02d} | MSE {mse_val:.6f} \"\n",
    "              f\"({sign}{abs(delta):.6f})\")\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Saved top segment indices per channel:\")\n",
    "for ch, seg_list in top_seg_idx_ES_val_1E.items():\n",
    "    print(f\"Channel {ch}: {seg_list}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed59ab7",
   "metadata": {},
   "source": [
    "### 1E->1E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697c30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Train-set  Per-Channel  Top-N  MSE\n",
      "\n",
      "=== Channel 0 ===\n",
      "Vanilla MSE : 0.106383\n",
      "\n",
      "  Top  1 | seg 48 | MSE 0.096268 (+0.010115)\n",
      "  Top  2 | seg 26 | MSE 0.096536 (+0.009847)\n",
      "  Top  3 | seg 04 | MSE 0.096735 (+0.009648)\n",
      "  Top  4 | seg 29 | MSE 0.097117 (+0.009266)\n",
      "  Top  5 | seg 43 | MSE 0.097122 (+0.009261)\n",
      "  Top  6 | seg 49 | MSE 0.097180 (+0.009203)\n",
      "  Top  7 | seg 18 | MSE 0.097234 (+0.009149)\n",
      "  Top  8 | seg 57 | MSE 0.097252 (+0.009131)\n",
      "  Top  9 | seg 28 | MSE 0.097329 (+0.009054)\n",
      "  Top 10 | seg 52 | MSE 0.097628 (+0.008755)\n",
      "  Top 11 | seg 46 | MSE 0.097686 (+0.008697)\n",
      "  Top 12 | seg 32 | MSE 0.097970 (+0.008413)\n",
      "  Top 13 | seg 15 | MSE 0.098008 (+0.008375)\n",
      "  Top 14 | seg 64 | MSE 0.098187 (+0.008196)\n",
      "  Top 15 | seg 10 | MSE 0.098247 (+0.008136)\n",
      "  Top 16 | seg 22 | MSE 0.098399 (+0.007984)\n",
      "  Top 17 | seg 16 | MSE 0.098714 (+0.007669)\n",
      "  Top 18 | seg 61 | MSE 0.098837 (+0.007546)\n",
      "  Top 19 | seg 30 | MSE 0.098938 (+0.007445)\n",
      "  Top 20 | seg 59 | MSE 0.099036 (+0.007347)\n",
      "  Top 21 | seg 38 | MSE 0.099135 (+0.007248)\n",
      "  Top 22 | seg 11 | MSE 0.099326 (+0.007057)\n",
      "  Top 23 | seg 56 | MSE 0.099479 (+0.006904)\n",
      "  Top 24 | seg 02 | MSE 0.099518 (+0.006865)\n",
      "  Top 25 | seg 40 | MSE 0.099531 (+0.006852)\n",
      "  Top 26 | seg 03 | MSE 0.100756 (+0.005627)\n",
      "  Top 27 | seg 09 | MSE 0.100807 (+0.005576)\n",
      "  Top 28 | seg 14 | MSE 0.100848 (+0.005535)\n",
      "  Top 29 | seg 08 | MSE 0.100887 (+0.005496)\n",
      "  Top 30 | seg 27 | MSE 0.100933 (+0.005450)\n",
      "  Top 31 | seg 42 | MSE 0.101181 (+0.005202)\n",
      "  Top 32 | seg 47 | MSE 0.101740 (+0.004643)\n",
      "  Top 33 | seg 62 | MSE 0.102119 (+0.004264)\n",
      "  Top 34 | seg 35 | MSE 0.102281 (+0.004102)\n",
      "  Top 35 | seg 20 | MSE 0.102439 (+0.003944)\n",
      "  Top 36 | seg 01 | MSE 0.102788 (+0.003595)\n",
      "  Top 37 | seg 37 | MSE 0.102851 (+0.003532)\n",
      "  Top 38 | seg 24 | MSE 0.102978 (+0.003405)\n",
      "  Top 39 | seg 07 | MSE 0.103047 (+0.003336)\n",
      "  Top 40 | seg 63 | MSE 0.103298 (+0.003085)\n",
      "  Top 41 | seg 06 | MSE 0.103437 (+0.002946)\n",
      "  Top 42 | seg 39 | MSE 0.103439 (+0.002944)\n",
      "  Top 43 | seg 58 | MSE 0.103710 (+0.002673)\n",
      "  Top 44 | seg 55 | MSE 0.103772 (+0.002611)\n",
      "  Top 45 | seg 23 | MSE 0.103933 (+0.002450)\n",
      "  Top 46 | seg 60 | MSE 0.104140 (+0.002243)\n",
      "  Top 47 | seg 05 | MSE 0.104858 (+0.001525)\n",
      "  Top 48 | seg 31 | MSE 0.104968 (+0.001415)\n",
      "  Top 49 | seg 50 | MSE 0.104995 (+0.001388)\n",
      "  Top 50 | seg 54 | MSE 0.105051 (+0.001332)\n",
      "  Top 51 | seg 13 | MSE 0.105138 (+0.001245)\n",
      "  Top 52 | seg 00 | MSE 0.105467 (+0.000916)\n",
      "  Top 53 | seg 44 | MSE 0.105725 (+0.000658)\n",
      "  Top 54 | seg 45 | MSE 0.105817 (+0.000566)\n",
      "  Top 55 | seg 25 | MSE 0.106379 (+0.000004)\n",
      "  Top 56 | seg 51 | MSE 0.107167 (-0.000784)\n",
      "  Top 57 | seg 17 | MSE 0.108271 (-0.001888)\n",
      "  Top 58 | seg 53 | MSE 0.108440 (-0.002058)\n",
      "  Top 59 | seg 19 | MSE 0.109423 (-0.003040)\n",
      "  Top 60 | seg 34 | MSE 0.110119 (-0.003736)\n",
      "  Top 61 | seg 33 | MSE 0.111267 (-0.004884)\n",
      "  Top 62 | seg 12 | MSE 0.122043 (-0.015660)\n",
      "  Top 63 | seg 36 | MSE 0.122922 (-0.016539)\n",
      "  Top 64 | seg 21 | MSE 0.130596 (-0.024213)\n",
      "  Top 65 | seg 41 | MSE 0.130800 (-0.024417)\n",
      "\n",
      "Saved top segment indices per channel:\n",
      "Channel 0: [48, 26, 4, 29, 43, 49, 18, 57, 28, 52, 46, 32, 15, 64, 10, 22, 16, 61, 30, 59, 38, 11, 56, 2, 40, 3, 9, 14, 8, 27, 42, 47, 62, 35, 20, 1, 37, 24, 7, 63, 6, 39, 58, 55, 23, 60, 5, 31, 50, 54, 13, 0, 44, 45, 25, 51, 17, 53, 19, 34, 33, 12, 36, 21, 41]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TOP_N       = 65\n",
    "SEG_CNT     = 65\n",
    "CH_CNT      = 1\n",
    "metric_idx  = 0          \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mse_table = np.full((SEG_CNT, CH_CNT), np.inf)\n",
    "\n",
    "for seg in range(SEG_CNT):\n",
    "    if seg not in results2_ETTh1_E1_val_1E:\n",
    "        continue\n",
    "    for ch in range(CH_CNT):\n",
    "        if ch in results2_ETTh1_E1_val_1E[seg][\"metrics2_E1_val_1E\"]:\n",
    "            mse_table[seg, ch] = results2_ETTh1_E1_val_1E[seg][\"metrics2_E1_val_1E\"][ch][metric_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n▶ Train-set  Per-Channel  Top-N  MSE\\n\")\n",
    "top_seg_idx_E1_val_1E = {}                       \n",
    "\n",
    "for ch in range(CH_CNT):\n",
    "    print(f\"=== Channel {ch} ===\")\n",
    "    v_mse = results1_ETTh1_val[1]['mse_val_E1']\n",
    "    print(f\"Vanilla MSE : {v_mse:.6f}\\n\")\n",
    "\n",
    "    \n",
    "    pairs = [(seg, mse_table[seg, ch]) for seg in range(SEG_CNT)\n",
    "             if np.isfinite(mse_table[seg, ch])]\n",
    "    top   = sorted(pairs, key=lambda x: x[1])[:TOP_N]\n",
    "\n",
    "    \n",
    "    top_seg_idx_E1_val_1E[ch] = [seg for seg, _ in top]   \n",
    "\n",
    "    \n",
    "    for rank, (seg_id, mse_val) in enumerate(top, 1):\n",
    "        delta = v_mse - mse_val\n",
    "        sign  = \"+\" if delta > 0 else \"-\"\n",
    "        print(f\"  Top {rank:2d} | seg {seg_id:02d} | MSE {mse_val:.6f} \"\n",
    "              f\"({sign}{abs(delta):.6f})\")\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Saved top segment indices per channel:\")\n",
    "for ch, seg_list in top_seg_idx_E1_val_1E.items():\n",
    "    print(f\"Channel {ch}: {seg_list}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d84276",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1df6a",
   "metadata": {},
   "source": [
    "## ES->1E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32b0eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Channel 0 ===\n",
      "[Vanilla]   MSE : 0.057950 | MAE : 0.180536\n",
      "  — Var / |Corr| / Score —\n",
      "   K= 5 | Var=2.571e-02 | |ρ|=0.8125 | S=1.0000 \n",
      "   K=10 | Var=3.108e-02 | |ρ|=0.7851 | S=0.9399 \n",
      "   K=15 | Var=3.454e-02 | |ρ|=0.7673 | S=0.9001 \n",
      "   K=20 | Var=3.670e-02 | |ρ|=0.7517 | S=0.8467 \n",
      "   K=25 | Var=3.850e-02 | |ρ|=0.7421 | S=0.8237 \n",
      "   K=30 | Var=4.091e-02 | |ρ|=0.7269 | S=0.7785 <- pick\n",
      "   K=35 | Var=4.244e-02 | |ρ|=0.7222 | S=0.7805 \n",
      "   K=40 | Var=4.614e-02 | |ρ|=0.7130 | S=0.8000 \n",
      "   K=45 | Var=5.175e-02 | |ρ|=0.7065 | S=0.8755 \n",
      "   K=50 | Var=5.404e-02 | |ρ|=0.7021 | S=0.8956 \n",
      "   K=55 | Var=5.778e-02 | |ρ|=0.6869 | S=0.8779 \n",
      "   K=60 | Var=6.330e-02 | |ρ|=0.6763 | S=0.9255 \n",
      "   K=65 | Var=7.386e-02 | |ρ|=0.6532 | S=1.0000 \n",
      "[Var-Corr]  K=30\n",
      "  MSE = 0.054276 | Δ = +0.003674 | Δ% = +6.34%\n",
      "  MAE = 0.177827 | Δ = +0.002709 | Δ% = +1.50%\n",
      "\n",
      "──────── Summary (Var-Corr) ───────\n",
      "Vanilla  mean MSE : 0.057950\n",
      "Ensemble mean MSE : 0.054276\n",
      "Average Δ MSE     : +0.003674\n",
      "Average Δ MSE (%) : +6.34%\n",
      "\n",
      "Vanilla  mean MAE : 0.180536\n",
      "Ensemble mean MAE : 0.177827\n",
      "Average Δ MAE     : +0.002709\n",
      "Average Δ MAE (%) : +1.50%\n"
     ]
    }
   ],
   "source": [
    "import torch, os, numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "TOP_K    = 65\n",
    "CH_CNT   = 1\n",
    "batch_sz = 32\n",
    "\n",
    "def infer_one(model, loader, ch):\n",
    "    buf = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, pv in loader:\n",
    "            xb, pv = xb.to(device), pv.to(device)\n",
    "            buf.append(model(xb, pv)[:, :, ch].cpu())   \n",
    "    return torch.cat(buf, 0)   \n",
    "\n",
    "\n",
    "van_mse_list, ens_mse_list, delta_mse_list, delta_mse_pct_list = [], [], [], []\n",
    "van_mae_list, ens_mae_list, delta_mae_list, delta_mae_pct_list = [], [], [], []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for ch in range(CH_CNT):\n",
    "    print(f\"\\n=== Channel {ch} ===\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    v_mse = results1_ETTh1_test['ES']['mse_test_EES']\n",
    "    v_mae = results1_ETTh1_test['ES']['mae_test_EES']\n",
    "\n",
    "    print(f\"[Vanilla]   MSE : {v_mse:.6f} | MAE : {v_mae:.6f}\")\n",
    "\n",
    "    van_mse_list.append(v_mse)\n",
    "    van_mae_list.append(v_mae)\n",
    "\n",
    "    preds_test_arr = []\n",
    "    ok_segs = []\n",
    "\n",
    "    \n",
    "    for seg in top_seg_idx_ES_val_1E[ch][:TOP_K]:\n",
    "        ckpt = f\"./model2_ETTh1_EES_1E/seg{seg}/ch{ch}/checkpoint.pth\"\n",
    "        if not os.path.exists(ckpt):\n",
    "            print(f\"  [skip] ckpt missing : seg{seg}\")\n",
    "            continue\n",
    "\n",
    "        pv_seg = test_pred_segments_ES[:, seg, :, :]\n",
    "        dl     = DataLoader(TensorDataset(test_sequences, test_labels, pv_seg),\n",
    "                            batch_size=batch_sz, shuffle=False)\n",
    "\n",
    "        mdl = Model(base_cfg).to(device)\n",
    "        mdl.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "\n",
    "        preds_test_arr.append(infer_one(mdl, dl, ch))   \n",
    "        ok_segs.append(seg)\n",
    "\n",
    "    if len(preds_test_arr) < 5:\n",
    "        print(\"  Not enough valid models.\")\n",
    "        continue\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    step = 5\n",
    "    Ks = list(range(step, len(preds_test_arr)+1, step))\n",
    "\n",
    "    var_list, rho_list = [], []\n",
    "\n",
    "    for K in Ks:\n",
    "        F = torch.stack(preds_test_arr[:K], dim=0)\n",
    "\n",
    "        V = torch.var(F, dim=0).mean().item()\n",
    "        var_list.append(V)\n",
    "\n",
    "        flat = F.reshape(K, -1).numpy()\n",
    "        corr = np.corrcoef(flat)\n",
    "        R = np.mean(np.abs(corr[np.triu_indices(K, 1)]))\n",
    "        rho_list.append(R)\n",
    "\n",
    "    \n",
    "    V_min, V_max = min(var_list), max(var_list)\n",
    "    R_min, R_max = min(rho_list), max(rho_list)\n",
    "\n",
    "    V_norm = [(v - V_min)/(V_max - V_min + 1e-12) for v in var_list]\n",
    "    R_norm = [(r - R_min)/(R_max - R_min + 1e-12) for r in rho_list]\n",
    "\n",
    "    alpha, beta = 1, 1\n",
    "    scores = [alpha*v + beta*r for v, r in zip(V_norm, R_norm)]\n",
    "\n",
    "    best_K = Ks[int(np.argmin(scores))]\n",
    "\n",
    "    print(\"  — Var / |Corr| / Score —\")\n",
    "    for k, v, r, s in zip(Ks, var_list, rho_list, scores):\n",
    "        tag = \"<- pick\" if k == best_K else \"\"\n",
    "        print(f\"   K={k:2d} | Var={v:.3e} | |ρ|={r:.4f} | S={s:.4f} {tag}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ens_pred = torch.mean(torch.stack(preds_test_arr[:best_K]), dim=0)\n",
    "    y_true   = test_labels[:, -base_cfg.pred_len:, ch].cpu()\n",
    "\n",
    "    mse_ens = torch.mean((ens_pred - y_true)**2).item()\n",
    "    mae_ens = torch.mean(torch.abs(ens_pred - y_true)).item()\n",
    "\n",
    "    ens_mse_list.append(mse_ens)\n",
    "    ens_mae_list.append(mae_ens)\n",
    "\n",
    "    \n",
    "    delta_mse = v_mse - mse_ens\n",
    "    delta_mae = v_mae - mae_ens\n",
    "\n",
    "    \n",
    "    delta_mse_pct = (delta_mse / v_mse) * 100\n",
    "    delta_mae_pct = (delta_mae / v_mae) * 100\n",
    "\n",
    "    delta_mse_list.append(delta_mse)\n",
    "    delta_mae_list.append(delta_mae)\n",
    "\n",
    "    delta_mse_pct_list.append(delta_mse_pct)\n",
    "    delta_mae_pct_list.append(delta_mae_pct)\n",
    "\n",
    "    print(f\"[Var-Corr]  K={best_K}\")\n",
    "    print(f\"  MSE = {mse_ens:.6f} | Δ = {delta_mse:+.6f} | Δ% = {delta_mse_pct:+.2f}%\")\n",
    "    print(f\"  MAE = {mae_ens:.6f} | Δ = {delta_mae:+.6f} | Δ% = {delta_mae_pct:+.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if ens_mse_list:\n",
    "    print(\"\\n──────── Summary (Var-Corr) ───────\")\n",
    "    print(f\"Vanilla  mean MSE : {np.mean(van_mse_list):.6f}\")\n",
    "    print(f\"Ensemble mean MSE : {np.mean(ens_mse_list):.6f}\")\n",
    "    print(f\"Average Δ MSE     : {np.mean(delta_mse_list):+.6f}\")\n",
    "    print(f\"Average Δ MSE (%) : {np.mean(delta_mse_pct_list):+.2f}%\\n\")\n",
    "\n",
    "    print(f\"Vanilla  mean MAE : {np.mean(van_mae_list):.6f}\")\n",
    "    print(f\"Ensemble mean MAE : {np.mean(ens_mae_list):.6f}\")\n",
    "    print(f\"Average Δ MAE     : {np.mean(delta_mae_list):+.6f}\")\n",
    "    print(f\"Average Δ MAE (%) : {np.mean(delta_mae_pct_list):+.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ebf98",
   "metadata": {},
   "source": [
    "## 1E->1E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c3da2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18053586781024933"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results1_ETTh1_test['ES']['mae_test_EES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf0d7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Channel 0 ===\n",
      "[Vanilla]   MSE : 0.057950 | MAE : 0.180536\n",
      "  — Var / |Corr| / Score —\n",
      "   K= 5 | Var=2.334e-02 | |ρ|=0.8023 | S=1.2802 \n",
      "   K=10 | Var=2.479e-02 | |ρ|=0.8081 | S=1.3871 \n",
      "   K=15 | Var=2.532e-02 | |ρ|=0.7996 | S=1.2983 \n",
      "   K=20 | Var=2.869e-02 | |ρ|=0.7905 | S=1.2749 \n",
      "   K=25 | Var=3.090e-02 | |ρ|=0.7799 | S=1.2033 \n",
      "   K=30 | Var=3.265e-02 | |ρ|=0.7698 | S=1.1256 \n",
      "   K=35 | Var=3.649e-02 | |ρ|=0.7663 | S=1.1824 \n",
      "   K=40 | Var=3.848e-02 | |ρ|=0.7651 | S=1.2188 \n",
      "   K=45 | Var=4.075e-02 | |ρ|=0.7527 | S=1.1268 \n",
      "   K=50 | Var=4.340e-02 | |ρ|=0.7396 | S=1.0366 \n",
      "   K=55 | Var=4.710e-02 | |ρ|=0.7316 | S=1.0354 \n",
      "   K=60 | Var=5.060e-02 | |ρ|=0.7213 | S=0.9998 <- pick\n",
      "   K=65 | Var=6.224e-02 | |ρ|=0.6965 | S=1.0000 \n",
      "[Var-Corr]  K=60\n",
      "   MSE = 0.055310 | Δ = +0.002640 | Δ% = +4.56%\n",
      "   MAE = 0.178058 | Δ = +0.002478 | Δ% = +1.37%\n",
      "\n",
      "──────── Summary (Var-Corr) ───────\n",
      "Vanilla  mean MSE  : 0.057950\n",
      "Ensemble mean MSE  : 0.055310\n",
      "Average Δ MSE      : +0.002640\n",
      "Average Δ MSE (%)  : +4.56%\n",
      "\n",
      "Vanilla  mean MAE  : 0.180536\n",
      "Ensemble mean MAE  : 0.178058\n",
      "Average Δ MAE      : +0.002478\n",
      "Average Δ MAE (%)  : +1.37%\n"
     ]
    }
   ],
   "source": [
    "import torch, os, numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "TOP_K    = 65\n",
    "CH_CNT   = 1\n",
    "batch_sz = 32\n",
    "\n",
    "def infer_one(model, loader, ch):\n",
    "    buf = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, pv in loader:\n",
    "            xb, pv = xb.to(device), pv.to(device)\n",
    "            buf.append(model(xb, pv)[:, :, ch].cpu())   \n",
    "    return torch.cat(buf, 0)    \n",
    "\n",
    "\n",
    "\n",
    "van_mse_list, ens_mse_list, delta_mse_list, delta_mse_pct_list = [], [], [], []\n",
    "van_mae_list, ens_mae_list, delta_mae_list, delta_mae_pct_list = [], [], [], []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for ch in range(CH_CNT):\n",
    "\n",
    "    print(f\"\\n=== Channel {ch} ===\")\n",
    "\n",
    "    \n",
    "    v_mse = results1_ETTh1_test['ES']['mse_test_EES']\n",
    "    v_mae = results1_ETTh1_test['ES']['mae_test_EES']\n",
    "\n",
    "    print(f\"[Vanilla]   MSE : {v_mse:.6f} | MAE : {v_mae:.6f}\")\n",
    "\n",
    "    van_mse_list.append(v_mse)\n",
    "    van_mae_list.append(v_mae)\n",
    "\n",
    "    preds_test_arr = []\n",
    "    ok_segs = []\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    for seg in top_seg_idx_E1_val_1E[ch][:TOP_K]:\n",
    "\n",
    "        ckpt = f\"./model2_ETTh1_E1_1E/seg{seg}/ch{ch}/checkpoint.pth\"\n",
    "        if not os.path.exists(ckpt):\n",
    "            print(f\"  [skip] missing checkpoint : seg{seg}\")\n",
    "            continue\n",
    "\n",
    "        mdl = Model(base_cfg).to(device)\n",
    "        mdl.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "\n",
    "        pv_seg = test_pred_segments_E1[:, seg, :, :]\n",
    "        dl = DataLoader(\n",
    "            TensorDataset(test_sequences, test_labels, pv_seg),\n",
    "            batch_size=batch_sz, shuffle=False\n",
    "        )\n",
    "\n",
    "        preds_test_arr.append(infer_one(mdl, dl, ch))\n",
    "        ok_segs.append(seg)\n",
    "\n",
    "    if len(preds_test_arr) < 5:\n",
    "        print(\"  Not enough valid models.\")\n",
    "        continue\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    step = 5\n",
    "    Ks = list(range(step, len(preds_test_arr)+1, step))\n",
    "\n",
    "    var_list = []\n",
    "    rho_list = []\n",
    "\n",
    "    for K in Ks:\n",
    "        F = torch.stack(preds_test_arr[:K], dim=0)   \n",
    "\n",
    "        \n",
    "        V = torch.var(F, dim=0).mean().item()\n",
    "        var_list.append(V)\n",
    "\n",
    "        \n",
    "        flat = F.reshape(K, -1).numpy()\n",
    "        corr = np.corrcoef(flat)\n",
    "        R    = np.mean(np.abs(corr[np.triu_indices(K, 1)]))\n",
    "        rho_list.append(R)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    V_min, V_max = min(var_list), max(var_list)\n",
    "    R_min, R_max = min(rho_list), max(rho_list)\n",
    "\n",
    "    V_norm = [(v - V_min) / (V_max - V_min + 1e-12) for v in var_list]\n",
    "    R_norm = [(r - R_min) / (R_max - R_min + 1e-12) for r in rho_list]\n",
    "\n",
    "    alpha, beta = 1, 1.35   \n",
    "\n",
    "    scores = [alpha*v + beta*r for v, r in zip(V_norm, R_norm)]\n",
    "    best_K = Ks[int(np.argmin(scores))]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"  — Var / |Corr| / Score —\")\n",
    "    for k, v, r, s in zip(Ks, var_list, rho_list, scores):\n",
    "        tag = \"<- pick\" if k == best_K else \"\"\n",
    "        print(f\"   K={k:2d} | Var={v:.3e} | |ρ|={r:.4f} | S={s:.4f} {tag}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ens_pred = torch.mean(torch.stack(preds_test_arr[:best_K]), dim=0)  \n",
    "    y_true   = test_labels[:, -base_cfg.pred_len:, ch].cpu()\n",
    "\n",
    "    mse_ens = torch.mean((ens_pred - y_true)**2).item()\n",
    "    mae_ens = torch.mean(torch.abs(ens_pred - y_true)).item()\n",
    "\n",
    "    ens_mse_list.append(mse_ens)\n",
    "    ens_mae_list.append(mae_ens)\n",
    "\n",
    "    \n",
    "    delta_mse = v_mse - mse_ens\n",
    "    delta_mae = v_mae - mae_ens\n",
    "\n",
    "    \n",
    "    delta_mse_pct = (delta_mse / v_mse) * 100\n",
    "    delta_mae_pct = (delta_mae / v_mae) * 100\n",
    "\n",
    "    delta_mse_list.append(delta_mse)\n",
    "    delta_mae_list.append(delta_mae)\n",
    "    delta_mse_pct_list.append(delta_mse_pct)\n",
    "    delta_mae_pct_list.append(delta_mae_pct)\n",
    "\n",
    "    sign = \"+\" if delta_mse > 0 else \"-\"\n",
    "    print(f\"[Var-Corr]  K={best_K}\")\n",
    "    print(f\"   MSE = {mse_ens:.6f} | Δ = {delta_mse:+.6f} | Δ% = {delta_mse_pct:+.2f}%\")\n",
    "    print(f\"   MAE = {mae_ens:.6f} | Δ = {delta_mae:+.6f} | Δ% = {delta_mae_pct:+.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if ens_mse_list:\n",
    "    print(\"\\n──────── Summary (Var-Corr) ───────\")\n",
    "    print(f\"Vanilla  mean MSE  : {np.mean(van_mse_list):.6f}\")\n",
    "    print(f\"Ensemble mean MSE  : {np.mean(ens_mse_list):.6f}\")\n",
    "    print(f\"Average Δ MSE      : {np.mean(delta_mse_list):+.6f}\")\n",
    "    print(f\"Average Δ MSE (%)  : {np.mean(delta_mse_pct_list):+.2f}%\\n\")\n",
    "\n",
    "    print(f\"Vanilla  mean MAE  : {np.mean(van_mae_list):.6f}\")\n",
    "    print(f\"Ensemble mean MAE  : {np.mean(ens_mae_list):.6f}\")\n",
    "    print(f\"Average Δ MAE      : {np.mean(delta_mae_list):+.6f}\")\n",
    "    print(f\"Average Δ MAE (%)  : {np.mean(delta_mae_pct_list):+.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a80e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c326e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3dc285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06b967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sunho_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
